#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ£€æŸ¥ç‰¹å®šå›¾çš„ç‰¹å¾ï¼Œè¯Šæ–­NaNé—®é¢˜
"""

import pickle
import numpy as np
import os

def check_graph_features(graph_path):
    """æ£€æŸ¥å›¾æ•°æ®çš„å„é¡¹ç‰¹å¾ç»Ÿè®¡"""
    print(f"\n{'='*70}")
    print(f"æ£€æŸ¥å›¾: {os.path.basename(graph_path)}")
    print(f"{'='*70}")
    
    try:
        with open(graph_path, 'rb') as f:
            graph_data = pickle.load(f)
        
        print(f"\nğŸ“Š åŸºæœ¬ä¿¡æ¯:")
        print(f"   èŠ‚ç‚¹æ•°: {graph_data.x.shape[0]:,}")
        print(f"   è¾¹æ•°: {graph_data.edge_index.shape[1]:,}")
        print(f"   ç‰¹å¾ç»´åº¦: {graph_data.x.shape[1]}")
        
        print(f"\nğŸ“ˆ èŠ‚ç‚¹ç‰¹å¾ç»Ÿè®¡:")
        x = graph_data.x.numpy() if hasattr(graph_data.x, 'numpy') else graph_data.x
        
        print(f"   å½¢çŠ¶: {x.shape}")
        print(f"   æ•°æ®ç±»å‹: {x.dtype}")
        print(f"   æœ€å°å€¼: {np.min(x):.6f}")
        print(f"   æœ€å¤§å€¼: {np.max(x):.6f}")
        print(f"   å‡å€¼: {np.mean(x):.6f}")
        print(f"   æ ‡å‡†å·®: {np.std(x):.6f}")
        print(f"   ä¸­ä½æ•°: {np.median(x):.6f}")
        
        # æ£€æŸ¥å¼‚å¸¸å€¼
        print(f"\nğŸ” å¼‚å¸¸å€¼æ£€æŸ¥:")
        nan_count = np.isnan(x).sum()
        inf_count = np.isinf(x).sum()
        zero_count = (x == 0).sum()
        total_elements = x.size
        
        print(f"   NaNæ•°é‡: {nan_count} / {total_elements} ({100*nan_count/total_elements:.2f}%)")
        print(f"   Infæ•°é‡: {inf_count} / {total_elements} ({100*inf_count/total_elements:.2f}%)")
        print(f"   é›¶å€¼æ•°é‡: {zero_count} / {total_elements} ({100*zero_count/total_elements:.2f}%)")
        
        if nan_count > 0:
            print(f"   âš ï¸ è­¦å‘Š: å‘ç°NaNå€¼!")
        if inf_count > 0:
            print(f"   âš ï¸ è­¦å‘Š: å‘ç°Infå€¼!")
        
        # æ£€æŸ¥æ¯ä¸ªç‰¹å¾ç»´åº¦
        print(f"\nğŸ“Š å„ç‰¹å¾ç»´åº¦ç»Ÿè®¡:")
        for i in range(x.shape[1]):
            feat = x[:, i]
            print(f"   ç»´åº¦ {i:2d}: min={np.min(feat):8.4f}, max={np.max(feat):8.4f}, "
                  f"mean={np.mean(feat):8.4f}, std={np.std(feat):8.4f}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æç«¯å€¼
            if np.max(np.abs(feat)) > 100:
                print(f"           âš ï¸ è­¦å‘Š: ç‰¹å¾å€¼è¿‡å¤§ (|max|={np.max(np.abs(feat)):.2f})")
            if np.std(feat) < 1e-6:
                print(f"           âš ï¸ è­¦å‘Š: æ ‡å‡†å·®è¿‡å° (å¯èƒ½æ˜¯å¸¸é‡ç‰¹å¾)")
        
        # æ£€æŸ¥è¾¹ç´¢å¼•
        print(f"\nğŸ“Š è¾¹ç´¢å¼•ç»Ÿè®¡:")
        edge_index = graph_data.edge_index.numpy() if hasattr(graph_data.edge_index, 'numpy') else graph_data.edge_index
        print(f"   å½¢çŠ¶: {edge_index.shape}")
        print(f"   æœ€å°ç´¢å¼•: {np.min(edge_index)}")
        print(f"   æœ€å¤§ç´¢å¼•: {np.max(edge_index)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è‡ªç¯
        self_loops = np.sum(edge_index[0] == edge_index[1])
        print(f"   è‡ªç¯æ•°é‡: {self_loops}")
        
        # æ£€æŸ¥åº¦åˆ†å¸ƒ
        degrees = np.bincount(edge_index[0])
        print(f"   æœ€å°åº¦: {np.min(degrees)}")
        print(f"   æœ€å¤§åº¦: {np.max(degrees)}")
        print(f"   å¹³å‡åº¦: {np.mean(degrees):.2f}")
        
        if np.max(degrees) > 1000:
            print(f"   âš ï¸ è­¦å‘Š: å­˜åœ¨è¶…é«˜åº¦èŠ‚ç‚¹ (max_degree={np.max(degrees)})")
        
        # ç‰¹å¾ç›¸å…³æ€§æ£€æŸ¥
        print(f"\nğŸ“Š ç‰¹å¾ç›¸å…³æ€§åˆ†æ:")
        corr_matrix = np.corrcoef(x.T)
        max_corr = np.max(corr_matrix[np.triu_indices_from(corr_matrix, k=1)])
        print(f"   æœ€å¤§ç‰¹å¾é—´ç›¸å…³ç³»æ•°: {max_corr:.4f}")
        
        if max_corr > 0.95:
            print(f"   âš ï¸ è­¦å‘Š: ç‰¹å¾é«˜åº¦ç›¸å…³ (å¯èƒ½å†—ä½™)")
        
        # æ½œåœ¨é—®é¢˜æ€»ç»“
        print(f"\nğŸ¯ æ½œåœ¨é—®é¢˜æ€»ç»“:")
        issues = []
        
        if nan_count > 0:
            issues.append(f"âœ— åŒ…å«{nan_count}ä¸ªNaNå€¼")
        if inf_count > 0:
            issues.append(f"âœ— åŒ…å«{inf_count}ä¸ªInfå€¼")
        if np.max(np.abs(x)) > 100:
            issues.append(f"âœ— ç‰¹å¾å€¼è¿‡å¤§ (max={np.max(np.abs(x)):.2f})")
        if np.max(degrees) > 1000:
            issues.append(f"âœ— è¶…é«˜åº¦èŠ‚ç‚¹ (max_degree={np.max(degrees)})")
        if zero_count / total_elements > 0.5:
            issues.append(f"âœ— ç¨€ç–ç‰¹å¾ (é›¶å€¼å æ¯”{100*zero_count/total_elements:.1f}%)")
        
        if issues:
            for issue in issues:
                print(f"   {issue}")
            print(f"\n   ğŸ’¡ å»ºè®®: è¯¥å›¾å¯èƒ½éœ€è¦åŠ å…¥é»‘åå•æˆ–è¿›è¡Œç‰¹å¾é¢„å¤„ç†")
        else:
            print(f"   âœ“ æœªå‘ç°æ˜æ˜¾é—®é¢˜")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    # æ£€æŸ¥é—®é¢˜å›¾
    problem_graph = r"e:\Project\VGAT-ZeroWatermark-V5\convertToGraph\Graph\TrainingSet\Original\tianjin-latest-free.shp-gis_osm_transport_free_1_graph.pkl"
    
    if os.path.exists(problem_graph):
        check_graph_features(problem_graph)
    else:
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {problem_graph}")
    
    # å¯¹æ¯”æ£€æŸ¥ä¸€ä¸ªæ­£å¸¸çš„å›¾
    print(f"\n\n" + "="*70)
    print("å¯¹æ¯”ï¼šæ£€æŸ¥ä¸€ä¸ªæ­£å¸¸çš„å›¾")
    print("="*70)
    
    normal_graphs = [
        r"e:\Project\VGAT-ZeroWatermark-V5\convertToGraph\Graph\TrainingSet\Original\H51-RESA_graph.pkl",
        r"e:\Project\VGAT-ZeroWatermark-V5\convertToGraph\Graph\TrainingSet\Original\H51-RESP_graph.pkl",
    ]
    
    for normal_graph in normal_graphs:
        if os.path.exists(normal_graph):
            check_graph_features(normal_graph)
            break
