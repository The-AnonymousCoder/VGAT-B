#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""Improved VGAT training pipeline for robust zero-watermark extraction."""

import json
import logging
import os
import pickle
import random
import subprocess
import sys
import traceback
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Iterable, List, Optional, Sequence, Tuple

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.cuda import amp
from torch_geometric.nn import GATv2Conv, GraphNorm  # ä½¿ç”¨GATv2 + GraphNorm
from tqdm import tqdm


def default_memory_refresh_marks(total_epochs: int) -> Tuple[int, ...]:
    if total_epochs <= 4:
        return (max(1, total_epochs - 1),)
    marks = []
    marks.append(max(1, int(total_epochs * 0.4)))
    marks.append(max(marks[-1] + 1, int(total_epochs * 0.55)))
    marks = [min(total_epochs - 1, m) for m in marks]
    unique_marks = sorted(set(marks))
    return tuple(mark for mark in unique_marks if mark >= 1)


def should_reset_patience(epoch: int) -> bool:
    return epoch in PATIENT_STAGE_EPOCHS


FIXED_LOSS_WEIGHTS = {
    'contrastive': 1.0,
    'similarity': 0.5,
    'diversity': 0.3,
    'binary_consistency': 1.0,
}
FULL_CHAIN_ATTACK_KEYWORDS = ("full_attack_chain", "full_chain", "compound_seq", "compound_seq_all")
COMBO_ATTACK_KEYWORDS = ("combo", )
COMPOSITE_ATTACK_KEYWORDS = FULL_CHAIN_ATTACK_KEYWORDS + COMBO_ATTACK_KEYWORDS
WEAK_ATTACK_KEYWORDS = ("noise", "add", "crop", "rotate", "flip")
PATIENT_STAGE_EPOCHS = (20, 30, 40)


def get_attack_name(graph) -> str:
    """Return the normalized attack name attached to a graph sample."""
    return str(getattr(graph, 'attack_type', '')).lower()


def is_full_chain_attack(name: str) -> bool:
    return any(keyword in name for keyword in FULL_CHAIN_ATTACK_KEYWORDS)


def is_combo_attack(name: str) -> bool:
    return any(keyword in name for keyword in COMBO_ATTACK_KEYWORDS) and not is_full_chain_attack(name)


def is_composite_attack(name: str) -> bool:
    return is_full_chain_attack(name) or is_combo_attack(name)


def has_weak_perturbation(name: str) -> bool:
    return any(keyword in name for keyword in WEAK_ATTACK_KEYWORDS)


def attack_sample_weight(name: str) -> float:
    """Heuristically score attack difficulty for sampling/weighting."""
    weight = 1.0
    if is_full_chain_attack(name):
        weight = 3.0
    elif is_combo_attack(name):
        weight = 2.4
    if 'noise' in name:
        weight *= 1.4
    if 'add' in name:
        weight *= 1.4
    if 'crop' in name:
        weight *= 1.3
    if 'rotate' in name:
        weight *= 1.3
    if 'flip' in name:
        weight *= 1.3
    return weight


def compute_stage_progress(epoch: int, total_epochs: int) -> Tuple[str, float]:
    early_end = max(1, int(total_epochs * 0.3))
    mid_end = max(early_end + 1, int(total_epochs * 0.7))
    if epoch < early_end:
        return "early", epoch / max(1, early_end)
    if epoch < mid_end:
        return "mid", (epoch - early_end) / max(1, mid_end - early_end)
    return "late", (epoch - mid_end) / max(1, total_epochs - mid_end)


STAGE_DESCRIPTIONS = {
    "early": "å‰æœŸ-åŒºåˆ†+å”¯ä¸€æ€§",
    "mid": "ä¸­æœŸ-å¹³è¡¡ä¼˜åŒ–",
    "late": "åæœŸ-å¼ºåŒ–é²æ£’æ€§",
}


def describe_stage(stage: str, progress: float) -> str:
    base = STAGE_DESCRIPTIONS.get(stage, stage)
    progress_clamped = max(0.0, min(1.0, progress))
    if stage == "early":
        return base
    return f"{base} ({progress_clamped * 100:.0f}%)"


def run_fig12_evaluation_for_model(model_path: str) -> Optional[float]:
    """
    ä½¿ç”¨å½“å‰æ¨¡å‹è¿è¡Œä¸€æ¬¡ zNC-Test/Fig12.pyï¼Œå¹¶è§£æFig12çš„Average NCå€¼ã€‚
    æ³¨æ„ï¼šè¿™æ˜¯ä¸€ä¸ªé‡æ“ä½œï¼Œå»ºè®®ä»…åœ¨å…³é”®epochè°ƒç”¨ã€‚
    """
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = str(Path(script_dir).parents[0])
        znc_root = os.path.join(project_root, "zNC-Test")
        zerowm_dir = os.path.join(znc_root, "vector-data-zerowatermark")
        fig12_results_dir = os.path.join(znc_root, "NC-Results", "Fig12")

        # â­å…³é”®ä¿®å¤ï¼šæ¯æ¬¡è¯„ä¼°å‰åˆ é™¤NC-Results/Fig12æ–‡ä»¶å¤¹ï¼Œç¡®ä¿Fig12.pyé‡æ–°ç”Ÿæˆç»“æœ
        if os.path.isdir(fig12_results_dir):
            try:
                import shutil
                shutil.rmtree(fig12_results_dir)
                logging.getLogger(__name__).info(f"[Fig12] å·²åˆ é™¤æ—§ç»“æœç›®å½•: {fig12_results_dir}")
            except Exception as e:
                logging.getLogger(__name__).warning(f"[Fig12] åˆ é™¤ç»“æœç›®å½•å¤±è´¥ {fig12_results_dir}: {e}")

        # æ¯æ¬¡è¯„ä¼°å‰æ¸…ç©ºé›¶æ°´å°ç›®å½•ï¼Œç¡®ä¿ç”¨å½“å‰æ¨¡å‹é‡æ–°ç”Ÿæˆ
        if os.path.isdir(zerowm_dir):
            for fname in os.listdir(zerowm_dir):
                if fname.startswith("."):
                    continue
                fpath = os.path.join(zerowm_dir, fname)
                try:
                    if os.path.isfile(fpath):
                        os.remove(fpath)
                except Exception as e:
                    logging.getLogger(__name__).warning(f"[Fig12] åˆ é™¤é›¶æ°´å°æ–‡ä»¶å¤±è´¥ {fpath}: {e}")

        env = os.environ.copy()
        env["VGAT_MODEL_PATH"] = model_path

        # åœ¨zNC-Testç›®å½•ä¸‹è°ƒç”¨Fig12.py
        logging.getLogger(__name__).info(f"[Fig12] ä½¿ç”¨æ¨¡å‹è¯„ä¼°é²æ£’æ€§: {model_path}")
        logging.getLogger(__name__).info(f"[Fig12] ç¯å¢ƒå˜é‡VGAT_MODEL_PATH={model_path}")
        
        # â­å…³é”®ä¿®å¤ï¼šç¡®ä¿subprocesså®Œæˆå¹¶æ£€æŸ¥è¿”å›ç 
        result = subprocess.run(
            [sys.executable, "Fig12.py"],
            cwd=znc_root,
            env=env,
            check=False,
            capture_output=True,
            text=True,
        )
        
        if result.returncode != 0:
            logging.getLogger(__name__).warning(f"[Fig12] Fig12.pyæ‰§è¡Œå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
            if result.stderr:
                logging.getLogger(__name__).warning(f"[Fig12] é”™è¯¯è¾“å‡º: {result.stderr[:500]}")

        # â­å…³é”®ä¿®å¤ï¼šç­‰å¾…æ–‡ä»¶ç³»ç»ŸåŒæ­¥ï¼Œç¡®ä¿CSVæ–‡ä»¶å·²å®Œå…¨å†™å…¥
        import time
        time.sleep(0.5)  # ç­‰å¾…0.5ç§’ç¡®ä¿æ–‡ä»¶å†™å…¥å®Œæˆ

        # è§£æç»“æœCSVï¼Œæå–Average NC
        csv_path = os.path.join(znc_root, "NC-Results", "Fig12", "fig12_compound_seq_nc.csv")
        if not os.path.exists(csv_path):
            logging.getLogger(__name__).warning(f"[Fig12] æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶: {csv_path}")
            logging.getLogger(__name__).warning(f"[Fig12] è¯·æ£€æŸ¥Fig12.pyæ˜¯å¦æ­£å¸¸æ‰§è¡Œå®Œæˆ")
            return None

        avg_nc = None
        try:
            import csv as _csv

            with open(csv_path, "r", encoding="utf-8-sig") as f:
                reader = _csv.DictReader(f)
                for row in reader:
                    # æ ‡å‡†åˆ—åï¼š'å¤åˆæ”»å‡»(é¡ºåº)', 'VGAT', 'ç±»å‹'
                    row_type = row.get("ç±»å‹", "").strip()
                    if row_type == "average":
                        v = row.get("VGAT", "").strip()
                        try:
                            avg_nc = float(v)
                        except Exception:
                            continue
        except Exception as e:
            logging.getLogger(__name__).error(f"[Fig12] è§£æç»“æœCSVå¤±è´¥: {e}")
            return None

        return avg_nc
    except Exception as e:
        logging.getLogger(__name__).error(f"[Fig12] è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        logging.getLogger(__name__).error(traceback.format_exc())
        return None


def stage_temperature(stage: str) -> float:
    return {
        "early": 0.15,
        "mid": 0.12,
        "late": 0.09,
    }.get(stage, 0.12)


def stage_augmentation_probability(stage: str) -> float:
    return {
        "early": 0.30,
        "mid": 0.40,
        "late": 0.50,
    }.get(stage, 0.40)


@dataclass
class TrainingScheduleConfig:
    metric_eval_interval: int = 5
    metric_patience: int = 3
    min_epoch_for_metric_stop: int = 15
    nc_improve_tol: float = 0.005
    distinction_improve_tol: float = 0.005
    onecycle_max_lr: float = 0.0015  # ä»0.001æå‡åˆ°0.0015ï¼Œå¢å¼ºåæœŸå­¦ä¹ èƒ½åŠ›
    onecycle_pct_start: float = 0.2
    onecycle_div_factor: float = 100.0
    onecycle_final_div: float = 5000.0  # ä»10000.0é™ä½åˆ°5000.0ï¼Œé¿å…åæœŸå­¦ä¹ ç‡è¿‡å°
    robust_warmup_epochs: int = 4
    robust_lr_boost: float = 1.6
    robust_supcon_temp: float = 0.12
    robust_memory_keep_ratio: float = 0.6
    robust_memory_refresh_interval: int = 2


def log_training_overview():
    logger.info("=" * 70)
    logger.info("æ”¹è¿›VGATï¼šçŸ¢é‡é›¶æ°´å°é²æ£’ç‰¹å¾è®­ç»ƒ")
    logger.info("æ ¸å¿ƒä¼˜åŒ–: InfoNCEä¿®å¤ | äºŒå€¼åŒ–æŸå¤± | GATv2+æ®‹å·® | OneCycleLR | åŠ¨æ€æŸå¤±æƒé‡")
    logger.info("=" * 70)
    logger.info("")


def log_device_info() -> str:
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    if device == 'cuda':
        try:
            logger.info(f"GPU: {torch.cuda.get_device_name(0)}")
            total_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"æ˜¾å­˜: {total_mem_gb:.2f} GB")
        except Exception as err:
            logger.warning(f"æ— æ³•è¯»å–GPUä¿¡æ¯: {err}")
    logger.info("")
    return device


def infer_input_dim(graphs) -> int:
    try:
        first_graph = next(iter(graphs.values()))
    except StopIteration as exc:
        raise ValueError("original_graphsä¸ºç©ºï¼Œæ— æ³•æ¨æ–­è¾“å…¥ç»´åº¦") from exc
    return int(first_graph.x.shape[1])


def log_feature_profile(input_dim: int):
    logger.info(f"æ£€æµ‹åˆ°è¾“å…¥ç‰¹å¾ç»´åº¦: {input_dim}")
    if input_dim == 20:
        logger.info("âœ… ä½¿ç”¨20ç»´æœ€ä¼˜ç‰¹å¾ï¼ˆæ–¹æ¡ˆDï¼šå…¨å±€+å±€éƒ¨å¤šå°ºåº¦+èŠ‚ç‚¹æ•°ç¼–ç ï¼‰")
    elif input_dim == 19:
        logger.info("âœ… ä½¿ç”¨19ç»´ä¼˜åŒ–ç‰¹å¾ï¼ˆHuä¸å˜çŸ© + å…¨å±€/å±€éƒ¨ä½ç½® + æ‹“æ‰‘é‚»åŸŸï¼‰")
    elif input_dim == 16:
        logger.info("âš ï¸ ä½¿ç”¨16ç»´ç‰¹å¾ï¼ˆå»ºè®®å‡çº§åˆ°20ç»´ä»¥æå‡é²æ£’æ€§ï¼‰")
    elif input_dim == 13:
        logger.info("âš ï¸ ä½¿ç”¨åŸå§‹13ç»´ç‰¹å¾ï¼ˆå»ºè®®å‡çº§åˆ°20ç»´ï¼‰")
    else:
        logger.info("âš ï¸ æœªè¯†åˆ«çš„ç‰¹å¾ç»´åº¦ï¼Œæ¨¡å‹å°†è‡ªåŠ¨é€‚é…")
    logger.info("")


def log_model_summary(model: nn.Module):
    total_params = sum(p.numel() for p in model.parameters())
    logger.info(f"æ¨¡å‹æ€»å‚æ•°æ•°é‡: {total_params:,}")
    logger.info("")


def resolve_batch_size(default: int = 6) -> int:
    try:
        batch_size = int(os.environ.get('VGAT_BATCH_SIZE', str(default)))
    except Exception:
        batch_size = default
    logger.info(f"åˆå§‹æ‰¹æ¬¡å¤§å°: {batch_size}")
    logger.info(f"æ™ºèƒ½é™çº§ç­–ç•¥: {batch_size}â†’6â†’4â†’2â†’1 (è‡ªé€‚åº”)")
    if batch_size == 8:
        logger.info("   âœ… æ¨èé…ç½®ï¼š2åŸå›¾Ã—4æ ·æœ¬ï¼Œ16ä¸ªæ­£æ ·æœ¬å¯¹ï¼›å¦‚OOMè¯·é™åˆ°6")
    elif batch_size > 8:
        logger.warning(f"âš ï¸ batch_size={batch_size} å¯èƒ½å¯¼è‡´OOMï¼Œå»ºè®®<=8")
    elif batch_size == 6:
        logger.info("   å…¼å®¹é…ç½®ï¼š2åŸå›¾Ã—3æ ·æœ¬ï¼Œ12ä¸ªæ­£æ ·æœ¬å¯¹")
    elif batch_size <= 2:
        logger.warning("   batch_sizeè¿‡å°ï¼Œå¯¹æ¯”å­¦ä¹ æ•ˆæœå—é™")
    elif batch_size == 4:
        logger.warning("   batch_size=4 ä»…æä¾›2ä¸ªæ­£æ ·æœ¬å¯¹ï¼Œå»ºè®®æå‡è‡³6æˆ–8")
    logger.info("")
    return batch_size


def resolve_checkpoint_choice(default_checkpoint: str) -> Optional[str]:
    mode = os.environ.get('VGAT_RESUME_TRAINING', 'auto').lower()
    checkpoint_exists = os.path.exists(default_checkpoint)

    if mode in {'false', '0', 'no'}:
        logger.info("ä»å¤´å¼€å§‹æ–°çš„è®­ç»ƒ")
        return None

    if not checkpoint_exists:
        if mode in {'true', '1'}:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°checkpointæ–‡ä»¶: {default_checkpoint}")
        return None

    if mode in {'true', '1'}:
        logger.info(f"âœ… å¼ºåˆ¶ä»checkpointæ¢å¤: {default_checkpoint}")
        return default_checkpoint

    if mode == 'auto':
        logger.info(f"ğŸ” æ£€æµ‹åˆ°checkpointæ–‡ä»¶: {default_checkpoint}")
        try:
            user_choice = input("æ˜¯å¦ä»checkpointæ¢å¤è®­ç»ƒï¼Ÿ[y/N]: ").strip().lower()
        except EOFError:
            logger.warning("æ ‡å‡†è¾“å…¥ä¸å¯ç”¨ï¼Œé»˜è®¤ä»å¤´è®­ç»ƒ")
            return None
        if user_choice in {'y', 'yes'}:
            logger.info("é€‰æ‹©ä»checkpointæ¢å¤è®­ç»ƒ")
            return default_checkpoint
        logger.info("é€‰æ‹©ä»å¤´å¼€å§‹è®­ç»ƒ")
    else:
        logger.info("ä»å¤´å¼€å§‹æ–°çš„è®­ç»ƒ")
    return None


# =============================================================
# æ—¥å¿—ä¸å…¨å±€çŠ¶æ€
# =============================================================

# è®¾ç½®æ—¥å¿—
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•ï¼ˆæŒ‰æ—¶é—´æˆ³+PIDç”Ÿæˆå”¯ä¸€æ–‡ä»¶ï¼Œå¹¶ç»´æŠ¤ latest æ–‡ä»¶ï¼‰"""
    # ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼Œç¡®ä¿æ—¥å¿—ç›®å½•ä½äºè„šæœ¬æ‰€åœ¨ç›®å½•
    script_dir = os.path.dirname(os.path.abspath(__file__))
    log_dir = os.path.join(script_dir, "logs")
    
    if not os.path.exists(log_dir):
        os.makedirs(log_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    pid = os.getpid()
    log_file = os.path.join(log_dir, f"step3_training_IMPROVED_{timestamp}_{pid}.log")
    latest_file = os.path.join(log_dir, "step3_training_IMPROVED_latest.log")

    # æ¸…ç†å¯èƒ½å­˜åœ¨çš„é‡å¤ handler
    root_logger = logging.getLogger()
    if root_logger.handlers:
        for h in list(root_logger.handlers):
            root_logger.removeHandler(h)

    # é…ç½®ä¸¤ä¸ªæ–‡ä»¶è¾“å‡ºï¼šå”¯ä¸€æ—¥å¿—æ–‡ä»¶ + latest å¿«ç…§
    file_handler_unique = logging.FileHandler(log_file, encoding='utf-8')
    file_handler_latest = logging.FileHandler(latest_file, mode='w', encoding='utf-8')
    console_handler = logging.StreamHandler()

    for h in (file_handler_unique, file_handler_latest, console_handler):
        h.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))

    # è®¾ç½®æ—¥å¿—çº§åˆ«ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡æ§åˆ¶ï¼‰
    log_level = os.environ.get('VGAT_LOG_LEVEL', 'INFO')  # é»˜è®¤INFOï¼ˆå‡å°‘æ—¥å¿—é‡ï¼‰
    root_logger.setLevel(getattr(logging, log_level.upper(), logging.INFO))
    root_logger.addHandler(file_handler_unique)
    root_logger.addHandler(file_handler_latest)
    root_logger.addHandler(console_handler)

    # å°†å½“å‰æ—¥å¿—è·¯å¾„æš´éœ²ä¸ºæ¨¡å—çº§å˜é‡
    globals()["CURRENT_LOG_FILE"] = log_file
    globals()["CURRENT_LATEST_LOG"] = latest_file
    os.environ["VGAT_CURRENT_LOG"] = log_file
    os.environ["VGAT_CURRENT_LOG_LATEST"] = latest_file

    logger = logging.getLogger(__name__)
    logger.info(f"ä¼˜åŒ–ç‰ˆGATæ¨¡å‹è®­ç»ƒ")
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    logger.info(f"æœ€æ–°æ—¥å¿—(è¦†ç›–): {latest_file}")
    return logger

# â­ åªåœ¨ç›´æ¥è¿è¡Œè®­ç»ƒæ—¶æ‰è®¾ç½®æ—¥å¿—ï¼ˆè¢«å…¶ä»–è„šæœ¬å¯¼å…¥æ—¶ä¸è®¾ç½®ï¼‰
# åˆå§‹åŒ–ä¸€ä¸ªåŸºæœ¬çš„ loggerï¼ˆè¢«å¯¼å…¥æ—¶ä¸å†™æ–‡ä»¶ï¼‰
logger = logging.getLogger(__name__)
if not logger.handlers:
    logger.addHandler(logging.NullHandler())  # é»˜è®¤ä¸è¾“å‡ºæ—¥å¿—

class ImprovedGATModel(nn.Module):
    """å›¾çº§é²æ£’ç‰¹å¾æå–éª¨å¹²ï¼šGATv2 + æ®‹å·® + GraphNorm + å¤šå°ºåº¦æ± åŒ–ã€‚"""
    
    def __init__(self, input_dim, hidden_dim=256, output_dim=1024, num_heads=8, dropout=0.3):
        super(ImprovedGATModel, self).__init__()
        
        logger.info(f"åˆ›å»ºæ”¹è¿›çš„GATæ¨¡å‹ï¼ˆèŠ‚ç‚¹+å›¾æ± åŒ–æ¶æ„ï¼‰:")
        logger.info(f"  åŸå§‹è¾“å…¥ç»´åº¦: {input_dim}")
        
        # âœ… ç‰¹å¾åˆ†ç¦»ï¼šèŠ‚ç‚¹çº§ vs å›¾çº§
        # èŠ‚ç‚¹çº§ç‰¹å¾ï¼ˆæ¯ä¸ªèŠ‚ç‚¹ä¸åŒï¼‰ï¼šç»´åº¦5-10, 14-17 = 10ç»´
        self.node_feature_dims = [5, 6, 7, 8, 9, 10, 14, 15, 16, 17]
        # å›¾çº§ç‰¹å¾ï¼ˆæ•´ä¸ªå›¾å…±äº«ï¼‰ï¼šç»´åº¦0-2, 11-13, 18-19 = 8ç»´
        self.graph_feature_dims = [0, 1, 2, 11, 12, 13, 18, 19]
        
        self.node_input_dim = len(self.node_feature_dims)  # 10
        self.graph_input_dim = len(self.graph_feature_dims)  # 8
        
        logger.info(f"  èŠ‚ç‚¹çº§ç‰¹å¾ç»´åº¦: {self.node_input_dim} (dims: {self.node_feature_dims})")
        logger.info(f"  å›¾çº§ç‰¹å¾ç»´åº¦: {self.graph_input_dim} (dims: {self.graph_feature_dims})")
        logger.info(f"  éšè—ç»´åº¦: {hidden_dim}")
        logger.info(f"  è¾“å‡ºç»´åº¦: {output_dim}")
        logger.info(f"  æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
        logger.info(f"  Dropout: {dropout}")
        
        # ç¬¬1å±‚ï¼šGATv2ï¼ˆconcatå¤šå¤´ï¼‰- åªå¤„ç†èŠ‚ç‚¹çº§ç‰¹å¾
        self.gat1 = GATv2Conv(self.node_input_dim, hidden_dim, heads=num_heads, dropout=dropout, concat=True)
        # GraphNormï¼šæŒ‰å›¾å½’ä¸€åŒ–ï¼Œå…¼å®¹å•å›¾/å°batch
        self.gn1 = GraphNorm(hidden_dim * num_heads)
        
        # ç¬¬2å±‚ï¼šGATv2ï¼ˆä¸concatï¼Œè¾“å‡ºhidden_dimï¼‰
        self.gat2 = GATv2Conv(hidden_dim * num_heads, hidden_dim, heads=num_heads, dropout=dropout, concat=False)
        # GraphNormï¼šä¿æŒä¸åŒå›¾è§„æ¨¡ä¸‹çš„æ•°å€¼ç¨³å®š
        self.gn2 = GraphNorm(hidden_dim)
        
        # æ®‹å·®æŠ•å½±ï¼ˆå¦‚æœç»´åº¦ä¸åŒ¹é…ï¼‰
        if self.node_input_dim != hidden_dim:
            self.residual_proj = nn.Linear(self.node_input_dim, hidden_dim)
            logger.info(f"  æ·»åŠ æ®‹å·®æŠ•å½±: {self.node_input_dim} -> {hidden_dim}")
        else:
            self.residual_proj = None
        
        # æ³¨æ„åŠ›æ± åŒ–
        self.attention_pool = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )
        
        # å›¾çº§ç‰¹å¾ç¼–ç å™¨ï¼ˆå¯é€‰ï¼Œå¦‚æœå›¾çº§ç‰¹å¾éœ€è¦å­¦ä¹ ï¼‰
        self.graph_encoder = nn.Sequential(
            nn.Linear(self.graph_input_dim, hidden_dim // 2),
            nn.LayerNorm(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # ç‰¹å¾èåˆï¼š3ç§æ± åŒ–ï¼ˆå‡å€¼+æœ€å¤§+æ³¨æ„åŠ›ï¼‰+ å›¾çº§ç‰¹å¾ -> output_dim
        fusion_input_dim = hidden_dim * 3 + hidden_dim // 2  # èŠ‚ç‚¹æ± åŒ– + å›¾çº§ç¼–ç 
        self.fusion = nn.Sequential(
            nn.Linear(fusion_input_dim, hidden_dim * 2),
            nn.LayerNorm(hidden_dim * 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, output_dim),
            nn.Tanh()  # è¾“å‡º[-1,1]ï¼Œé€‚åˆä¸­ä½æ•°é˜ˆå€¼äºŒå€¼åŒ–
        )
        
        logger.info(f"  èåˆå±‚è¾“å…¥ç»´åº¦: {fusion_input_dim} = {hidden_dim*3}(èŠ‚ç‚¹æ± åŒ–) + {hidden_dim//2}(å›¾çº§ç¼–ç )")
        
        self.dropout = nn.Dropout(dropout)
        
        # âœ… æ”¹è¿›æƒé‡åˆå§‹åŒ–ï¼ˆé˜²æ­¢ç¬¬ä¸€ä¸ªbatch NaNï¼‰
        self._init_weights()
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = sum(p.numel() for p in self.parameters())
        logger.info(f"  æ€»å‚æ•°æ•°é‡: {total_params:,}")
    
    def _init_weights(self):
        """
        æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–ç­–ç•¥ï¼ˆé˜²æ­¢è®­ç»ƒåˆæœŸNaNï¼‰
        """
        for name, param in self.named_parameters():
            if 'weight' in name:
                if len(param.shape) >= 2:
                    # çº¿æ€§å±‚/å·ç§¯å±‚ä½¿ç”¨Xavieråˆå§‹åŒ–ï¼ˆæ ‡å‡†gainï¼‰
                    nn.init.xavier_uniform_(param, gain=1.0)  # âœ… æ”¹å›æ ‡å‡†gain=1.0
                else:
                    # 1Dæƒé‡ï¼ˆå¦‚attentionï¼‰ä½¿ç”¨æ­£æ€åˆ†å¸ƒ
                    nn.init.normal_(param, mean=0.0, std=0.1)  # âœ… å¢å¤§stdåˆ°0.1
            elif 'bias' in name:
                # âœ… åç½®åˆå§‹åŒ–ä¸º0ï¼ˆæ ‡å‡†åšæ³•ï¼‰
                nn.init.constant_(param, 0.0)
        
        logger.info("  âœ… å·²åº”ç”¨æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–ï¼ˆé˜²æ­¢NaNï¼‰")
    
    def forward(self, x, edge_index, batch=None):
        """
        å‰å‘ä¼ æ’­ï¼šæå–å›¾çº§é²æ£’ç‰¹å¾
        
        æ¶æ„ï¼š
        1. åˆ†ç¦»èŠ‚ç‚¹çº§ç‰¹å¾å’Œå›¾çº§ç‰¹å¾
        2. GATå¤„ç†èŠ‚ç‚¹çº§ç‰¹å¾ï¼ˆå­¦ä¹ èŠ‚ç‚¹é—´å…³ç³»ï¼‰
        3. å›¾çº§æ± åŒ–ï¼ˆå‡å€¼+æœ€å¤§+æ³¨æ„åŠ›ï¼‰
        4. æ‹¼æ¥å›¾çº§ç‰¹å¾ï¼ˆå‡ ä½•ç±»å‹ã€èŠ‚ç‚¹æ•°ç­‰ï¼‰
        5. èåˆå±‚è¾“å‡ºæœ€ç»ˆç‰¹å¾
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, 20]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            batch: æ‰¹æ¬¡ç´¢å¼•ï¼ˆå•å›¾æ¨ç†æ—¶ä¸ºNoneï¼‰
        
        Returns:
            output: å›¾çº§ç‰¹å¾ [output_dim] æˆ– [batch_size, output_dim]
        """
        from torch_geometric.nn import global_mean_pool, global_max_pool
        
        # âœ… æ­¥é¡¤1ï¼šåˆ†ç¦»èŠ‚ç‚¹çº§å’Œå›¾çº§ç‰¹å¾
        node_features = x[:, self.node_feature_dims]  # [num_nodes, 10]
        graph_features = x[:, self.graph_feature_dims]  # [num_nodes, 8] - æ¯ä¸ªèŠ‚ç‚¹éƒ½ç›¸åŒ
        
        # æå–å›¾çº§ç‰¹å¾ï¼ˆå–ç¬¬ä¸€ä¸ªèŠ‚ç‚¹å³å¯ï¼Œå› ä¸ºæ‰€æœ‰èŠ‚ç‚¹ç›¸åŒï¼‰
        if batch is None:
            # å•å›¾æ¨¡å¼
            graph_features_unique = graph_features[0]  # [8]
        else:
            # æ‰¹æ¬¡æ¨¡å¼ï¼šä»æ¯ä¸ªå›¾æå–ä¸€ä¸ªèŠ‚ç‚¹çš„å›¾çº§ç‰¹å¾
            batch_size = batch.max().item() + 1
            graph_features_list = []
            for i in range(batch_size):
                mask = (batch == i)
                graph_features_list.append(graph_features[mask][0])  # å–ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
            graph_features_unique = torch.stack(graph_features_list, dim=0)  # [batch_size, 8]
        
        # âœ… æ­¥é¡¤2ï¼šGATå¤„ç†èŠ‚ç‚¹çº§ç‰¹å¾
        # ç¬¬1å±‚ GATv2
        x1 = self.gat1(node_features, edge_index)
        if batch is None:
            norm_batch = torch.zeros(x1.size(0), dtype=torch.long, device=x1.device)
        else:
            norm_batch = batch.to(x1.device)
        # GraphNormæŒ‰å›¾å½’ä¸€åŒ–ï¼Œå…¼å®¹å•å›¾/å¤šå›¾
        x1 = self.gn1(x1, norm_batch)
        x1 = F.elu(x1)
        x1 = self.dropout(x1)
        
        # ç¬¬2å±‚ GATv2 + æ®‹å·®è¿æ¥
        x2 = self.gat2(x1, edge_index)
        x2 = self.gn2(x2, norm_batch)
        
        # æ·»åŠ æ®‹å·®ï¼ˆåŸºäºèŠ‚ç‚¹çº§ç‰¹å¾ï¼‰
        if self.residual_proj is not None:
            residual = self.residual_proj(node_features)
        else:
            residual = node_features
        x2 = F.elu(x2 + residual)
        
        # âœ… æ­¥é¡¤3ï¼šå›¾çº§æ± åŒ–ï¼ˆèŠ‚ç‚¹çº§ç‰¹å¾èšåˆï¼‰
        if batch is None:
            # å•å›¾æ¨ç†ï¼šç›´æ¥å¯¹æ‰€æœ‰èŠ‚ç‚¹æ± åŒ–
            mean_pool = torch.mean(x2, dim=0)
            max_pool, _ = torch.max(x2, dim=0)
            attn_weights = F.softmax(self.attention_pool(x2), dim=0)
            attn_pool = torch.sum(x2 * attn_weights, dim=0)
            
            # æ‹¼æ¥ä¸‰ç§æ± åŒ–ç»“æœ
            pooled_node_features = torch.cat([mean_pool, max_pool, attn_pool], dim=0)  # [hidden_dim*3]
            
            # âœ… æ­¥é¡¤4ï¼šç¼–ç å›¾çº§ç‰¹å¾
            graph_features_encoded = self.graph_encoder(graph_features_unique)  # [hidden_dim//2]
            
            # âœ… æ­¥é¡¤5ï¼šèåˆèŠ‚ç‚¹æ± åŒ– + å›¾çº§ç‰¹å¾
            final_features = torch.cat([pooled_node_features, graph_features_encoded], dim=0)
            output = self.fusion(final_features)
        else:
            # æ‰¹é‡æ¨ç†ï¼šå¯¹æ¯ä¸ªå›¾åˆ†åˆ«æ± åŒ–
            # 1. å‡å€¼æ± åŒ–
            mean_pool = global_mean_pool(x2, batch)
            
            # 2. æœ€å¤§æ± åŒ–
            max_pool = global_max_pool(x2, batch)
            
            # 3. æ³¨æ„åŠ›åŠ æƒæ± åŒ–ï¼ˆå¯¹æ¯ä¸ªå›¾åˆ†åˆ«è®¡ç®—ï¼‰
            attn_scores = self.attention_pool(x2)  # [num_nodes, 1]
            
            # æŒ‰å›¾åˆ†ç»„è®¡ç®—softmax
            batch_size = batch.max().item() + 1
            attn_pool_list = []
            for i in range(batch_size):
                mask = (batch == i)
                x_i = x2[mask]  # ç¬¬iä¸ªå›¾çš„èŠ‚ç‚¹ç‰¹å¾
                attn_i = attn_scores[mask]  # ç¬¬iä¸ªå›¾çš„æ³¨æ„åŠ›åˆ†æ•°
                attn_weights_i = F.softmax(attn_i, dim=0)
                attn_pool_i = torch.sum(x_i * attn_weights_i, dim=0)
                attn_pool_list.append(attn_pool_i)
            attn_pool = torch.stack(attn_pool_list, dim=0)  # [batch_size, hidden_dim]
            
            # æ‹¼æ¥ä¸‰ç§æ± åŒ–ç»“æœ
            pooled_node_features = torch.cat([mean_pool, max_pool, attn_pool], dim=1)  # [batch_size, hidden_dim*3]
            
            # âœ… æ­¥é¡¤4ï¼šç¼–ç å›¾çº§ç‰¹å¾
            graph_features_encoded = self.graph_encoder(graph_features_unique)  # [batch_size, hidden_dim//2]
            
            # âœ… æ­¥é¡¤5ï¼šèåˆèŠ‚ç‚¹æ± åŒ– + å›¾çº§ç‰¹å¾
            final_features = torch.cat([pooled_node_features, graph_features_encoded], dim=1)  # [batch_size, hidden_dim*3 + hidden_dim//2]
            output = self.fusion(final_features)  # [batch_size, output_dim]
        
        return output

# =============================================================
# æ¸©åº¦é€€ç«ä¸æ•°æ®å¢å¼º
# =============================================================


class AdaptiveTemperature:
    """æŒ‡æ•°é€€ç«æ¸©åº¦è°ƒåº¦ï¼šè½¯äºŒå€¼åŒ– â†’ ç¡¬äºŒå€¼åŒ–ã€‚"""
    
    def __init__(self, init_temp=1.0, final_temp=0.08, total_epochs=50):
        """
        åˆå§‹åŒ–æ¸©åº¦å‚æ•°
        
        Args:
            init_temp: åˆå§‹æ¸©åº¦ï¼ˆé»˜è®¤1.0ï¼Œè½¯äºŒå€¼åŒ–ï¼‰
            final_temp: æœ€ç»ˆæ¸©åº¦ï¼ˆé»˜è®¤0.08ï¼Œé¿å…è¿‡ç¡¬å¯¼è‡´æ¢¯åº¦æ¶ˆå¤±ï¼Œä¿æŒå­¦ä¹ èƒ½åŠ›ï¼‰
            total_epochs: æ€»è®­ç»ƒè½®æ•°
        """
        self.init_temp = init_temp
        self.final_temp = final_temp
        self.total_epochs = total_epochs
        
        logger.info(f"è‡ªé€‚åº”æ¸©åº¦åˆå§‹åŒ–:")
        logger.info(f"  åˆå§‹æ¸©åº¦: {init_temp} (è½¯äºŒå€¼åŒ–)")
        logger.info(f"  æœ€ç»ˆæ¸©åº¦: {final_temp} (ç¡¬äºŒå€¼åŒ–)")
        logger.info(f"  é€€ç«ç­–ç•¥: æŒ‡æ•°è¡°å‡")
    
    def get_temperature(self, epoch):
        """è¿”å›æŒ‡å®š epoch ä¸‹çš„æ¸©åº¦ï¼štemp = init * (final / init)^(t/T)ã€‚"""
        if epoch >= self.total_epochs:
            return self.final_temp
        
        # æŒ‡æ•°è¡°å‡
        progress = epoch / self.total_epochs  # 0 â†’ 1
        temp = self.init_temp * (self.final_temp / self.init_temp) ** progress
        
        return temp

def augment_graph_data(data, augment_prob=0.3, training=True):
    """åœ¨çº¿å›¾å¢å¼ºï¼šéšæœºæ‰§è¡Œå™ªå£°/è¾¹è£å‰ª/ç‰¹å¾é®ç½©/å¤åˆé“¾ã€‚"""
    if not training or random.random() > augment_prob:
        return data
    
    # â­ä¿®å¤ï¼šæ·»åŠ CUDAé”™è¯¯æ¢å¤ï¼Œé˜²æ­¢cloneå¤±è´¥å¯¼è‡´è®­ç»ƒå´©æºƒ
    try:
        data = data.clone()
    except (RuntimeError, torch.cuda.OutOfMemoryError) as e:
        # cloneå¤±è´¥æ—¶è¿”å›åŸå§‹æ•°æ®ï¼Œä¸è¿›è¡Œå¢å¼º
        logger.warning(f"âš ï¸ æ•°æ®å¢å¼ºcloneå¤±è´¥ï¼Œè·³è¿‡å¢å¼º: {e}")
        return data
    aug_type = random.choice(['vertex_noise', 'edge_drop', 'feature_mask', 'aug_chain'])
    
    if aug_type == 'vertex_noise':
        # â­ å™ªå£°å¢å¼ºï¼š30%æ¦‚ç‡ä½¿ç”¨æ›´å¼ºå™ªå£°ï¼Œåº”å¯¹Fig4å™ªå£°æ”»å‡»0.833çŸ­æ¿
        if random.random() < 0.3:  # 20% -> 30%
            noise_ratio = random.uniform(0.08, 0.18)  # æ‰©å¤§èŒƒå›´ï¼Œè´´è¿‘Fig4å¼ºåº¦
        else:
            noise_ratio = random.uniform(0.05, 0.10)
        noise = torch.randn_like(data.x) * noise_ratio
        data.x = data.x + noise
        
    elif aug_type == 'edge_drop':
        # æ¨¡æ‹Ÿè¾¹åˆ é™¤ï¼šéšæœºdrop 5-10%çš„è¾¹
        if data.edge_index.size(1) > 10:  # è‡³å°‘ä¿ç•™10æ¡è¾¹
            drop_ratio = random.uniform(0.05, 0.10)
            keep_prob = 1.0 - drop_ratio
            mask = torch.rand(data.edge_index.size(1), device=data.edge_index.device) > drop_ratio
            data.edge_index = data.edge_index[:, mask]
            
    elif aug_type == 'feature_mask':
        # æ¨¡æ‹Ÿç‰¹å¾ç¼ºå¤±ï¼šéšæœºmask 5-10%çš„ç‰¹å¾ç»´åº¦
        mask_ratio = random.uniform(0.05, 0.10)
        mask = torch.rand(data.x.size(1), device=data.x.device) > mask_ratio
        data.x = data.x * mask.float()
    elif aug_type == 'aug_chain':
        # å¤åˆå¢å¼ºï¼šé¡ºåºæ‰§è¡Œ2-3ç§è½»é‡å¢å¼ºï¼Œæ¨¡æ‹Ÿé“¾å¼æ”»å‡»çš„æ‰°åŠ¨åˆ†å¸ƒ
        chain_ops = ['vertex_noise', 'edge_drop', 'feature_mask']
        random.shuffle(chain_ops)
        k = random.choice([2, 3])
        for op in chain_ops[:k]:
            if op == 'vertex_noise':
                noise_ratio = random.uniform(0.03, 0.08)
                noise = torch.randn_like(data.x) * noise_ratio
                data.x = data.x + noise
            elif op == 'edge_drop':
                if data.edge_index.size(1) > 10:
                    drop_ratio = random.uniform(0.03, 0.08)
                    mask_e = torch.rand(data.edge_index.size(1), device=data.edge_index.device) > drop_ratio
                    data.edge_index = data.edge_index[:, mask_e]
            elif op == 'feature_mask':
                mask_ratio = random.uniform(0.03, 0.08)
                mask_f = torch.rand(data.x.size(1), device=data.x.device) > mask_ratio
                data.x = data.x * mask_f.float()
    
    return data


# =============================================================
# è®­ç»ƒå™¨ï¼šå¯¹æ¯”å­¦ä¹  + å¤šæŸå¤±è°ƒåº¦
# =============================================================


class ImprovedContrastiveTrainer:
    """
    æ”¹è¿›çš„å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨
    
    æ ¸å¿ƒä¼˜åŒ–ï¼š
    1. ä¿®å¤InfoNCEæŸå¤±
    2. æ·»åŠ äºŒå€¼åŒ–æ„ŸçŸ¥æŸå¤±
    3. åŠ¨æ€æŸå¤±æƒé‡
    4. éªŒè¯é›†è¯„ä¼°
    5. OneCycleLRå­¦ä¹ ç‡
    """
    
    def __init__(
        self,
        model,
        device='cpu',
        temperature=0.1,
        use_amp=True,
        batch_size=6,
        checkpoint_name='gat_checkpoint_latest.pth',
        model_prefix='IMPROVED',
        schedule_config: Optional[TrainingScheduleConfig] = None,
    ):
        self.model = model.to(device)
        self.device = device
        self.temperature = temperature  # å¢å¤§è‡³0.1ï¼Œå¢å¼ºæ•°å€¼ç¨³å®šæ€§ â­ä¿®å¤NaN
        self.use_amp = use_amp
        self.batch_size = batch_size
        self.checkpoint_name = checkpoint_name  # è‡ªå®šä¹‰checkpointæ–‡ä»¶å
        self.model_prefix = model_prefix  # æ¨¡å‹æ–‡ä»¶åå‰ç¼€ï¼ˆå¦‚'IMPROVED'æˆ–'Ablation1_NodeOnly'ï¼‰
        self.schedule = schedule_config or TrainingScheduleConfig()
        
        # ä¼˜åŒ–å™¨ï¼ˆAdamW with weight decayï¼‰
        base_lr = (self.schedule.onecycle_max_lr / self.schedule.onecycle_div_factor)
        self.optimizer = torch.optim.AdamW(
            model.parameters(), 
            lr=base_lr,
            weight_decay=0.01,
            betas=(0.9, 0.999)
        )
        self.base_lr = base_lr
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆç¨ååœ¨trainä¸­åˆå§‹åŒ–ï¼Œå› ä¸ºéœ€è¦çŸ¥é“steps_per_epochï¼‰
        self.scheduler = None
        
        # AMPç¼©æ”¾å™¨
        self.scaler = amp.GradScaler(enabled=self.use_amp)
        self.current_lr_multiplier = 1.0
        self.robust_warmup_state = None
        self.supcon_temp_override = None
        self.last_memory_refresh_epoch = None
        self.default_binary_temp_floor = 0.10
        self.binary_temp_floor = self.default_binary_temp_floor
        self.current_stage = "early"
        self.robust_warmup_triggered = False
        self.min_full_chain_per_batch = 1
        self.max_full_chain_per_batch = 2
        
        # è‡ªé€‚åº”æ¸©åº¦å‚æ•°ï¼ˆäºŒå€¼åŒ–æ„ŸçŸ¥æŸå¤±ï¼‰
        self.adaptive_temp = AdaptiveTemperature(
            init_temp=1.0,      # åˆå§‹æ¸©åº¦ï¼šè½¯äºŒå€¼åŒ–
            final_temp=0.08,    # æœ€ç»ˆæ¸©åº¦ï¼šç•¥å¾®æ”¾å®½ï¼Œå‡è½»è¿‡ç¡¬äºŒå€¼åŒ–å¯¹å”¯ä¸€æ€§çš„å½±å“
            total_epochs=20     # âœ… æ€»è½®æ•°ï¼ˆé»˜è®¤20ï¼Œä¼šåœ¨trainæ–¹æ³•ä¸­æ ¹æ®num_epochsæ›´æ–°ï¼‰
        )
        self.memory_refresh_marks = default_memory_refresh_marks(self.adaptive_temp.total_epochs)
        
        # è®­ç»ƒå†å²è®°å½•ï¼ˆâœ… æ·»åŠ æ‰€æœ‰éœ€è¦çš„é”®ï¼‰
        self.training_history = {
            'epoch_losses': [],
            'contrastive_losses': [],
            'similarity_losses': [],
            'diversity_losses': [],  # â­ ä¿®å¤ KeyError
            'uniqueness_losses': [],
            'binary_consistency_losses': [],  # â­ ä¿®å¤ç»˜å›¾é”™è¯¯
            'feature_stats': [],
            'gradient_norms': [],
            'oom_retries': [],
            'learning_rates': [],
            'temperatures': []
        }
        
        # âœ… åŠ¨æ€é»‘åå•ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­OOMçš„å›¾ä¼šè¢«åŠ å…¥
        self.dynamic_blacklist = set()
        self.ema_median = None
        self.ema_momentum = 0.99
        
        # Memory Bankæœºåˆ¶ï¼šç¼“è§£å°batchå¯¼è‡´çš„è´Ÿæ ·æœ¬ä¸è¶³
        self.memory_bank_size = 8192  # å­˜å‚¨æœ€è¿‘4096ä¸ªæ ·æœ¬çš„ç‰¹å¾
        self.memory_features = None  # [size, feature_dim]
        self.memory_labels = None     # [size]
        self.memory_ptr = 0           # å½“å‰æŒ‡é’ˆä½ç½®
        self.memory_initialized = False
        self.memory_seen_count = 0    # å·²å†™å…¥çš„æ€»æ ·æœ¬æ•°ï¼ˆç”¨äºæœ‰æ•ˆå¤§å°è®¡ç®—ï¼‰
        
        # åŸå‹å­—å…¸ï¼šä¸ºæ¯ä¸ªlabelç»´æŠ¤åŠ¨é‡æ›´æ–°çš„ç±»ä¸­å¿ƒ
        self.prototypes = {}  # {label: prototype_tensor}
        self.prototype_momentum = 0.95  # åŠ¨é‡ç³»æ•°
        
        logger.info("åˆ›å»ºæ”¹è¿›çš„å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨:")
        logger.info(f"  æ¸©åº¦å‚æ•°: {temperature}")
        logger.info(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        logger.info(f"  æ··åˆç²¾åº¦è®­ç»ƒ: {use_amp}")
        logger.info(f"  ä¼˜åŒ–å™¨: AdamW (lr=0.001, weight_decay=0.01)")
        logger.info("  Memory Bankå¤§å°: %s æ ·æœ¬", self.memory_bank_size)
        logger.info("  åŸå‹åŠ¨é‡: %.2f", self.prototype_momentum)
        logger.info("  ç›®çš„: ç¼“è§£å°batchè´Ÿæ ·æœ¬ä¸è¶³ï¼Œæ‰©å¤§éš¾ä¾‹è¦†ç›–")

    # =============================================================
    # æŸå¤±å‡½æ•°ä¸å·¥å…·
    # =============================================================

    def _apply_lr_multiplier(self):
        if self.scheduler is None:
            return
        for group in self.optimizer.param_groups:
            group['_last_scheduler_lr'] = group['lr']
        if abs(self.current_lr_multiplier - 1.0) < 1e-6:
            return
        for group in self.optimizer.param_groups:
            base_lr = group.get('_last_scheduler_lr', group['lr'])
            group['lr'] = base_lr * self.current_lr_multiplier

    def _clear_robust_overrides(self, reset_trigger: bool = False):
        self.current_lr_multiplier = 1.0
        self.supcon_temp_override = None
        self.binary_temp_floor = self.default_binary_temp_floor
        self.robust_warmup_state = None
        if reset_trigger:
            self.robust_warmup_triggered = False

    def _start_robust_warmup(self, epoch: int):
        if self.robust_warmup_triggered or self.schedule.robust_warmup_epochs <= 0:
            return
        self.robust_warmup_triggered = True
        self.robust_warmup_state = {
            'phase': 'boost',
            'remaining': self.schedule.robust_warmup_epochs,
        }
        self.current_lr_multiplier = self.schedule.robust_lr_boost
        self.supcon_temp_override = self.schedule.robust_supcon_temp
        self.binary_temp_floor = max(self.binary_temp_floor, 0.10)

    def _advance_robust_warmup(self):
        if not self.robust_warmup_state:
            return
        state = self.robust_warmup_state
        config = self.schedule
        if state['phase'] == 'boost':
            state['remaining'] -= 1
            if state['remaining'] <= 0:
                state['phase'] = 'decay'
                state['cooldown'] = max(1, config.robust_warmup_epochs)
        elif state['phase'] == 'decay':
            state['cooldown'] -= 1
            ratio = max(state['cooldown'], 0) / max(1, config.robust_warmup_epochs)
            base_temp = stage_temperature("late")
            self.current_lr_multiplier = 1.0 + (config.robust_lr_boost - 1.0) * ratio
            self.supcon_temp_override = base_temp + (config.robust_supcon_temp - base_temp) * ratio
            if state['cooldown'] <= 0:
                self._clear_robust_overrides()
        else:
            self._clear_robust_overrides()

    def _update_robust_phase_state(self, epoch: int) -> Tuple[str, float]:
        total_epochs = getattr(self, "total_epochs", 20)
        stage, progress = compute_stage_progress(epoch, total_epochs)
        self.current_stage = stage
        if stage == "late":
            if not self.robust_warmup_triggered:
                self._start_robust_warmup(epoch)
            self._advance_robust_warmup()
        else:
            self._clear_robust_overrides(reset_trigger=False)
        return stage, progress

    def _should_refresh_memory(self, epoch: int) -> bool:
        if not self.memory_initialized or self.memory_seen_count < self.memory_bank_size:
            return False
        if epoch in self.memory_refresh_marks:
            return True
        if getattr(self, "current_stage", None) == "late":
            interval = max(1, self.schedule.robust_memory_refresh_interval)
            if self.last_memory_refresh_epoch is None:
                return True
            if epoch - self.last_memory_refresh_epoch >= interval:
                return True
        return False

    def _refresh_memory_bank(self, epoch: int, keep_ratio: float = 0.5):
        if not self.memory_initialized:
            return
        valid_size = min(self.memory_seen_count, self.memory_bank_size)
        if valid_size == 0:
            return
        keep_ratio = max(0.1, min(0.95, keep_ratio))
        keep_count = max(1, int(valid_size * keep_ratio))
        keep_count = min(keep_count, valid_size)
        try:
            hardness_slice = self.memory_hardness[:valid_size]
            topk = torch.topk(hardness_slice, k=keep_count, largest=True)
            top_indices = topk.indices
            self.memory_features[:keep_count] = self.memory_features[top_indices].clone()
            self.memory_labels[:keep_count] = self.memory_labels[top_indices].clone()
            self.memory_hardness[:keep_count] = hardness_slice[top_indices].clone()
            self.memory_ptr = keep_count
            self.memory_seen_count = keep_count
            self.last_memory_refresh_epoch = epoch
            logger.info(
                f"ğŸ”„ Memory Bankåˆ·æ–°: ä¿ç•™ {keep_count}/{valid_size} ä¸ªéš¾æ ·æœ¬ (keep_ratio={keep_ratio:.2f}), epoch={epoch}"
            )
        except Exception as err:
            logger.error(f"Memory Bankåˆ·æ–°å¤±è´¥: {err}")
            logger.error(traceback.format_exc())

    def contrastive_loss_fixed(self, features_original, features_attacked, labels):
        """æ•°å€¼ç¨³å®šç‰ˆ InfoNCEï¼ˆlog-sum-exp + è£å‰ªï¼‰ã€‚"""
        # æ£€æŸ¥è¾“å…¥ç‰¹å¾æ˜¯å¦å¼‚å¸¸ 
        if torch.isnan(features_original).any() or torch.isinf(features_original).any():
            logger.error("InfoNCE: åŸå§‹ç‰¹å¾åŒ…å«NaN/Inf")
            logger.error(f"   NaNæ•°é‡: {torch.isnan(features_original).sum().item()}")
            logger.error(f"   Infæ•°é‡: {torch.isinf(features_original).sum().item()}")
            logger.error(f"   ç‰¹å¾èŒƒå›´: [{features_original.min().item():.4f}, {features_original.max().item():.4f}]")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        if torch.isnan(features_attacked).any() or torch.isinf(features_attacked).any():
            logger.error("InfoNCE: æ”»å‡»ç‰¹å¾åŒ…å«NaN/Inf")
            logger.error(f"   NaNæ•°é‡: {torch.isnan(features_attacked).sum().item()}")
            logger.error(f"   Infæ•°é‡: {torch.isinf(features_attacked).sum().item()}")
            logger.error(f"   ç‰¹å¾èŒƒå›´: [{features_attacked.min().item():.4f}, {features_attacked.max().item():.4f}]")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # L2å½’ä¸€åŒ–
        features_original = F.normalize(features_original, p=2, dim=1)
        features_attacked = F.normalize(features_attacked, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼ˆå·²é™¤ä»¥æ¸©åº¦ï¼‰
        sim_matrix = torch.matmul(features_original, features_attacked.T) / self.temperature
        
        # è®°å½•ç›¸ä¼¼åº¦çŸ©é˜µç»Ÿè®¡ä¿¡æ¯ç”¨äºè¯Šæ–­ â­è¯Šæ–­
        sim_min, sim_max = sim_matrix.min().item(), sim_matrix.max().item()
        if abs(sim_min) > 40 or abs(sim_max) > 40:
            logger.warning(f"InfoNCE: ç›¸ä¼¼åº¦çŸ©é˜µæ•°å€¼è¾ƒå¤§ [min={sim_min:.2f}, max={sim_max:.2f}]")
        
        # è£å‰ªé˜²æ­¢æº¢å‡ºï¼ˆé™åˆ¶åœ¨åˆç†èŒƒå›´ï¼‰â­æ•°å€¼ç¨³å®šæ€§
        sim_matrix = torch.clamp(sim_matrix, min=-50, max=50)
        
        # åˆ›å»ºæ­£æ ·æœ¬mask
        labels = labels.contiguous().view(-1, 1)
        mask = torch.eq(labels, labels.T).float().to(self.device)
        
        # å¯¹è§’çº¿maskï¼ˆæ’é™¤è‡ªå·±ä¸è‡ªå·±ï¼‰
        batch_size = features_original.size(0)
        logits_mask = torch.scatter(
            torch.ones_like(mask),
            1,
            torch.arange(batch_size).view(-1, 1).to(self.device),
            0
        )
        mask = mask * logits_mask
        
        # ä½¿ç”¨log-sum-expæŠ€å·§è®¡ç®—ï¼ˆæ•°å€¼ç¨³å®šï¼‰â­ä¿®å¤NaN
        # log_prob = log(exp(sim_ij) / sum(exp(sim_ik)))
        #          = sim_ij - log(sum(exp(sim_ik)))
        #          = sim_ij - logsumexp(sim_i)
        max_sim = torch.max(sim_matrix, dim=1, keepdim=True)[0]
        exp_logits = torch.exp(sim_matrix - max_sim) * logits_mask
        log_sum_exp = max_sim + torch.log(exp_logits.sum(1, keepdim=True) + 1e-9)
        log_prob = sim_matrix - log_sum_exp
        
        # åªå¯¹æ­£æ ·æœ¬è®¡ç®—æŸå¤±
        mean_log_prob_pos = (mask * log_prob).sum(1) / (mask.sum(1) + 1e-9)
        
        loss = -mean_log_prob_pos.mean()
        
        # âœ… è¯Šæ–­ï¼šæ£€æŸ¥æ­£æ ·æœ¬å¯¹æ•°é‡ï¼ˆåªåœ¨è®­ç»ƒå¼€å§‹æ—¶è¾“å‡ºä¸€æ¬¡ï¼‰
        num_positive_pairs = mask.sum().item()
        if not hasattr(self, '_infonce_first_check'):
            self._infonce_first_check = True
            logger.info("InfoNCEæŸå¤±è¯Šæ–­ï¼ˆé¦–æ¬¡ï¼‰:")
            logger.info(f"   Batchå¤§å°: {features_original.size(0)}")
            logger.info(f"   æ­£æ ·æœ¬å¯¹æ•°é‡: {num_positive_pairs}")
            logger.info(f"   æŸå¤±å€¼: {loss.item():.4f}")
            if num_positive_pairs == 0:
                logger.warning("   è­¦å‘Šï¼šæ²¡æœ‰æ­£æ ·æœ¬å¯¹ï¼Œæ£€æŸ¥åˆ†ç»„é‡‡æ ·")
        
        # å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœlossæ˜¯NaNæˆ–Infï¼Œè¿”å›0é¿å…ä¼ æ’­ â­è¯Šæ–­å¢å¼º
        if torch.isnan(loss) or torch.isinf(loss):
            logger.error(f"InfoNCEæŸå¤±å¼‚å¸¸: {loss.item()}")
            logger.error(f"   ç›¸ä¼¼åº¦çŸ©é˜µèŒƒå›´: [{sim_min:.4f}, {sim_max:.4f}]")
            logger.error(f"   log_probèŒƒå›´: [{log_prob.min().item():.4f}, {log_prob.max().item():.4f}]")
            logger.error(f"   mean_log_prob_pos: {mean_log_prob_pos}")
            logger.error(f"   batch_size: {features_original.size(0)}")
            logger.error(f"   æ­£æ ·æœ¬å¯¹æ•°é‡: {num_positive_pairs}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        return loss

    def intra_class_alignment_loss_weighted(self, features, labels, sample_weights):
        """
        åŠ æƒåŒç±»å¯¹é½æŸå¤±ï¼šå¯¹å¤åˆ/é“¾å¼æ”»å‡»æ ·æœ¬ç»™äºˆæ›´é«˜å¯¹é½æƒé‡ã€‚
        sample_weights: [N] æ¯ä¸ªæ ·æœ¬çš„æƒé‡ï¼ˆåŸå›¾=1.0ï¼Œcomboâ‰ˆ1.4ï¼Œfull_chainâ‰ˆ1.6ï¼‰
        """
        if features.size(0) < 2:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        feats = F.normalize(features, p=2, dim=1)
        sim = torch.matmul(feats, feats.T)
        N = sim.size(0)
        same = (labels.unsqueeze(1) == labels.unsqueeze(0))
        diag = torch.eye(N, device=sim.device, dtype=torch.bool)
        pos_mask = same & (~diag)
        if not pos_mask.any():
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        w = sample_weights.view(-1, 1)
        pair_w = w @ w.T
        pos_w = pair_w[pos_mask].view(-1)
        pos_sim = sim[pos_mask].view(-1)
        pos_dist = (1.0 - pos_sim)
        k = int(max(0, int(0.3 * pos_dist.numel())))
        if k > 0:
            top_vals, top_idx = torch.topk(pos_dist, k, largest=True)
            pos_w = pos_w.clone()
            pos_w[top_idx] = pos_w[top_idx] * 2.0
        loss = (pos_w * pos_dist).sum() / (pos_w.sum() + 1e-8)
        if torch.isnan(loss) or torch.isinf(loss):
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        return loss
    
    def diversity_loss(self, features):
        """
        å¤šæ ·æ€§æŸå¤±ï¼ˆè½»é‡ç‰ˆï¼‰ï¼šé˜²æ­¢ç‰¹å¾åå¡Œ
        
        åªåŒ…å«åŸºç¡€çš„æ–¹å·®å’Œå»ç›¸å…³çº¦æŸï¼Œä¸åŒ…å«åŒºåˆ†åº¦çº¦æŸï¼ˆç”±uniqueness_lossè´Ÿè´£ï¼‰
        """
        # â­æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(features).any() or torch.isinf(features).any():
            logger.error(f"ğŸ”´ diversity_lossè¾“å…¥å¼‚å¸¸ï¼")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # 1. ç‰¹å¾æ–¹å·®çº¦æŸï¼ˆæ¯ä¸ªç»´åº¦éƒ½åº”è¯¥æœ‰è¶³å¤Ÿçš„å˜åŒ–ï¼‰
        feature_var = torch.var(features, dim=0) + 1e-8
        var_loss = torch.mean(torch.relu(0.1 - feature_var))
        
        if torch.isnan(var_loss) or torch.isinf(var_loss):
            logger.error(f"ğŸ”´ var_losså¼‚å¸¸: {var_loss.item()}")
            var_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # 2. ç‰¹å¾å»ç›¸å…³ï¼ˆé˜²æ­¢æ‰€æœ‰ç»´åº¦éƒ½ç¼–ç ç›¸åŒçš„ä¿¡æ¯ï¼‰
        features_centered = features - features.mean(dim=0, keepdim=True)
        
        if torch.all(torch.abs(features_centered) < 1e-7):
            decorr_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        else:
            cov_matrix = torch.matmul(features_centered.T, features_centered) / (features.size(0) + 1e-8)
            identity = torch.eye(cov_matrix.size(0), device=cov_matrix.device)
            decorr_loss = torch.mean((cov_matrix - identity * cov_matrix.diagonal().unsqueeze(1)) ** 2)
            
            if torch.isnan(decorr_loss) or torch.isinf(decorr_loss):
                logger.error(f"ğŸ”´ decorr_losså¼‚å¸¸: {decorr_loss.item()}")
                decorr_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        
        total_diversity_loss = var_loss + 0.1 * decorr_loss
        
        if torch.isnan(total_diversity_loss) or torch.isinf(total_diversity_loss):
            logger.error(f"ğŸ”´ total_diversity_losså¼‚å¸¸: {total_diversity_loss.item()}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        return total_diversity_loss
    
    def update_memory_bank(self, features, labels, use_hard_mining=True):
        """ç»´æŠ¤Memory Bankï¼šå†·å¯åŠ¨å†™å…¥+åæœŸéš¾æ ·æœ¬æ›¿æ¢ã€‚"""
        batch_size = features.size(0)
        feature_dim = features.size(1)
        
        # åˆå§‹åŒ–memory bank
        if not self.memory_initialized:
            self.memory_features = torch.zeros(self.memory_bank_size, feature_dim, 
                                              device=self.device, dtype=features.dtype)
            self.memory_labels = torch.zeros(self.memory_bank_size, 
                                            device=self.device, dtype=labels.dtype)
            self.memory_hardness = torch.zeros(self.memory_bank_size, device=self.device)
            self.memory_initialized = True
            self.memory_seen_count = 0
        
        # å½’ä¸€åŒ–ç‰¹å¾ï¼ˆç”¨äºè®¡ç®—ç›¸ä¼¼åº¦ï¼‰
        features_norm = F.normalize(features.detach(), p=2, dim=1)
        
        # ===== é˜¶æ®µ1ï¼šMemory Bankæœªæ»¡ï¼Œç›´æ¥å­˜å‚¨ï¼ˆå†·å¯åŠ¨ï¼‰ =====
        if self.memory_seen_count < self.memory_bank_size:
            space_left = self.memory_bank_size - self.memory_seen_count
            actual_batch_size = min(batch_size, space_left)
            
            end_ptr = self.memory_ptr + actual_batch_size
            if end_ptr <= self.memory_bank_size:
                # æ­£å¸¸æƒ…å†µï¼šä¸è·¨è¾¹ç•Œ
                self.memory_features[self.memory_ptr:end_ptr] = features[:actual_batch_size].detach()
                self.memory_labels[self.memory_ptr:end_ptr] = labels[:actual_batch_size]
                if use_hard_mining:
                    hardness_scores = self._compute_hardness_scores(features_norm[:actual_batch_size], labels[:actual_batch_size])
                    self.memory_hardness[self.memory_ptr:end_ptr] = hardness_scores
            else:
                # è·¨è¶Šè¾¹ç•Œ
                first_part = self.memory_bank_size - self.memory_ptr
                self.memory_features[self.memory_ptr:] = features[:first_part].detach()
                self.memory_labels[self.memory_ptr:] = labels[:first_part]
                if use_hard_mining:
                    hardness_scores = self._compute_hardness_scores(features_norm[:actual_batch_size], labels[:actual_batch_size])
                    self.memory_hardness[self.memory_ptr:] = hardness_scores[:first_part]
                
                second_part = actual_batch_size - first_part
                if second_part > 0:
                    self.memory_features[:second_part] = features[first_part:actual_batch_size].detach()
                    self.memory_labels[:second_part] = labels[first_part:actual_batch_size]
                    if use_hard_mining:
                        self.memory_hardness[:second_part] = hardness_scores[first_part:]
            
            self.memory_ptr = (self.memory_ptr + actual_batch_size) % self.memory_bank_size
            self.memory_seen_count = min(self.memory_seen_count + actual_batch_size, self.memory_bank_size)
        
        # ===== é˜¶æ®µ2ï¼šMemory Bankå·²æ»¡ï¼Œéš¾è´Ÿæ ·æœ¬æ›¿æ¢ç­–ç•¥ =====
        else:
            if use_hard_mining:
                # è®¡ç®—å½“å‰batchçš„éš¾åº¦åˆ†æ•°
                new_hardness = self._compute_hardness_scores(features_norm, labels)
                
                # æ‰¾å‡ºå½“å‰batchä¸­çš„"éš¾æ ·æœ¬"ï¼ˆéš¾åº¦åˆ†æ•°é«˜äºMemory Bankå¹³å‡å€¼ï¼‰
                memory_avg_hardness = self.memory_hardness[:self.memory_seen_count].mean()
                hard_samples_mask = new_hardness > memory_avg_hardness
                
                if hard_samples_mask.any():
                    hard_features = features[hard_samples_mask].detach()
                    hard_labels = labels[hard_samples_mask]
                    hard_scores = new_hardness[hard_samples_mask]
                    num_hard = hard_samples_mask.sum().item()
                    
                    # æ‰¾å‡ºMemory Bankä¸­æœ€ç®€å•çš„æ ·æœ¬ï¼ˆæ›¿æ¢å®ƒä»¬ï¼‰
                    easy_indices = torch.topk(self.memory_hardness, k=num_hard, largest=False).indices
                    
                    # æ›¿æ¢
                    self.memory_features[easy_indices] = hard_features
                    self.memory_labels[easy_indices] = hard_labels
                    self.memory_hardness[easy_indices] = hard_scores
                else:
                    # å½“å‰batchéƒ½æ˜¯ç®€å•æ ·æœ¬ï¼Œéšæœºæ›¿æ¢ä¸€éƒ¨åˆ†ï¼ˆä¿æŒå¤šæ ·æ€§ï¼‰
                    replace_indices = torch.randperm(self.memory_bank_size, device=self.device)[:batch_size]
                    self.memory_features[replace_indices] = features.detach()
                    self.memory_labels[replace_indices] = labels
                    self.memory_hardness[replace_indices] = new_hardness
            else:
                # ä¸ä½¿ç”¨éš¾æ ·æœ¬æŒ–æ˜ï¼Œä½¿ç”¨åŸæ¥çš„å¾ªç¯é˜Ÿåˆ—ï¼ˆå·²ä¿®å¤è¾¹ç•Œæ£€æŸ¥ï¼‰
                end_ptr = self.memory_ptr + batch_size
                if end_ptr <= self.memory_bank_size:
                    # æ­£å¸¸æƒ…å†µï¼šä¸è·¨è¾¹ç•Œ
                    self.memory_features[self.memory_ptr:end_ptr] = features.detach()
                    self.memory_labels[self.memory_ptr:end_ptr] = labels
                else:
                    # è·¨è¶Šè¾¹ç•Œ
                    first_part = self.memory_bank_size - self.memory_ptr
                    if first_part > 0:
                        self.memory_features[self.memory_ptr:] = features[:first_part].detach()
                        self.memory_labels[self.memory_ptr:] = labels[:first_part]
                    
                    second_part = batch_size - first_part
                    if second_part > 0:
                        self.memory_features[:second_part] = features[first_part:].detach()
                        self.memory_labels[:second_part] = labels[first_part:]
                
                self.memory_ptr = (self.memory_ptr + batch_size) % self.memory_bank_size
    
    def _compute_hardness_scores(self, features_norm, labels):
        """è®¡ç®—æ ·æœ¬è·¨ç±»æœ€é«˜ç›¸ä¼¼åº¦ï¼Œå¾—åˆ†è¶Šé«˜è¡¨ç¤ºè¶Šéš¾ã€‚"""
        batch_size = features_norm.size(0)
        hardness_scores = torch.zeros(batch_size, device=self.device)
        
        # å¯¹æ¯ä¸ªæ ·æœ¬ï¼Œè®¡ç®—å…¶ä¸ä¸åŒç±»æ ·æœ¬çš„æœ€å¤§ç›¸ä¼¼åº¦
        for i in range(batch_size):
            current_label = labels[i]
            current_feat = features_norm[i:i+1]
            
            # æ‰¾åˆ°æ‰€æœ‰ä¸åŒç±»çš„æ ·æœ¬
            cross_label_mask = (labels != current_label)
            if cross_label_mask.any():
                cross_features = features_norm[cross_label_mask]
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarities = torch.mm(current_feat, cross_features.t()).squeeze()
                # æœ€é«˜ç›¸ä¼¼åº¦ = éš¾åº¦åˆ†æ•°
                hardness_scores[i] = similarities.max() if similarities.numel() > 0 else 0.0
            else:
                hardness_scores[i] = 0.0
        
        return hardness_scores
    
    def update_prototypes(self, features, labels):
        """ä¸ºæ¯ä¸ªlabelç»´æŠ¤åŠ¨é‡åŸå‹ï¼Œé¿å…ç‰¹å¾æ¼‚ç§»ã€‚"""
        unique_labels = labels.unique()
        
        for label in unique_labels:
            label_key = label.item()
            mask = (labels == label)
            
            # è®¡ç®—å½“å‰batchä¸­è¯¥labelçš„å¹³å‡ç‰¹å¾
            current_proto = features[mask].mean(dim=0).detach()
            
            # åŠ¨é‡æ›´æ–°
            if label_key in self.prototypes:
                self.prototypes[label_key] = (
                    self.prototype_momentum * self.prototypes[label_key] + 
                    (1 - self.prototype_momentum) * current_proto
                )
            else:
                self.prototypes[label_key] = current_proto
    
    def supervised_contrastive_loss_with_memory(self, features, labels, temperature=0.07, epoch=None):
        """å¸¦Memory Bankä¸åŸå‹å¯¹æ¯”çš„SupConæŸå¤±ã€‚"""
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(features).any() or torch.isinf(features).any():
            logger.error(f"ğŸ”´ supervised_contrastive_loss_with_memoryè¾“å…¥å¼‚å¸¸ï¼")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        batch_size = features.size(0)
        if batch_size < 2:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # å½’ä¸€åŒ–ç‰¹å¾
        features_norm = F.normalize(features, p=2, dim=1)
        
        # ===== éƒ¨åˆ†1ï¼šä¸Memory Bankä¸­çš„æ ·æœ¬å¯¹æ¯” =====
        memory_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        
        if self.memory_initialized and self.memory_seen_count > batch_size:
            # è·å–æœ‰æ•ˆçš„memory bankå†…å®¹ï¼ˆå·²å¡«å……çš„éƒ¨åˆ†ï¼‰
            valid_size = min(self.memory_seen_count, self.memory_bank_size)
            memory_features_valid = self.memory_features[:valid_size]
            memory_labels_valid = self.memory_labels[:valid_size]
            
            # å½’ä¸€åŒ–memoryç‰¹å¾
            memory_features_norm = F.normalize(memory_features_valid, p=2, dim=1)
            
            # è®¡ç®—ä¸memoryçš„ç›¸ä¼¼åº¦ [batch_size, memory_size]
            sim_to_memory = torch.matmul(features_norm, memory_features_norm.T) / temperature
            
            # æ„å»ºmaskï¼šå“ªäº›memoryæ ·æœ¬ä¸å½“å‰æ ·æœ¬åŒlabel
            labels_expanded = labels.unsqueeze(1)  # [batch_size, 1]
            memory_labels_expanded = memory_labels_valid.unsqueeze(0)  # [1, memory_size]
            mask_positive_memory = (labels_expanded == memory_labels_expanded).float()  # [batch_size, memory_size]
            
            # å¯¹æ¯ä¸ªanchorè®¡ç®—æŸå¤±
            losses_memory = []
            for i in range(batch_size):
                pos_mask = mask_positive_memory[i]
                num_positives = pos_mask.sum()
                
                if num_positives < 1:
                    continue  # è¯¥labelåœ¨memoryä¸­æ²¡æœ‰æ ·æœ¬
                
                # log-sum-expæŠ€å·§
                logits = sim_to_memory[i]
                logits_max, _ = torch.max(logits, dim=0, keepdim=True)
                logits = logits - logits_max.detach()
                
                exp_logits = torch.exp(logits)
                log_denominator = torch.log(exp_logits.sum() + 1e-8)
                log_numerator = torch.log((exp_logits * pos_mask).sum() + 1e-8)
                
                loss = log_denominator - log_numerator
                losses_memory.append(loss)
            
            if len(losses_memory) > 0:
                memory_loss = torch.mean(torch.stack(losses_memory))
        
        # ===== éƒ¨åˆ†2ï¼šä¸å½“å‰batchå†…æ ·æœ¬å¯¹æ¯”ï¼ˆåŸå§‹SupConï¼‰ =====
        batch_loss = self.supervised_contrastive_loss(features, labels, temperature)
        
        # ===== éƒ¨åˆ†3ï¼šä¸æ‰€æœ‰åŸå‹å¯¹æ¯”ï¼ˆç¡®ä¿è¦†ç›–æ‰€æœ‰ç±»åˆ«ï¼‰ =====
        prototype_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        
        if len(self.prototypes) >= 2:
            # è·å–æ‰€æœ‰åŸå‹
            proto_labels = list(self.prototypes.keys())
            proto_features = torch.stack([self.prototypes[k] for k in proto_labels]).to(self.device)
            proto_features_norm = F.normalize(proto_features, p=2, dim=1)
            
            # è®¡ç®—ä¸åŸå‹çš„ç›¸ä¼¼åº¦ [batch_size, num_prototypes]
            sim_to_proto = torch.matmul(features_norm, proto_features_norm.T) / temperature
            
            # æ„å»ºmask
            proto_labels_tensor = torch.tensor(proto_labels, device=self.device)
            labels_expanded = labels.unsqueeze(1)
            proto_labels_expanded = proto_labels_tensor.unsqueeze(0)
            mask_positive_proto = (labels_expanded == proto_labels_expanded).float()
            
            # è®¡ç®—æŸå¤±
            losses_proto = []
            for i in range(batch_size):
                pos_mask = mask_positive_proto[i]
                num_positives = pos_mask.sum()
                
                if num_positives < 1:
                    continue
                
                logits = sim_to_proto[i]
                logits_max, _ = torch.max(logits, dim=0, keepdim=True)
                logits = logits - logits_max.detach()
                
                exp_logits = torch.exp(logits)
                log_denominator = torch.log(exp_logits.sum() + 1e-8)
                log_numerator = torch.log((exp_logits * pos_mask).sum() + 1e-8)
                
                loss = log_denominator - log_numerator
                losses_proto.append(loss)
            
            if len(losses_proto) > 0:
                prototype_loss = torch.mean(torch.stack(losses_proto))
        
        # ç»„åˆä¸‰éƒ¨åˆ†æŸå¤±ï¼ˆbatchå†… + memory + prototypeï¼‰
        # æ—©æœŸè¿›è¡Œramp-upï¼Œé™ä½memory/prototypeå¸¦æ¥çš„å™ªå£°å½±å“ï¼ˆå‰5ä¸ªepochçº¿æ€§ä¸Šå‡ï¼‰
        if epoch is None:
            ramp = 1.0
        else:
            ramp = min(1.0, (epoch + 1) / 5.0)
        memory_w_base = 0.3
        proto_w_base = 0.1
        total_loss = batch_loss + (memory_w_base * ramp) * memory_loss + (proto_w_base * ramp) * prototype_loss
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            logger.error(f"ğŸ”´ supervised_contrastive_loss_with_memoryå¼‚å¸¸: {total_loss.item()}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        return total_loss
    
    def supervised_contrastive_loss(self, features, labels, temperature=0.07):
        """
        â­â­â­â­â­ ç›‘ç£å¯¹æ¯”æŸå¤±ï¼ˆSupervised Contrastive Lossï¼‰- æ ¹æœ¬è§£å†³æ–¹æ¡ˆ
        
        è®ºæ–‡ï¼šSupervised Contrastive Learning (Khosla et al., NeurIPS 2020)
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        - åŒlabelçš„æ ·æœ¬èšé›†ï¼ˆé²æ£’æ€§ï¼šåŒåŸå›¾çš„æ”»å‡»ç‰ˆæœ¬åº”ç›¸ä¼¼ï¼‰
        - ä¸åŒlabelçš„æ ·æœ¬åˆ†ç¦»ï¼ˆå”¯ä¸€æ€§ï¼šä¸åŒåŸå›¾åº”ä¸åŒï¼‰
        - ç»Ÿä¸€çš„ä¼˜åŒ–ç›®æ ‡ï¼Œé¿å…InfoNCE+uniquenessçš„å†²çª
        
        ä¼˜åŠ¿ï¼š
        1. ç›´æ¥æ„å»ºåˆ¤åˆ«æ€§ç‰¹å¾ç©ºé—´
        2. åˆ©ç”¨batchä¸­æ‰€æœ‰åŒlabelæ ·æœ¬ï¼ˆæ›´å¼ºçš„é²æ£’æ€§ä¿¡å·ï¼‰
        3. æ˜¾å¼æ¨è¿œä¸åŒlabelæ ·æœ¬ï¼ˆæ›´å¥½çš„å”¯ä¸€æ€§ï¼‰
        4. ç†è®ºåŸºç¡€æ‰å®ï¼Œåœ¨å›¾åƒ/NLPé¢†åŸŸå·²å¹¿æ³›éªŒè¯
        
        å‚æ•°ï¼š
        - features: [N, D] ç‰¹å¾å‘é‡
        - labels: [N] åŸå›¾IDï¼ˆåŒåŸå›¾çš„ä¸åŒæ”»å‡»ç‰ˆæœ¬åº”æœ‰ç›¸åŒlabelï¼‰
        - temperature: æ¸©åº¦å‚æ•°ï¼ˆæ¨è0.07-0.1ï¼‰
        
        é¢„æœŸæ•ˆæœï¼š
        - é²æ£’æ€§ï¼šåŒå›¾æ”»å‡»NC > 0.95ï¼ˆåˆ©ç”¨æ‰€æœ‰åŒlabelæ ·æœ¬ï¼‰
        - å”¯ä¸€æ€§ï¼šè·¨å›¾NC < 0.60ï¼ˆæ˜¾å¼æ¨è¿œä¸åŒlabelï¼‰
        """
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(features).any() or torch.isinf(features).any():
            logger.error(f"ğŸ”´ supervised_contrastive_lossè¾“å…¥å¼‚å¸¸ï¼")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        batch_size = features.size(0)
        if batch_size < 2:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # å½’ä¸€åŒ–ç‰¹å¾ï¼ˆL2å½’ä¸€åŒ–ï¼‰
        features = F.normalize(features, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µï¼šS[i,j] = features[i] Â· features[j]
        similarity_matrix = torch.matmul(features, features.T) / temperature
        
        # æ„å»ºmask
        labels = labels.contiguous().view(-1, 1)
        mask_positive = torch.eq(labels, labels.T).float().to(self.device)  # åŒlabelä¸º1
        mask_anchor = torch.eye(batch_size, dtype=torch.bool, device=self.device)  # å¯¹è§’çº¿
        
        # æ’é™¤è‡ªå·±
        mask_positive = mask_positive * (~mask_anchor).float()
        
        # è®¡ç®—æ¯ä¸ªanchorçš„æŸå¤±
        losses = []
        for i in range(batch_size):
            # è¯¥anchorçš„æ­£æ ·æœ¬mask
            pos_mask = mask_positive[i]
            num_positives = pos_mask.sum()
            
            # å¦‚æœæ²¡æœ‰æ­£æ ·æœ¬ï¼ˆbatchä¸­åªæœ‰è¯¥åŸå›¾çš„ä¸€ä¸ªæ ·æœ¬ï¼‰ï¼Œè·³è¿‡
            if num_positives < 1:
                continue
            
            # å¯¹æ•°-æ±‚å’Œ-æŒ‡æ•°æŠ€å·§ï¼ˆlog-sum-exp trickï¼‰é˜²æ­¢æ•°å€¼æº¢å‡º
            # log(Î£exp(x)) = max(x) + log(Î£exp(x - max(x)))
            logits = similarity_matrix[i]
            logits_max, _ = torch.max(logits, dim=0, keepdim=True)
            logits = logits - logits_max.detach()  # æ•°å€¼ç¨³å®š
            
            # è®¡ç®—åˆ†æ¯ï¼šÎ£exp(anchorÂ·all) - exp(anchorÂ·anchor)
            exp_logits = torch.exp(logits)
            # â­â­â­ ä¿®å¤ï¼šä½¿ç”¨maskæ’é™¤è‡ªå·±ï¼Œé¿å…in-placeæ“ä½œç ´åæ¢¯åº¦å›¾
            # åˆ›å»ºæ’é™¤è‡ªå·±çš„maskï¼ˆéin-placeæ–¹å¼ï¼‰
            indices = torch.arange(batch_size, device=exp_logits.device)
            mask_exclude_self = (indices != i).float()
            exp_logits_masked = exp_logits * mask_exclude_self
            log_denominator = torch.log(exp_logits_masked.sum() + 1e-8)
            
            # è®¡ç®—åˆ†å­ï¼šÎ£exp(anchorÂ·positives)ï¼ˆä¹Ÿä½¿ç”¨maskedç‰ˆæœ¬ï¼‰
            log_numerator = torch.log((exp_logits_masked * pos_mask).sum() + 1e-8)
            
            # Loss = -log(æ­£æ ·æœ¬å æ¯”) = log(åˆ†æ¯) - log(åˆ†å­)
            loss = log_denominator - log_numerator
            losses.append(loss)
        
        if len(losses) == 0:
            logger.warning(f"âš ï¸ batchä¸­æ‰€æœ‰æ ·æœ¬éƒ½æ²¡æœ‰æ­£æ ·æœ¬å¯¹ï¼ŒSupConæŸå¤±æ— æ³•è®¡ç®—")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        total_loss = torch.mean(torch.stack(losses))
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            logger.error(f"ğŸ”´ supervised_contrastive_losså¼‚å¸¸: {total_loss.item()}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        return total_loss
    
    def prototype_loss(self, features, labels):
        """
        â­â­â­ åŸå‹æŸå¤±ï¼ˆPrototype Lossï¼‰- è¾…åŠ©SupConçš„å¼ºåŒ–æœºåˆ¶
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        - æ¯ä¸ªåŸå›¾å­¦ä¹ ä¸€ä¸ª"åŸå‹ä¸­å¿ƒ"ï¼ˆprototypeï¼‰
        - åŒåŸå›¾çš„æ‰€æœ‰æ ·æœ¬è¢«æ‹‰å‘å…¶åŸå‹
        - ä¸åŒåŸå›¾çš„åŸå‹è¢«æ¨è¿œ
        
        ä¼˜åŠ¿ï¼š
        1. é˜²æ­¢ç‰¹å¾æ¼‚ç§»ï¼ˆåŸå‹ä½œä¸ºé”šç‚¹ï¼‰
        2. æ›´ç¨³å®šçš„ç‰¹å¾ç©ºé—´
        3. å¼ºåŒ–SupConçš„èšç±»æ•ˆæœ
        
        å®ç°ï¼š
        - Intra-loss: ç‰¹å¾æ¥è¿‘è‡ªå·±çš„åŸå‹ï¼ˆèšé›†ï¼‰
        - Inter-loss: åŸå‹ä¹‹é—´è¿œç¦»ï¼ˆåˆ†ç¦»ï¼‰
        """
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(features).any() or torch.isinf(features).any():
            logger.error(f"ğŸ”´ prototype_lossè¾“å…¥å¼‚å¸¸ï¼")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # è®¡ç®—æ¯ä¸ªlabelçš„åŸå‹ï¼ˆç‰¹å¾ä¸­å¿ƒï¼‰
        unique_labels = labels.unique()
        prototypes = {}
        
        for label in unique_labels:
            mask = (labels == label)
            if mask.sum() > 0:
                prototypes[label.item()] = features[mask].mean(dim=0)
        
        if len(prototypes) < 2:
            # batchä¸­åªæœ‰ä¸€ä¸ªåŸå›¾ï¼Œæ— æ³•è®¡ç®—inter-loss
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # 1. Intra-loss: ç‰¹å¾æ¥è¿‘è‡ªå·±çš„åŸå‹
        intra_loss = 0
        count = 0
        for label in unique_labels:
            mask = (labels == label)
            if mask.sum() > 0:
                proto = prototypes[label.item()]
                # ä½¿ç”¨ä½™å¼¦è·ç¦»
                feat_norm = F.normalize(features[mask], p=2, dim=1)
                proto_norm = F.normalize(proto.unsqueeze(0), p=2, dim=1)
                cosine_sim = torch.matmul(feat_norm, proto_norm.T).squeeze()
                intra_loss += (1 - cosine_sim).mean()  # 1-cosineä½œä¸ºè·ç¦»
                count += 1
        
        intra_loss = intra_loss / count if count > 0 else torch.tensor(0.0, device=self.device)
        
        # 2. Inter-loss: åŸå‹ä¹‹é—´è¿œç¦»
        proto_list = torch.stack([prototypes[k] for k in sorted(prototypes.keys())])
        proto_norm = F.normalize(proto_list, p=2, dim=1)
        proto_sim_matrix = torch.matmul(proto_norm, proto_norm.T)
        
        # æƒ©ç½šéå¯¹è§’çº¿çš„é«˜ç›¸ä¼¼åº¦
        mask_diag = torch.eye(proto_sim_matrix.size(0), device=self.device)
        inter_sim = proto_sim_matrix * (1 - mask_diag)  # åªçœ‹éå¯¹è§’çº¿
        inter_loss = torch.mean(inter_sim)  # æœ€å°åŒ–åŸå‹é—´ç›¸ä¼¼åº¦
        
        # é™ä½åŸå‹é—´åˆ†ç¦»å¼ºåº¦ï¼Œé¿å…è¿‡åˆ†æ‹‰è¿œå¯¼è‡´é²æ£’æ€§ä¸‹é™
        total_proto_loss = intra_loss + 0.5 * inter_loss
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(total_proto_loss) or torch.isinf(total_proto_loss):
            logger.error(f"ğŸ”´ prototype_losså¼‚å¸¸: {total_proto_loss.item()}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        return total_proto_loss
    
    def intra_class_alignment_loss(self, features, labels):
        """
        åŒç±»å¯¹é½æŸå¤±ï¼šé¢å¤–æ”¶ç¼©åŒlabelæ ·æœ¬ï¼ˆåŸå›¾ä¸ä¸åŒæ”»å‡»ç‰ˆæœ¬ã€æ”»å‡»ä¸æ”»å‡»ï¼‰
        ä½¿ç”¨cosineç›¸ä¼¼åº¦çš„(1 - sim)ä½œä¸ºè·ç¦»ï¼Œå¹³å‡æ‰€æœ‰æ­£æ ·æœ¬å¯¹ã€‚
        """
        if features.size(0) < 2:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        # å½’ä¸€åŒ–
        feats = F.normalize(features, p=2, dim=1)
        sim = torch.matmul(feats, feats.T)
        N = sim.size(0)
        same = (labels.unsqueeze(1) == labels.unsqueeze(0))
        diag = torch.eye(N, device=sim.device, dtype=torch.bool)
        pos_mask = same & (~diag)
        if not pos_mask.any():
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        pos_sims = sim[pos_mask]
        loss = torch.mean(1.0 - pos_sims)
        if torch.isnan(loss) or torch.isinf(loss):
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        return loss
    
    def label_aware_uniqueness_loss(self, features, labels):
        """
        â­â­â­ æ ‡ç­¾æ„ŸçŸ¥çš„å”¯ä¸€æ€§æŸå¤±ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼ä¸¤å…¨å…¶ç¾æ–¹æ¡ˆï¼‰
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        - åªæƒ©ç½šä¸åŒåŸå›¾ï¼ˆä¸åŒlabelï¼‰ä¹‹é—´çš„é«˜ç›¸ä¼¼åº¦
        - ä¸å½±å“åŒä¸€åŸå›¾ä¸åŒæ”»å‡»ç‰ˆæœ¬çš„ç›¸ä¼¼åº¦ï¼ˆä¿æŠ¤é²æ£’æ€§ï¼‰
        - é’ˆå¯¹æ€§è§£å†³Railways-Waterwaysç­‰è·¨å›¾é«˜ç›¸ä¼¼åº¦é—®é¢˜
        
        å®ç°ï¼š
        1. æ„å»ºè·¨æ ‡ç­¾maskï¼šåªé€‰æ‹©ä¸åŒlabelçš„ç‰¹å¾å¯¹
        2. è®¡ç®—è·¨æ ‡ç­¾ç›¸ä¼¼åº¦çŸ©é˜µ
        3. æ¿€è¿›æƒ©ç½šé«˜ç›¸ä¼¼åº¦ï¼ˆé˜ˆå€¼0.12ï¼‰
        4. é¢å¤–æƒ©ç½šæç«¯ç›¸ä¼¼åº¦ï¼ˆNC>0.6æ—¶*10ï¼‰
        
        é¢„æœŸæ•ˆæœï¼š
        - Railways-Waterways NC: 0.934 â†’ <0.3
        - é²æ£’æ€§ï¼šä¸å—å½±å“ï¼ˆåŒä¸€åŸå›¾çš„æ”»å‡»ç‰ˆæœ¬ç›¸ä¼¼åº¦ç”±contrastive_lossä¼˜åŒ–ï¼‰
        """
        # â­æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(features).any() or torch.isinf(features).any():
            logger.error(f"ğŸ”´ label_aware_uniqueness_lossè¾“å…¥å¼‚å¸¸ï¼")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        if features.size(0) < 2:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # å½’ä¸€åŒ–ç‰¹å¾
        features_norm = F.normalize(features, p=2, dim=1)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = torch.matmul(features_norm, features_norm.T)
        similarity_matrix = torch.clamp(similarity_matrix, min=-1.0, max=1.0)
        
        # â­â­â­ æ„å»ºè·¨æ ‡ç­¾maskï¼ˆåªé€‰æ‹©ä¸åŒlabelçš„ç‰¹å¾å¯¹ï¼‰
        labels = labels.contiguous().view(-1, 1)
        cross_label_mask = ~torch.eq(labels, labels.T).to(self.device)  # ä¸åŒlabelä¸ºTrue
        
        # æ’é™¤å¯¹è§’çº¿ï¼ˆè™½ç„¶å¯¹è§’çº¿è‚¯å®šæ˜¯åŒlabelï¼Œä½†ä¸ºäº†ä¿é™©ï¼‰
        diagonal_mask = ~torch.eye(similarity_matrix.size(0), dtype=torch.bool, device=similarity_matrix.device)
        cross_label_mask = cross_label_mask & diagonal_mask
        
        # æå–è·¨æ ‡ç­¾ç›¸ä¼¼åº¦
        if cross_label_mask.sum() == 0:
            # å¦‚æœbatchä¸­æ‰€æœ‰æ ·æœ¬éƒ½æ˜¯åŒä¸€labelï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨åˆ†ç»„é‡‡æ ·ï¼‰
            logger.warning("batchä¸­æ²¡æœ‰è·¨æ ‡ç­¾æ ·æœ¬å¯¹ï¼Œå”¯ä¸€æ€§æŸå¤±æ— æ³•è®¡ç®—")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        cross_label_similarity = similarity_matrix[cross_label_mask]
        
        # â­â­â­ ç¨³å®šç‰ˆå”¯ä¸€æ€§æŸå¤±ï¼šé˜²æ­¢è´Ÿå€¼å’Œçˆ†ç‚¸
        # ç›®æ ‡ï¼šè®©æ‰€æœ‰è·¨æ ‡ç­¾ç›¸ä¼¼åº¦æ¥è¿‘0ï¼Œä½†è¦é˜²æ­¢æ¢¯åº¦ä¸ç¨³å®š
        
        # åŸºç¡€æŸå¤±ï¼šåªæƒ©ç½šæ­£ç›¸ä¼¼åº¦ï¼ˆè´Ÿç›¸ä¼¼åº¦è¯´æ˜å·²ç»è¶³å¤Ÿè¿œäº†ï¼‰
        base_loss = torch.mean(torch.relu(cross_label_similarity))
        
        # â­å…³é”®ä¼˜åŒ–ï¼šå¢å¼ºåˆ†å±‚æƒ©ç½šï¼Œç‰¹åˆ«æ˜¯é’ˆå¯¹>0.9çš„æƒ…å†µ
        high_sim_penalty = torch.mean(torch.relu(cross_label_similarity - 0.50)) * 3.0   # ä»2.0æå‡åˆ°3.0
        extreme_penalty = torch.mean(torch.relu(cross_label_similarity - 0.70)) * 6.0   # ä»4.0æå‡åˆ°6.0
        disaster_penalty = torch.mean(torch.relu(cross_label_similarity - 0.85)) * 12.0  # ä»8.0æå‡åˆ°12.0ï¼ˆé’ˆå¯¹>0.85ï¼‰
        critical_penalty = torch.mean(torch.relu(cross_label_similarity - 0.90)) * 20.0  # â­æ–°å¢ï¼šé’ˆå¯¹>0.9çš„æç«¯æƒ©ç½š

        # é’ˆå¯¹æœ€åpairçš„å•ç‹¬æƒ©ç½šï¼ˆä¸“é—¨å‹åˆ¶ Railwaysâ€“Waterwaysï¼‰
        max_cross_sim = torch.max(cross_label_similarity)
        max_penalty = torch.relu(max_cross_sim - 0.60) * 8.0  # ä»5.0æå‡åˆ°8.0ï¼Œèšç„¦äºçœŸæ­£é«˜ç›¸ä¼¼çš„pair
        
        # æ€»æŸå¤±ï¼ˆå¸¦è£å‰ªï¼Œé˜²æ­¢å•ä¸ªbatchæŸå¤±è¿‡å¤§ï¼‰
        total_uniqueness_loss = base_loss + high_sim_penalty + extreme_penalty + disaster_penalty + critical_penalty + max_penalty
        total_uniqueness_loss = torch.clamp(total_uniqueness_loss, min=0.0, max=15.0)  # â­å…³é”®ï¼šä»10.0æå‡åˆ°15.0ï¼Œå…è®¸æ›´å¼ºçš„å”¯ä¸€æ€§æƒ©ç½š
        
        # â­æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(total_uniqueness_loss) or torch.isinf(total_uniqueness_loss):
            logger.error(f"ğŸ”´ label_aware_uniqueness_losså¼‚å¸¸: {total_uniqueness_loss.item()}")
            logger.error(f"   base_loss={base_loss.item()}, high_sim_penalty={high_sim_penalty.item()}")
            logger.error(f"   extreme_penalty={extreme_penalty.item()}, disaster_penalty={disaster_penalty.item()}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)

        return total_uniqueness_loss

    def get_dynamic_loss_weights(self, epoch, max_epoch):
        """ä¸‰é˜¶æ®µåŠ¨æ€æŸå¤±æƒé‡ï¼ˆæ€»æƒé‡æ’å®šï¼‰ï¼Œå‰æœŸå”¯ä¸€æ€§â†’ä¸­æœŸå¹³è¡¡â†’åæœŸé²æ£’æ€§ã€‚
        
        ä¼˜åŒ–ç­–ç•¥ï¼š
        1. æ›´å¹³æ»‘çš„æƒé‡è¿‡æ¸¡ï¼Œé¿å…åæœŸçªç„¶å˜åŒ–å¯¼è‡´æŸå¤±ä¸Šå‡
        2. åæœŸæ¸è¿›å¼å¢åŠ é²æ£’æ€§æƒé‡ï¼ŒåŒæ—¶ä¿æŒå”¯ä¸€æ€§
        3. é’ˆå¯¹Fig12å¤åˆæ”»å‡»ï¼Œåœ¨åæœŸè¿›ä¸€æ­¥å¼ºåŒ–binaryå’Œalignæƒé‡
        4. åæœŸå…è®¸é€‚åº¦ç‰ºç‰²å”¯ä¸€æ€§æ¢å–é²æ£’æ€§ï¼ˆuniquenessé™ä½è‡³â‰ˆ1.3ï¼‰

        ä¸ºäº†åœ¨ä¸åŒæ€»epochä¸‹ä¿æŒç›¸åŒç­–ç•¥ï¼Œè¿™é‡Œæ ¹æ® max_epoch è‡ªé€‚åº”åˆ’åˆ†é˜¶æ®µï¼Œ
        å¹¶ä¿è¯å„é˜¶æ®µ supcon+proto+binary+diversity+uniqueness+align çš„å’Œæ’å®šï¼ˆ8.0ï¼‰ã€‚
        â­ä¼˜åŒ–ï¼šå‰æœŸæ›´é•¿ï¼ˆ50%ï¼‰ä»¥å¼ºåŒ–å”¯ä¸€æ€§ï¼Œå› ä¸ºEpoch 1å°±è¾¾åˆ°æœ€ä½³é²æ£’æ€§ä½†å”¯ä¸€æ€§ä¸è¶³
        """
        # â­ä¼˜åŒ–ï¼šè°ƒæ•´é˜¶æ®µåˆ’åˆ†ï¼Œå‰æœŸæ›´é•¿ä»¥å¼ºåŒ–å”¯ä¸€æ€§
        # å¯¹äº12ä¸ªepochï¼šå‰æœŸ6ä¸ªï¼ˆ50%ï¼‰ï¼Œä¸­æœŸ4ä¸ªï¼ˆ33%ï¼‰ï¼ŒåæœŸ2ä¸ªï¼ˆ17%ï¼‰
        early_end = max(1, int(max_epoch * 0.5))  # ä»30%æå‡åˆ°50%
        mid_end = max(early_end + 1, int(max_epoch * 0.83))  # ä»70%è°ƒæ•´åˆ°83%
        
        # è®¡ç®—é˜¶æ®µå†…è¿›åº¦ï¼ˆ0.0-1.0ï¼‰ï¼Œç”¨äºå¹³æ»‘æ’å€¼
        if epoch < early_end:
            stage_progress = epoch / max(1, early_end)
            supcon = 1.8 - 0.1 * stage_progress  # ç•¥é™ï¼Œä¸ºå”¯ä¸€æ€§è®©è·¯
            proto = 1.0
            binary = 0.6 + 0.2 * stage_progress  # å‰æœŸé™ä½ï¼Œä¸“æ³¨å”¯ä¸€æ€§
            diversity = 1.2 - 0.1 * stage_progress
            uniqueness = 3.2 - 0.2 * stage_progress  # â­å…³é”®ï¼šä»2.5æå‡åˆ°3.2ï¼Œå¼ºåŒ–å”¯ä¸€æ€§
            align = 0.2 + 0.1 * stage_progress  # å‰æœŸé™ä½ï¼Œä¸“æ³¨å”¯ä¸€æ€§
        elif epoch < mid_end:
            stage_progress = (epoch - early_end) / max(1, mid_end - early_end)
            supcon = 1.8 - 0.1 * stage_progress  # ç•¥é™
            proto = 1.0 - 0.05 * stage_progress
            binary = 0.8 + 0.6 * stage_progress  # 0.8 â†’ 1.4ï¼ˆä»1.2â†’1.8é™ä½èµ·ç‚¹ï¼‰
            diversity = 1.1 - 0.2 * stage_progress
            uniqueness = 3.0 - 0.8 * stage_progress  # â­å…³é”®ï¼š3.0 â†’ 2.2ï¼ˆä»2.0â†’1.6æå‡ï¼Œä¿æŒæ›´é«˜å”¯ä¸€æ€§ï¼‰
            align = 0.3 + 0.3 * stage_progress      # 0.3 â†’ 0.6ï¼ˆä»0.6â†’0.8é™ä½èµ·ç‚¹ï¼‰
        else:
            stage_progress = (epoch - mid_end) / max(1, max_epoch - mid_end)
            supcon = 1.7 - 0.1 * stage_progress  # 1.7 â†’ 1.6 (ç•¥é™)
            proto = 0.95 - 0.05 * stage_progress
            binary = 1.4 + 0.4 * stage_progress   # 1.4 â†’ 1.8 (ä»1.8â†’2.4é™ä½ï¼Œé¿å…è¿‡åº¦ç‰ºç‰²å”¯ä¸€æ€§)
            diversity = 0.9 - 0.1 * stage_progress  # 0.9 â†’ 0.8
            uniqueness = 2.2 - 0.4 * stage_progress  # â­å…³é”®ï¼š2.2 â†’ 1.8 (ä»1.7â†’1.4æå‡ï¼Œä¿æŒæ›´é«˜å”¯ä¸€æ€§)
            align = 0.6 + 0.4 * stage_progress    # 0.6 â†’ 1.0 (ä»0.8â†’1.3é™ä½ï¼Œé¿å…è¿‡åº¦ç‰ºç‰²å”¯ä¸€æ€§)

        # â­å…³é”®ä¿®å¤ï¼šç¡®ä¿æ€»æƒé‡æ’å®šï¼ˆçº¦8.0ï¼‰ï¼Œé¿å…åæœŸæƒé‡å¢åŠ å¯¼è‡´æŸå¤±ä¸å†é™ä½
        # è®¡ç®—å½“å‰æ€»æƒé‡
        total_weight = supcon + proto + binary + diversity + uniqueness + align
        # å¦‚æœæ€»æƒé‡åç¦»8.0ï¼ŒæŒ‰æ¯”ä¾‹å½’ä¸€åŒ–ï¼ˆä¿æŒç›¸å¯¹æ¯”ä¾‹ä¸å˜ï¼‰
        if abs(total_weight - 8.0) > 0.01:
            scale_factor = 8.0 / total_weight
            supcon *= scale_factor
            proto *= scale_factor
            binary *= scale_factor
            diversity *= scale_factor
            uniqueness *= scale_factor
            align *= scale_factor
        
        return {
            'supcon': supcon,
            'proto': proto,
            'binary': binary,
            'diversity': diversity,
            'uniqueness': uniqueness,
            'align': align,
        }

    def binary_consistency_loss(self, features_original, features_attacked, epoch=0):
        """
        äºŒå€¼åŒ–ä¸€è‡´æ€§æŸå¤±ï¼ˆæ ¸å¿ƒåˆ›æ–° + è‡ªé€‚åº”æ¸©åº¦ä¼˜åŒ– + æ•°å€¼ç¨³å®šæ€§ï¼‰â­â­â­â­â­
        
        åŠ¨æœºï¼š
        - é›¶æ°´å°æœ€ç»ˆä½¿ç”¨çš„æ˜¯äºŒå€¼åŒ–åçš„ç‰¹å¾
        - åº”è¯¥ç›´æ¥ä¼˜åŒ–äºŒå€¼åŒ–åçš„ä¸€è‡´æ€§
        - è¿™æ‰æ˜¯NCå€¼çš„çœŸå®ä¼˜åŒ–ç›®æ ‡
        
        å®ç°ï¼š
        - è½¯äºŒå€¼åŒ–ï¼ˆå¯å¾®åˆ†ï¼‰
        - è‡ªé€‚åº”æ¸©åº¦ï¼šä»è½¯åˆ°ç¡¬ï¼ˆæ¸©åº¦é€€ç«ï¼‰â­æ–°å¢
        - æœ€å°åŒ–äºŒå€¼åŒ–åçš„æ±‰æ˜è·ç¦»
        - æƒ©ç½šæ¥è¿‘é˜ˆå€¼çš„ç‰¹å¾ï¼ˆé¼“åŠ±æ˜ç¡®çš„0/1ï¼‰
        
        é¢„æœŸæ•ˆæœï¼šNCå€¼æå‡ +25%ï¼ˆåŸ+20% + æ¸©åº¦ä¼˜åŒ–+5%ï¼‰
        """
        # â­æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(features_original).any() or torch.isinf(features_original).any():
            logger.error(f"ğŸ”´ binary_consistency_loss: åŸå§‹ç‰¹å¾å¼‚å¸¸ï¼")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        if torch.isnan(features_attacked).any() or torch.isinf(features_attacked).any():
            logger.error(f"ğŸ”´ binary_consistency_loss: æ”»å‡»ç‰¹å¾å¼‚å¸¸ï¼")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # è®¡ç®—ä¸­ä½æ•°ï¼ˆé˜ˆå€¼ï¼‰
        median_orig = torch.median(features_original, dim=1, keepdim=True)[0]
        median_attack = torch.median(features_attacked, dim=1, keepdim=True)[0]
        
        # è‡ªé€‚åº”æ¸©åº¦ï¼šä»è½¯åˆ°ç¡¬çš„æ¸©åº¦é€€ç« â­
        # æ—©æœŸï¼ˆtemp=1.0ï¼‰ï¼šæ¢¯åº¦å¹³æ»‘ï¼Œå®¹æ˜“ä¼˜åŒ–
        # åæœŸï¼ˆtemp=0.01ï¼‰ï¼šæ¥è¿‘ç¡¬äºŒå€¼åŒ–ï¼Œç²¾ç¡®ä¼˜åŒ–NCå€¼
        temp = max(self.adaptive_temp.get_temperature(epoch), self.binary_temp_floor)
        
        # è®¡ç®—logitsï¼ˆä¸ç»è¿‡sigmoidï¼Œç”¨äºAMPå®‰å…¨çš„BCEï¼‰â­æ·»åŠ è£å‰ª
        logits_orig = (features_original - median_orig) / temp
        logits_attack = (features_attacked - median_attack) / temp
        
        # â­è£å‰ªlogitsåˆ°åˆç†èŒƒå›´ï¼Œé˜²æ­¢æº¢å‡º
        logits_orig = torch.clamp(logits_orig, min=-20, max=20)
        logits_attack = torch.clamp(logits_attack, min=-20, max=20)
        
        combined = torch.cat([features_original, features_attacked], dim=0)
        inst_median_dim = torch.median(combined, dim=0, keepdim=False)[0]
        if self.ema_median is None:
            self.ema_median = inst_median_dim.detach().to(features_original.device, dtype=features_original.dtype)
        else:
            if self.ema_median.shape != inst_median_dim.shape or self.ema_median.device != features_original.device:
                self.ema_median = inst_median_dim.detach().to(features_original.device, dtype=features_original.dtype)
            else:
                self.ema_median = self.ema_momentum * self.ema_median + (1.0 - self.ema_momentum) * inst_median_dim.detach()
        ema_median = self.ema_median.view(1, -1)
        logits_orig_shared = torch.clamp((features_original - ema_median) / temp, min=-20, max=20)
        logits_attack_shared = torch.clamp((features_attacked - ema_median) / temp, min=-20, max=20)
        
        # äºŒå€¼åŒ–åçš„ä¸€è‡´æ€§æŸå¤±ï¼ˆä½¿ç”¨binary_cross_entropy_with_logitsï¼ŒAMPå®‰å…¨ï¼‰
        # targetéœ€è¦æ˜¯sigmoidåçš„å€¼ï¼ˆ0-1ä¹‹é—´ï¼‰
        bce_loss = F.binary_cross_entropy_with_logits(logits_orig, torch.sigmoid(logits_attack.detach())) + \
                   F.binary_cross_entropy_with_logits(logits_attack, torch.sigmoid(logits_orig.detach()))
        bce_loss = bce_loss / 2.0
        
        # â­æ£€æŸ¥bce_lossæ˜¯å¦å¼‚å¸¸
        if torch.isnan(bce_loss) or torch.isinf(bce_loss):
            logger.error(f"ğŸ”´ bce_losså¼‚å¸¸: {bce_loss.item()}")
            logger.error(f"   temp={temp}, logits_origèŒƒå›´=[{logits_orig.min().item():.2f}, {logits_orig.max().item():.2f}]")
            bce_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # è¾¹ç•Œæ¸…æ™°åº¦æŸå¤±ï¼šç‰¹å¾å€¼åº”è¿œç¦»é˜ˆå€¼
        # é¼“åŠ±ç‰¹å¾å€¼å¾ˆå¤§æˆ–å¾ˆå°ï¼Œé¿å…åœ¨é˜ˆå€¼é™„è¿‘ï¼ˆä¸´ç•ŒçŠ¶æ€ä¸ç¨³å®šï¼‰
        margin_orig = torch.abs(features_original - median_orig)
        margin_attack = torch.abs(features_attacked - median_attack)
        
        # æœŸæœ›margin > 0.5ï¼ˆæ ‡å‡†å·®çš„ä¸€åŠï¼‰
        margin_loss = torch.mean(torch.relu(0.5 - margin_orig)) + \
                     torch.mean(torch.relu(0.5 - margin_attack))
        margin_loss = margin_loss / 2.0
        
        # â­æ£€æŸ¥margin_lossæ˜¯å¦å¼‚å¸¸
        if torch.isnan(margin_loss) or torch.isinf(margin_loss):
            logger.error(f"ğŸ”´ margin_losså¼‚å¸¸: {margin_loss.item()}")
            margin_loss = torch.tensor(0.0, device=self.device, requires_grad=True)
        
        logit_mse = F.mse_loss(logits_orig_shared, logits_attack_shared)
        total_loss = bce_loss + 0.2 * margin_loss + 0.1 * logit_mse
        
        # â­æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(total_loss) or torch.isinf(total_loss):
            logger.error(f"ğŸ”´ binary_consistency total_losså¼‚å¸¸: {total_loss.item()}")
            logger.error(f"   bce_loss={bce_loss.item()}, margin_loss={margin_loss.item()}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        return total_loss
    
    def composite_attack_binary_loss(self, batch_pairs, features_original, features_attacked, epoch=0, stage="mid"):
        """
        â­â­â­ é’ˆå¯¹å¤åˆæ”»å‡»çš„ä¸“é—¨äºŒå€¼åŒ–æŸå¤±ï¼ˆæå‡Fig12é²æ£’æ€§ï¼‰
        
        æ ¸å¿ƒæ€æƒ³ï¼š
        - å¤åˆæ”»å‡»ï¼ˆfull_chain, comboï¼‰æ˜¯æœ€éš¾çš„æ”»å‡»ç±»å‹
        - éœ€è¦æ›´ä¸¥æ ¼çš„äºŒå€¼åŒ–ä¸€è‡´æ€§çº¦æŸ
        - ä½¿ç”¨æ›´ä½çš„æ¸©åº¦ï¼ˆæ›´ç¡¬çš„äºŒå€¼åŒ–ï¼‰å’Œæ›´å¼ºçš„æƒ©ç½š
        
        å®ç°ï¼š
        1. è¯†åˆ«batchä¸­çš„å¤åˆæ”»å‡»æ ·æœ¬å¯¹
        2. å¯¹è¿™äº›æ ·æœ¬å¯¹è®¡ç®—æ›´ä¸¥æ ¼çš„äºŒå€¼åŒ–æŸå¤±
        3. å¦‚æœbatchä¸­æ²¡æœ‰å¤åˆæ”»å‡»ï¼Œè¿”å›0ï¼ˆä¸å½±å“å…¶ä»–æŸå¤±ï¼‰
        
        é¢„æœŸæ•ˆæœï¼šFig12 NCå€¼ä»0.67æå‡åˆ°0.8+
        """
        # è¯†åˆ«å¤åˆæ”»å‡»æ ·æœ¬å¯¹
        composite_indices = []
        for idx, (_orig_g, atk_g) in enumerate(batch_pairs):
            atype = get_attack_name(atk_g)
            if is_composite_attack(atype):
                composite_indices.append(idx)
        
        # å¦‚æœbatchä¸­æ²¡æœ‰å¤åˆæ”»å‡»ï¼Œè¿”å›0
        if len(composite_indices) == 0:
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # æå–å¤åˆæ”»å‡»çš„ç‰¹å¾å¯¹
        composite_orig = features_original[composite_indices]
        composite_attack = features_attacked[composite_indices]
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(composite_orig).any() or torch.isinf(composite_orig).any():
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        if torch.isnan(composite_attack).any() or torch.isinf(composite_attack).any():
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # è®¡ç®—ä¸­ä½æ•°ï¼ˆé˜ˆå€¼ï¼‰
        median_orig = torch.median(composite_orig, dim=1, keepdim=True)[0]
        median_attack = torch.median(composite_attack, dim=1, keepdim=True)[0]
        
        # â­å…³é”®ä¼˜åŒ–ï¼šå¯¹å¤åˆæ”»å‡»ä½¿ç”¨æ›´ä½çš„æ¸©åº¦ï¼ˆæ›´ç¡¬çš„äºŒå€¼åŒ–ï¼Œè¿›ä¸€æ­¥æå‡é²æ£’æ€§ï¼‰
        base_temp = max(self.adaptive_temp.get_temperature(epoch), self.binary_temp_floor)
        stage_temp_scale = {
            "early": 0.40,  # ä»0.45é™åˆ°0.40ï¼Œæ›´ç¡¬
            "mid": 0.55,    # ä»0.60é™åˆ°0.55ï¼Œæ›´ç¡¬
            "late": 0.70,   # ä»0.75é™åˆ°0.70ï¼Œæ›´ç¡¬
        }.get(stage, 0.55)
        composite_temp = max(base_temp * stage_temp_scale, self.binary_temp_floor * 0.75)  # ä»0.8é™åˆ°0.75
        
        # è®¡ç®—logits
        logits_orig = (composite_orig - median_orig) / composite_temp
        logits_attack = (composite_attack - median_attack) / composite_temp
        
        # è£å‰ªlogits
        logits_orig = torch.clamp(logits_orig, min=-20, max=20)
        logits_attack = torch.clamp(logits_attack, min=-20, max=20)
        
        # ä½¿ç”¨å…±äº«é˜ˆå€¼ï¼ˆEMAä¸­ä½æ•°ï¼‰è®¡ç®—æ›´ä¸¥æ ¼çš„æŸå¤±
        if self.ema_median is not None:
            ema_median = self.ema_median.view(1, -1)
            logits_orig_shared = torch.clamp((composite_orig - ema_median) / composite_temp, min=-20, max=20)
            logits_attack_shared = torch.clamp((composite_attack - ema_median) / composite_temp, min=-20, max=20)
            
            # æ›´ä¸¥æ ¼çš„BCEæŸå¤±ï¼ˆæƒé‡å¢åŠ ï¼‰
            bce_loss = F.binary_cross_entropy_with_logits(logits_orig_shared, torch.sigmoid(logits_attack_shared.detach())) + \
                       F.binary_cross_entropy_with_logits(logits_attack_shared, torch.sigmoid(logits_orig_shared.detach()))
            bce_loss = bce_loss / 2.0
            
            # æ›´ä¸¥æ ¼çš„MSEæŸå¤±ï¼ˆç›´æ¥ä¼˜åŒ–ç‰¹å¾ä¸€è‡´æ€§ï¼‰
            mse_loss = F.mse_loss(logits_orig_shared, logits_attack_shared)
        else:
            # å¦‚æœæ²¡æœ‰EMAä¸­ä½æ•°ï¼Œä½¿ç”¨å®ä¾‹ä¸­ä½æ•°
            bce_loss = F.binary_cross_entropy_with_logits(logits_orig, torch.sigmoid(logits_attack.detach())) + \
                       F.binary_cross_entropy_with_logits(logits_attack, torch.sigmoid(logits_orig.detach()))
            bce_loss = bce_loss / 2.0
            mse_loss = F.mse_loss(logits_orig, logits_attack)
        
        # è¾¹ç•Œæ¸…æ™°åº¦æŸå¤±ï¼ˆå¯¹å¤åˆæ”»å‡»æ›´ä¸¥æ ¼ï¼Œè¿›ä¸€æ­¥æå‡é²æ£’æ€§ï¼‰
        margin_orig = torch.abs(composite_orig - median_orig)
        margin_attack = torch.abs(composite_attack - median_attack)
        # æœŸæœ›margin > 0.65ï¼ˆä»0.6æå‡åˆ°0.65ï¼Œæ›´ä¸¥æ ¼ï¼‰
        margin_loss = torch.mean(torch.relu(0.65 - margin_orig)) + \
                     torch.mean(torch.relu(0.65 - margin_attack))
        margin_loss = margin_loss / 2.0
        
        if self.ema_median is not None:
            prob_orig = torch.sigmoid(logits_orig_shared)
            prob_attack = torch.sigmoid(logits_attack_shared)
        else:
            prob_orig = torch.sigmoid(logits_orig)
            prob_attack = torch.sigmoid(logits_attack)
        bit_margin = torch.mean(torch.abs(prob_orig - 0.5) + torch.abs(prob_attack - 0.5)) / 2.0
        bit_margin_penalty = torch.relu(0.50 - bit_margin)  # ä»0.45æå‡åˆ°0.50ï¼Œæ›´ä¸¥æ ¼
        
        # æ€»æŸå¤±ï¼šBCE + æ›´å¼ºçš„MSE + æ›´ä¸¥æ ¼çš„margin + bitç¨³å®šåº¦ï¼ˆè¿›ä¸€æ­¥æå‡æƒé‡ï¼‰
        total_composite_loss = bce_loss + 0.35 * mse_loss + 0.35 * margin_loss + 0.30 * bit_margin_penalty  # æƒé‡ä»0.3/0.3/0.25æå‡åˆ°0.35/0.35/0.30
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(total_composite_loss) or torch.isinf(total_composite_loss):
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        return total_composite_loss

    
    def select_diverse_attack_samples(self, attack_graphs, num_samples=8):
        """
        ä»æ”»å‡»å›¾åˆ—è¡¨ä¸­é€‰æ‹©å¤šæ ·åŒ–çš„æ ·æœ¬ï¼ˆåˆ†å±‚é‡‡æ ·ï¼‰
        ä¼˜å…ˆåŒ…å«å¤åˆæ”»å‡»ï¼ˆcomboï¼‰ï¼Œç„¶åè¦†ç›–ä¸åŒæ”»å‡»ç±»å‹
        
        ç­–ç•¥ï¼š
        1. æœ€é«˜ä¼˜å…ˆçº§ï¼šcombo_full_attack_chainï¼ˆFig12å®Œæ•´é“¾å¼æ”»å‡»ï¼‰
        2. æ¬¡ä¼˜å…ˆçº§ï¼šå…¶ä»–comboæ”»å‡»ï¼ˆå¤åˆæ”»å‡»ï¼‰
        3. ä»ä¸åŒæ”»å‡»ç±»å‹ä¸­å‡è¡¡é‡‡æ ·ï¼ˆaddã€deleteã€noiseã€cropç­‰ï¼‰
        
        Args:
            attack_graphs: æ”»å‡»å›¾å¯¹è±¡åˆ—è¡¨
            num_samples: é‡‡æ ·æ•°é‡ï¼ˆé»˜è®¤8ä¸ªï¼‰
        
        Returns:
            é€‰ä¸­çš„æ”»å‡»å›¾åˆ—è¡¨
        """
        if len(attack_graphs) <= num_samples:
            return attack_graphs
        
        import random
        
        # æŒ‰æ”»å‡»ç±»å‹åˆ†ç»„
        attack_types = {
            'full_chain': [], # combo_full_attack_chainï¼ˆFig12ï¼Œæœ€é«˜ä¼˜å…ˆçº§ï¼‰
            'combo': [],      # å…¶ä»–å¤åˆæ”»å‡»
            'add': [],        # é¡¶ç‚¹æ·»åŠ 
            'delete': [],     # é¡¶ç‚¹/å¯¹è±¡åˆ é™¤
            'noise': [],      # å™ªå£°æ‰°åŠ¨
            'crop': [],       # è£å‰ª
            'rotate': [],     # æ—‹è½¬
            'scale': [],      # ç¼©æ”¾
            'flip': [],       # ç¿»è½¬
            'translate': [],  # å¹³ç§»
            'shuffle': [],    # æ‰“ä¹±
            'reverse': [],    # åè½¬
            'other': []       # å…¶ä»–
        }
        
        # åˆ†ç±»æ”»å‡»å›¾
        for graph in attack_graphs:
            attack_name = getattr(graph, 'attack_type', '').lower()
            classified = False
            
            # æœ€é«˜ä¼˜å…ˆçº§ï¼šfull_attack_chain
            if is_full_chain_attack(attack_name):
                attack_types['full_chain'].append(graph)
                classified = True
            elif is_combo_attack(attack_name):
                attack_types['combo'].append(graph)
                classified = True
            else:
                # å…¶ä»–æ”»å‡»ç±»å‹
                for atype in ['add', 'delete', 'noise', 'crop', 'rotate', 'scale', 
                              'flip', 'translate', 'shuffle', 'reverse']:
                    if atype in attack_name:
                        attack_types[atype].append(graph)
                        classified = True
                        break
            
            if not classified:
                attack_types['other'].append(graph)
        
        samples = []
        
        # ç¬¬0ä¼˜å…ˆçº§ï¼šcombo_full_attack_chainï¼ˆå¿…é€‰ï¼Œå¦‚æœå­˜åœ¨ï¼‰
        if attack_types['full_chain']:
            samples.extend(attack_types['full_chain'][:1])  # åªæœ‰1ä¸ªï¼Œå…¨é€‰
            logger.info("  åŒ…å«Fig12å®Œæ•´é“¾å¼æ”»å‡»: combo_full_attack_chain")
        
        # ç¬¬1ä¼˜å…ˆçº§ï¼šå…¶ä»–comboæ”»å‡»ï¼ˆé€‰æ‹©2-3ä¸ªï¼‰
        remaining = num_samples - len(samples)
        if attack_types['combo'] and remaining > 0:
            combo_count = min(3, len(attack_types['combo']), remaining)
            samples.extend(random.sample(attack_types['combo'], combo_count))
        
        # ç¬¬2ä¼˜å…ˆçº§ï¼šä»å…¶ä»–ç±»å‹ä¸­å‡è¡¡é€‰æ‹©
        remaining_count = num_samples - len(samples)
        available_types = [t for t in attack_types if t not in ['full_chain', 'combo'] and len(attack_types[t]) > 0]
        
        if available_types and remaining_count > 0:
            # è®¡ç®—æ¯ä¸ªç±»å‹åº”è¯¥é€‰æ‹©å¤šå°‘ä¸ª
            per_type = max(1, remaining_count // len(available_types))
            
            for atype in available_types:
                if len(samples) >= num_samples:
                    break
                count = min(per_type, len(attack_types[atype]))
                samples.extend(random.sample(attack_types[atype], count))
        
        # å¦‚æœè¿˜ä¸å¤Ÿï¼Œä»æ‰€æœ‰å‰©ä½™ä¸­éšæœºè¡¥å……
        if len(samples) < num_samples:
            all_graphs = [g for g in attack_graphs if g not in samples]
            if all_graphs:
                additional = random.sample(all_graphs, min(num_samples - len(samples), len(all_graphs)))
                samples.extend(additional)
        
        # æ‰“ä¹±é¡ºåº
        random.shuffle(samples)
        
        return samples[:num_samples]
    
    def evaluate_nc_on_validation(self, val_orig, val_attack):
        """
        åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°NCå€¼ï¼ˆç›´æ¥è¯„ä¼°æœ€ç»ˆç›®æ ‡ï¼‰
        
        æ¨¡æ‹Ÿå®Œæ•´çš„é›¶æ°´å°æµç¨‹ï¼š
        1. æå–åŸå›¾ç‰¹å¾ -> äºŒå€¼åŒ–
        2. æå–æ”»å‡»å›¾ç‰¹å¾ -> äºŒå€¼åŒ–
        3. è®¡ç®—äºŒå€¼ç‰¹å¾çš„ä¸€è‡´æ€§ï¼ˆNCï¼‰
        
        æ”¹è¿›ï¼šä½¿ç”¨åˆ†å±‚é‡‡æ ·ï¼ˆæ¯å›¾8ä¸ªæ”»å‡»ç‰ˆæœ¬ï¼‰è€Œéç®€å•å–å‰3ä¸ª
              ä¼˜å…ˆåŒ…å«å¤åˆæ”»å‡»ï¼ˆcomboï¼‰ï¼Œè¦†ç›–å¤šç§æ”»å‡»ç±»å‹
        
        Returns:
            dict: åŒ…å« 'avg_nc' (å¹³å‡NCå€¼) å’Œ 'fig12_nc' (Fig12é“¾å¼æ”»å‡»NCå€¼)
                  å¦‚æœæ²¡æœ‰Fig12æ•°æ®ï¼Œfig12_ncä¸ºNone
        """
        self.model.eval()
        nc_values = []
        attack_type_stats = {}
        
        try:
            with torch.no_grad():
                # è¯„ä¼°æ‰€æœ‰éªŒè¯é›†å›¾ï¼ˆç¡®ä¿NCå€¼å‡†ç¡®ï¼‰
                # å¦‚æœéªŒè¯é›†å¤ªå¤§ï¼Œå¯ä»¥é€šè¿‡è°ƒæ•´val_ratioæ¥æ§åˆ¶
                for graph_name in list(val_orig.keys()):
                    if graph_name not in val_attack or len(val_attack[graph_name]) == 0:
                        continue
                    
                    try:
                        # æå–åŸå›¾ç‰¹å¾
                        orig_graph = val_orig[graph_name].to(self.device)
                        features_orig = self.model(orig_graph.x, orig_graph.edge_index)
                        features_orig = features_orig.cpu().numpy()
                        
                        # äºŒå€¼åŒ–ï¼ˆä¸­ä½æ•°é˜ˆå€¼ï¼‰
                        threshold_orig = np.median(features_orig)
                        binary_orig = (features_orig > threshold_orig).astype(np.int32)
                        
                        # å¯¹æ¯ä¸ªæ”»å‡»ç‰ˆæœ¬ï¼ˆåˆ†å±‚é‡‡æ ·ï¼‰
                        # ä»æ”»å‡»åˆ—è¡¨ä¸­æ™ºèƒ½é€‰æ‹©8ä¸ªæ ·æœ¬ï¼Œè¦†ç›–ä¸åŒæ”»å‡»ç±»å‹
                        attack_samples = self.select_diverse_attack_samples(
                            val_attack[graph_name], 
                            num_samples=8  # ä»3ä¸ªå¢åŠ åˆ°8ä¸ªï¼Œæé«˜è¯„ä¼°å‡†ç¡®æ€§
                        )
                        
                        for attack_graph in attack_samples:
                            try:
                                attack_graph = attack_graph.to(self.device)
                                features_attack = self.model(attack_graph.x, attack_graph.edge_index)
                                features_attack = features_attack.cpu().numpy()
                                
                                # äºŒå€¼åŒ–
                                threshold_attack = np.median(features_attack)
                                binary_attack = (features_attack > threshold_attack).astype(np.int32)
                                
                                # è®¡ç®—NCï¼ˆæ±‰æ˜è·ç¦»ï¼‰
                                hamming_distance = np.sum(binary_orig != binary_attack)
                                nc = 1.0 - (hamming_distance / len(binary_orig))
                                nc_values.append(nc)
                                
                                # ç»Ÿè®¡æ”»å‡»ç±»å‹
                                attack_name = getattr(attack_graph, 'attack_type', 'unknown').lower()
                                if is_full_chain_attack(attack_name):
                                    attack_category = 'full_chain'  # Fig12å®Œæ•´é“¾å¼
                                elif is_combo_attack(attack_name):
                                    attack_category = 'combo'  # å…¶ä»–å¤åˆæ”»å‡»
                                else:
                                    attack_category = 'single'  # å•ä¸€æ”»å‡»
                                
                                if attack_category not in attack_type_stats:
                                    attack_type_stats[attack_category] = []
                                attack_type_stats[attack_category].append(nc)
                            except RuntimeError as e:
                                if 'out of memory' in str(e).lower():
                                    logger.warning(f"  éªŒè¯é›†å›¾OOMï¼Œè·³è¿‡: {graph_name} (æ”»å‡»ç‰ˆæœ¬)")
                                    # âœ… OOMæ¢å¤
                                    if torch.cuda.is_available():
                                        torch.cuda.empty_cache()
                                    continue
                                else:
                                    raise
                    except RuntimeError as e:
                        if 'out of memory' in str(e).lower():
                            logger.warning(f"  éªŒè¯é›†å›¾OOMï¼Œè·³è¿‡: {graph_name} (åŸå›¾)")
                            # âœ… OOMæ¢å¤
                            if torch.cuda.is_available():
                                torch.cuda.empty_cache()
                            continue
                        else:
                            raise
        except Exception as e:
            logger.error(f"éªŒè¯é›†è¯„ä¼°å‡ºé”™: {e}")
            # âœ… å¼‚å¸¸æ¢å¤
            if torch.cuda.is_available():
                try:
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()
                except:
                    pass
        finally:
            self.model.train()
        
        avg_nc = np.mean(nc_values) if nc_values else 0.0
        fig12_nc = None
        
        # è¾“å‡ºé‡‡æ ·ç»Ÿè®¡
        if nc_values:
            logger.info(f"")
            logger.info(f"="*70)
            logger.info(f"éªŒè¯é›†NCè¯„ä¼°è¯¦æƒ…")
            logger.info(f"="*70)
            logger.info(f"ğŸ“Š æ€»ä½“ç»Ÿè®¡:")
            logger.info(f"  è¯„ä¼°æ ·æœ¬æ•°: {len(nc_values)} ä¸ª")
            logger.info(f"  å¹³å‡NCå€¼: {avg_nc:.4f}")
            logger.info(f"  æœ€å°NCå€¼: {min(nc_values):.4f}")
            logger.info(f"  æœ€å¤§NCå€¼: {max(nc_values):.4f}")
            logger.info(f"  æ ‡å‡†å·®: {np.std(nc_values):.4f}")
            
            # è¾“å‡ºæ”»å‡»ç±»å‹ç»Ÿè®¡
            if attack_type_stats:
                logger.info(f"")
                logger.info(f"ğŸ“Š åˆ†æ”»å‡»ç±»å‹ç»Ÿè®¡:")
                
                # Fig12å®Œæ•´é“¾å¼æ”»å‡»
                if 'full_chain' in attack_type_stats:
                    fig12_nc = np.mean(attack_type_stats['full_chain'])
                    logger.info(f"  ğŸ”¥ Fig12å®Œæ•´é“¾å¼æ”»å‡»: {len(attack_type_stats['full_chain'])} ä¸ªæ ·æœ¬")
                    logger.info(f"     å¹³å‡NCå€¼: {fig12_nc:.4f}")
                    logger.info(f"     èŒƒå›´: [{min(attack_type_stats['full_chain']):.4f}, {max(attack_type_stats['full_chain']):.4f}]")
                    logger.info(f"")
                
                # å…¶ä»–å¤åˆæ”»å‡»
                if 'combo' in attack_type_stats:
                    combo_nc = np.mean(attack_type_stats['combo'])
                    logger.info(f"  ğŸ’¥ å…¶ä»–å¤åˆæ”»å‡» (Combo): {len(attack_type_stats['combo'])} ä¸ªæ ·æœ¬")
                    logger.info(f"     å¹³å‡NCå€¼: {combo_nc:.4f}")
                    logger.info(f"     èŒƒå›´: [{min(attack_type_stats['combo']):.4f}, {max(attack_type_stats['combo']):.4f}]")
                
                # å•ä¸€æ”»å‡»
                if 'single' in attack_type_stats:
                    single_nc = np.mean(attack_type_stats['single'])
                    logger.info(f"  âš¡ å•ä¸€æ”»å‡»: {len(attack_type_stats['single'])} ä¸ªæ ·æœ¬")
                    logger.info(f"     å¹³å‡NCå€¼: {single_nc:.4f}")
                    logger.info(f"     èŒƒå›´: [{min(attack_type_stats['single']):.4f}, {max(attack_type_stats['single']):.4f}]")
                
                # å¯¹æ¯”åˆ†æ
                logger.info(f"")
                logger.info(f"ğŸ’¡ å¯¹æ¯”åˆ†æ:")
                
                if 'full_chain' in attack_type_stats:
                    full_avg = np.mean(attack_type_stats['full_chain'])
                    logger.info(f"  ğŸ”¥ Fig12å®Œæ•´é“¾å¼æ”»å‡»: {full_avg:.4f}")
                
                if 'combo' in attack_type_stats and 'single' in attack_type_stats:
                    combo_avg = np.mean(attack_type_stats['combo'])
                    single_avg = np.mean(attack_type_stats['single'])
                    if combo_avg < single_avg:
                        diff = single_avg - combo_avg
                        logger.info(f"  ğŸ’¥ å¤åˆæ”»å‡»æ¯”å•ä¸€æ”»å‡»NCå€¼ä½ {diff:.4f} (æ›´å…·æŒ‘æˆ˜æ€§)")
                    else:
                        logger.info(f"  å¤åˆæ”»å‡»å’Œå•ä¸€æ”»å‡»è¡¨ç°ç›¸å½“")
                
                if 'full_chain' in attack_type_stats and 'combo' in attack_type_stats:
                    full_avg = np.mean(attack_type_stats['full_chain'])
                    combo_avg = np.mean(attack_type_stats['combo'])
                    if full_avg < combo_avg:
                        diff = combo_avg - full_avg
                        logger.info(f"  ğŸ”¥ Fig12é“¾å¼æ”»å‡»æ¯”æ™®é€šå¤åˆæ”»å‡»NCå€¼ä½ {diff:.4f} (æœ€å…·æŒ‘æˆ˜æ€§)")
                    elif full_avg > combo_avg:
                        diff = full_avg - combo_avg
                        logger.info(f"  âš ï¸ Fig12é“¾å¼æ”»å‡»NCå€¼åè€Œé«˜ {diff:.4f} (å¯èƒ½æ¨¡å‹å¯¹é“¾å¼æ”»å‡»é²æ£’)")
            
            logger.info(f"="*70)
            logger.info(f"")
        
        # â­è¿”å›å­—å…¸ï¼ŒåŒ…å«å¹³å‡NCå’ŒFig12 NCå€¼
        return {'avg_nc': avg_nc, 'fig12_nc': fig12_nc}
    
    def evaluate_feature_distinction(self, val_orig):
        """
        è¯„ä¼°ç‰¹å¾åŒºåˆ†åº¦ï¼šç¡®ä¿ä¸åŒçŸ¢é‡åœ°å›¾ç”Ÿæˆä¸åŒçš„ç‰¹å¾
        
        æ ¸å¿ƒæŒ‡æ ‡ï¼š
        1. æ‰¹æ¬¡å†…å¹³å‡ä½™å¼¦è·ç¦»ï¼ˆè¶Šå¤§è¶Šå¥½ï¼ŒæœŸæœ› > 0.7ï¼‰
        2. äºŒå€¼åŒ–åçš„æ±‰æ˜è·ç¦»ï¼ˆè¶Šå¤§è¶Šå¥½ï¼ŒæœŸæœ› > 400/1024ï¼‰
        3. ç‰¹å¾åå¡Œæ£€æµ‹ï¼ˆæ–¹å·®è¿‡å°çš„ç»´åº¦æ•°é‡ï¼‰
        
        Returns:
            distinction_score: åŒºåˆ†åº¦åˆ†æ•°ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰
        """
        self.model.eval()
        all_features = []
        all_binary_features = []
        
        with torch.no_grad():
            # æå–æ‰€æœ‰éªŒè¯é›†å›¾çš„ç‰¹å¾
            # å¦‚æœéªŒè¯é›†å¤ªå¤§å¯¼è‡´å†…å­˜é—®é¢˜ï¼Œå¯ä»¥é™åˆ¶æ•°é‡
            max_graphs = min(20, len(val_orig))  # æœ€å¤šè¯„ä¼°20ä¸ªå›¾
            for graph_name in list(val_orig.keys())[:max_graphs]:
                graph = val_orig[graph_name].to(self.device)
                features = self.model(graph.x, graph.edge_index)
                features_np = features.cpu().numpy()
                
                all_features.append(features_np)
                
                # äºŒå€¼åŒ–
                threshold = np.median(features_np)
                binary = (features_np > threshold).astype(np.int32)
                all_binary_features.append(binary)
        
        if len(all_features) < 2:
            self.model.train()
            return 0.0
        
        all_features = np.array(all_features)  # [num_graphs, 1024]
        all_binary_features = np.array(all_binary_features)  # [num_graphs, 1024]
        
        # 1. è®¡ç®—è¿ç»­ç‰¹å¾çš„å¹³å‡ä½™å¼¦è·ç¦»
        from sklearn.metrics.pairwise import cosine_similarity
        cos_sim_matrix = cosine_similarity(all_features)
        
        # æ’é™¤å¯¹è§’çº¿ï¼ˆè‡ªå·±ä¸è‡ªå·±ï¼‰
        mask = ~np.eye(cos_sim_matrix.shape[0], dtype=bool)
        inter_similarities = cos_sim_matrix[mask]
        avg_cosine_distance = 1.0 - np.mean(inter_similarities)
        
        # 2. è®¡ç®—äºŒå€¼åŒ–åçš„å¹³å‡æ±‰æ˜è·ç¦»
        hamming_distances = []
        for i in range(len(all_binary_features)):
            for j in range(i+1, len(all_binary_features)):
                hamming_dist = np.sum(all_binary_features[i] != all_binary_features[j])
                hamming_distances.append(hamming_dist / 1024.0)  # å½’ä¸€åŒ–åˆ°[0,1]
        avg_hamming_distance = np.mean(hamming_distances) if hamming_distances else 0.0
        
        # 3. æ£€æµ‹ç‰¹å¾åå¡Œï¼ˆæ–¹å·®è¿‡å°çš„ç»´åº¦ï¼‰
        feature_vars = np.var(all_features, axis=0)
        collapsed_dims = np.sum(feature_vars < 0.01)  # æ–¹å·®å°äº0.01è®¤ä¸ºåå¡Œ
        collapse_ratio = collapsed_dims / all_features.shape[1]
        
        # ç»¼åˆè¯„åˆ†
        distinction_score = (
            0.4 * avg_cosine_distance +      # æœŸæœ› > 0.7
            0.4 * avg_hamming_distance +     # æœŸæœ› > 0.4
            0.2 * (1.0 - collapse_ratio)     # æœŸæœ› collapse_ratio < 0.1
        )
        
        self.model.train()
        
        logger.info(f"ç‰¹å¾åŒºåˆ†åº¦è¯„ä¼°:")
        logger.info(f"  å¹³å‡ä½™å¼¦è·ç¦»: {avg_cosine_distance:.4f} (æœŸæœ› > 0.7)")
        logger.info(f"  å¹³å‡æ±‰æ˜è·ç¦»: {avg_hamming_distance:.4f} (æœŸæœ› > 0.4)")
        logger.info(f"  åå¡Œç»´åº¦æ¯”ä¾‹: {collapse_ratio:.4f} (æœŸæœ› < 0.1)")
        logger.info(f"  ç»¼åˆåŒºåˆ†åº¦åˆ†æ•°: {distinction_score:.4f} (è¶Šé«˜è¶Šå¥½)")
        
        return distinction_score
    
    def process_batch_with_retry(self, batch_pairs, batch_labels, weights, epoch, current_batch_size, batch_graph_names=None, is_oom_retry=False):
        """
        å¤„ç†å•ä¸ªbatchï¼Œå¦‚æœOOMåˆ™è‡ªåŠ¨é™ä½batch_sizeé‡è¯•
        
        Args:
            batch_pairs: batchä¸­çš„å›¾å¯¹
            batch_labels: batchä¸­çš„æ ‡ç­¾
            weights: æŸå¤±æƒé‡
            epoch: å½“å‰epoch
            current_batch_size: å½“å‰batchå¤§å°
            batch_graph_names: batchä¸­çš„å›¾åç§°åˆ—è¡¨ï¼ˆç”¨äºOOMè¿½è¸ªï¼‰
            is_oom_retry: æ˜¯å¦æ˜¯OOMé‡è¯•çš„å­batchï¼ˆå¦‚æœæ˜¯ï¼Œä¸è°ƒç”¨scheduler.step()ï¼‰
            
        Returns:
            (success, losses_dict, grad_norm) æˆ– Noneï¼ˆå¦‚æœå®Œå…¨å¤±è´¥ï¼‰
        """
        try:
            total_epochs = getattr(self, "total_epochs", 20)
            stage = getattr(self, "current_stage", None)
            if stage is None:
                stage, _ = compute_stage_progress(epoch, total_epochs)
            stage_for_batch = stage
            aug_p = stage_augmentation_probability(stage_for_batch)
            base_supcon_temp = stage_temperature(stage_for_batch)
            temperature = self.supcon_temp_override if self.supcon_temp_override is not None else base_supcon_temp
            self.current_supcon_temperature = temperature
            
            # å‡†å¤‡batchæ•°æ®
            batch_original_features = []
            batch_attacked_features = []
            
            for idx, (original_graph, attacked_graph) in enumerate(batch_pairs):
                # âœ… å…ˆåœ¨CPUä¸Šè¿›è¡Œå¢å¼ºï¼ˆaugment_graph_dataå†…éƒ¨ä¼šcloneï¼‰ï¼Œé¿å…æŠŠæ•°æ®é›†åŸå§‹å¯¹è±¡è¿ç§»åˆ°GPU
                # åŠ¨æ€å¢å¼ºæ¦‚ç‡ï¼ˆåæœŸæ›´å¼ºï¼‰
                original_graph_cpu = augment_graph_data(original_graph, augment_prob=aug_p, training=True)
                attacked_graph_cpu = augment_graph_data(attacked_graph, augment_prob=aug_p, training=True)

                # âœ… ä»…å°†å¢å¼ºåçš„å…‹éš†ä½“è¿ç§»åˆ°GPUï¼Œé¿å…æ•°æ®é›†ä¸­çš„å¯¹è±¡å¸¸é©»GPUå¯¼è‡´æ˜¾å­˜ç´¯ç§¯
                original_graph_gpu = original_graph_cpu.to(self.device)
                attacked_graph_gpu = attacked_graph_cpu.to(self.device)

                # æå–ç‰¹å¾ï¼ˆAMPï¼‰
                with amp.autocast(enabled=self.use_amp):
                    features_original = self.model(original_graph_gpu.x, original_graph_gpu.edge_index)
                    features_attacked = self.model(attacked_graph_gpu.x, attacked_graph_gpu.edge_index)
                    
                    # âœ… æ•°å€¼è£å‰ªï¼šé˜²æ­¢ç‰¹å¾å€¼è¿‡å¤§å¯¼è‡´åç»­è®¡ç®—NaN
                    features_original = torch.clamp(features_original, min=-10.0, max=10.0)
                    features_attacked = torch.clamp(features_attacked, min=-10.0, max=10.0)
                
                # æ£€æµ‹ç‰¹å¾æå–é˜¶æ®µæ˜¯å¦äº§ç”Ÿå¼‚å¸¸ â­è¯Šæ–­
                if torch.isnan(features_original).any() or torch.isinf(features_original).any():
                    graph_name = batch_graph_names[idx] if batch_graph_names and idx < len(batch_graph_names) else f"æœªçŸ¥å›¾{idx}"
                    logger.error(f"ğŸ”´ ç‰¹å¾æå–å¼‚å¸¸: åŸå§‹å›¾ç‰¹å¾åŒ…å«NaN/Inf")
                    logger.error(f"   é—®é¢˜å›¾: {graph_name}")
                    logger.error(f"   èŠ‚ç‚¹æ•°: {original_graph.x.size(0)}, è¾¹æ•°: {original_graph.edge_index.size(1)}")
                    logger.error(f"   è¾“å…¥ç‰¹å¾èŒƒå›´: [{original_graph.x.min().item():.4f}, {original_graph.x.max().item():.4f}]")
                    logger.error(f"   NaNæ•°é‡: {torch.isnan(features_original).sum().item()}/{features_original.numel()}")
                    
                if torch.isnan(features_attacked).any() or torch.isinf(features_attacked).any():
                    graph_name = batch_graph_names[idx] if batch_graph_names and idx < len(batch_graph_names) else f"æœªçŸ¥å›¾{idx}"
                    logger.error(f"ğŸ”´ ç‰¹å¾æå–å¼‚å¸¸: æ”»å‡»å›¾ç‰¹å¾åŒ…å«NaN/Inf")
                    logger.error(f"   é—®é¢˜å›¾: {graph_name}")
                    logger.error(f"   èŠ‚ç‚¹æ•°: {attacked_graph.x.size(0)}, è¾¹æ•°: {attacked_graph.edge_index.size(1)}")
                    logger.error(f"   è¾“å…¥ç‰¹å¾èŒƒå›´: [{attacked_graph.x.min().item():.4f}, {attacked_graph.x.max().item():.4f}]")
                    logger.error(f"   NaNæ•°é‡: {torch.isnan(features_attacked).sum().item()}/{features_attacked.numel()}")
                
                batch_original_features.append(features_original)
                batch_attacked_features.append(features_attacked)
            
            # å †å ç‰¹å¾
            batch_original = torch.stack(batch_original_features)
            batch_attacked = torch.stack(batch_attacked_features)
            batch_labels_tensor = torch.tensor(batch_labels, device=self.device)
            
            # è®¡ç®—å„é¡¹æŸå¤±ï¼ˆAMPï¼‰
            with amp.autocast(enabled=self.use_amp):
                # ç›‘ç£å¯¹æ¯”ï¼šæ‹¼æ¥åŸå§‹/æ”»å‡»ç‰¹å¾ä¸æ ‡ç­¾åšç»Ÿä¸€å¯¹æ¯”
                all_features = torch.cat([batch_original, batch_attacked], dim=0)
                all_labels = torch.cat([batch_labels_tensor, batch_labels_tensor], dim=0)
                
                # åœ¨è®¡ç®—æŸå¤±å‰æ›´æ–° Memory Bank ä¸åŸå‹
                self.update_memory_bank(all_features, all_labels)
                self.update_prototypes(all_features, all_labels)
                
                # 1. ç›‘ç£å¯¹æ¯”æŸå¤±ï¼ˆåŒ…å«Memory Bankå’ŒåŸå‹å¯¹æ¯”ï¼‰
                supcon_loss = self.supervised_contrastive_loss_with_memory(all_features, all_labels, temperature, epoch=epoch)
                
                # 2. åŸå‹æŸå¤±ï¼ˆè¾…åŠ©ï¼Œé˜²æ­¢ç‰¹å¾æ¼‚ç§»ï¼‰
                proto_loss = self.prototype_loss(all_features, all_labels)
                
                # 3. äºŒå€¼åŒ–ä¸€è‡´æ€§æŸå¤±ï¼ˆä¼˜åŒ–NCå€¼ï¼‰
                # è®¡ç®—åŸºç¡€äºŒå€¼åŒ–æŸå¤±
                binary_loss = self.binary_consistency_loss(batch_original, batch_attacked, epoch)
                composite_attack_indices = []
                full_chain_attack_indices = []
                
                # â­æ–°å¢ï¼šé’ˆå¯¹å¤åˆæ”»å‡»çš„ä¸“é—¨äºŒå€¼åŒ–æŸå¤±ï¼ˆå¢å¼ºé²æ£’æ€§ï¼‰
                composite_binary_loss = self.composite_attack_binary_loss(
                    batch_pairs, batch_original, batch_attacked, epoch, stage_for_batch
                )
                
                if composite_attack_indices:
                    composite_ratio = len(composite_attack_indices) / max(1, len(batch_pairs))
                    binary_loss = binary_loss * (1.0 + 0.3 * composite_ratio)  # ä»0.2æå‡åˆ°0.3
                if full_chain_attack_indices:
                    full_chain_ratio = len(full_chain_attack_indices) / max(1, len(batch_pairs))
                    binary_loss = binary_loss * (1.0 + 0.35 * full_chain_ratio)  # ä»0.25æå‡åˆ°0.35
                
                # 4. è½»é‡å¤šæ ·æ€§æŸå¤±ï¼ˆé˜²æ­¢ç‰¹å¾åå¡Œï¼‰
                diversity_loss = self.diversity_loss(all_features)
                uniqueness_loss = self.label_aware_uniqueness_loss(all_features, all_labels)
                
                # 5. åŠ æƒåŒç±»å¯¹é½æŸå¤±ï¼ˆå¤åˆ/é“¾å¼æ”»å‡»æ›´é«˜æƒé‡ï¼‰
                sample_weights_list = []
                composite_attack_indices = []  # è®°å½•å¤åˆæ”»å‡»çš„ç´¢å¼•
                full_chain_attack_indices = []
                for idx, (orig_g, atk_g) in enumerate(batch_pairs):
                    # åŸå›¾æ ·æœ¬æƒé‡
                    sample_weights_list.append(1.0)
                    # æ ¹æ®æ”»å‡»ç±»å‹åŠ¨æ€è°ƒæƒ
                    atype = str(getattr(atk_g, 'attack_type', '')).lower()
                    w_atk = 1.0
                    is_composite = False
                    if is_full_chain_attack(atype):
                        w_atk = 3.2  # é“¾å¼æ”»å‡»æƒé‡è¿›ä¸€æ­¥æé«˜
                        is_composite = True
                        full_chain_attack_indices.append(idx)
                    elif is_combo_attack(atype):
                        w_atk = 2.6  # ç»„åˆæ”»å‡»æ›´é«˜æƒé‡
                        is_composite = True
                    # â­V9æ¿€è¿›æé«˜å•ä¸€æ”»å‡»çš„æƒé‡ç³»æ•°
                    if 'noise' in atype:
                        w_atk = w_atk * 1.4  # ä»1.3æé«˜åˆ°1.4
                    if 'add' in atype:
                        w_atk = w_atk * 1.4  # ä»1.3æé«˜åˆ°1.4
                    if 'crop' in atype:
                        w_atk = w_atk * 1.3  # ä»1.2æé«˜åˆ°1.3
                    if 'rotate' in atype:
                        w_atk = w_atk * 1.3  # ä»1.2æé«˜åˆ°1.3
                    if 'flip' in atype:
                        w_atk = w_atk * 1.3  # ä»1.2æé«˜åˆ°1.3
                    sample_weights_list.append(w_atk)
                    if is_composite:
                        composite_attack_indices.append(idx)
                sample_weights_tensor = torch.tensor(sample_weights_list, device=self.device, dtype=all_features.dtype)
                align_loss = self.intra_class_alignment_loss_weighted(all_features, all_labels, sample_weights_tensor)
                
                weights_dyn = self.get_dynamic_loss_weights(epoch, total_epochs)
                
                # â­ä¼˜åŒ–ï¼šæ ¹æ®è®­ç»ƒé˜¶æ®µåŠ¨æ€è°ƒæ•´å¤åˆæ”»å‡»æŸå¤±çš„æƒé‡ï¼ˆè¿›ä¸€æ­¥æå‡é²æ£’æ€§ï¼‰
                # â­å…³é”®ä¿®å¤ï¼šåæœŸï¼ˆepoch >= 70%æ€»epochï¼‰æ—¶ï¼Œå¤§å¹…å¢åŠ å¤åˆæ”»å‡»æŸå¤±çš„æƒé‡
                # ä½†éœ€è¦ç¡®ä¿æ€»æŸå¤±å°ºåº¦ä¸ä¼šå› ä¸ºcomposite_weightå¢åŠ è€Œæ— é™å¢å¤§
                composite_weight = 0.0
                progress_ratio = epoch / max(1, total_epochs - 1)
                if progress_ratio < 0.3:
                    composite_weight = 0.2 + 0.5 * (progress_ratio / 0.3)  # 0.2â†’0.7 (æå‡)
                elif progress_ratio < 0.7:
                    composite_weight = 0.7 + 0.5 * ((progress_ratio - 0.3) / 0.4)  # 0.7â†’1.2 (æå‡)
                else:
                    composite_weight = 1.2 + 0.8 * ((progress_ratio - 0.7) / 0.3)  # 1.2â†’2.0 (å¤§å¹…æå‡ï¼š1.5â†’2.0)
                composite_weight = min(composite_weight, 2.0)
                
                total_batch_loss = (
                    weights_dyn['supcon'] * supcon_loss +     # ç›‘ç£å¯¹æ¯”ï¼ˆ2.0â†’1.85ï¼‰
                    weights_dyn['proto'] * proto_loss +       # åŸå‹æŸå¤±ï¼ˆ1.0â†’1.0ï¼‰
                    weights_dyn['binary'] * binary_loss +      # äºŒå€¼åŒ–ï¼ˆ0.6â†’1.2â†’1.8ï¼‰
                    composite_weight * composite_binary_loss +  # â­æ–°å¢ï¼šå¤åˆæ”»å‡»ä¸“é—¨æŸå¤±
                    weights_dyn['diversity'] * diversity_loss + # å¤šæ ·æ€§ï¼ˆ1.4â†’1.2â†’0.9ï¼‰
                    weights_dyn['uniqueness'] * uniqueness_loss + # å”¯ä¸€æ€§ï¼ˆ2.6â†’2.0â†’1.6ï¼‰
                    weights_dyn['align'] * align_loss          # ç±»å†…å¯¹é½ï¼ˆ0.4â†’0.6â†’0.95ï¼‰
                )
                
                # è®°å½•å„é¡¹æŸå¤±ï¼ˆç”¨äºæ—¥å¿—ï¼‰
                contrastive_loss = supcon_loss  # å…¼å®¹æ—¥å¿—è¾“å‡º
                similarity_loss = proto_loss     # å…¼å®¹æ—¥å¿—è¾“å‡º
            
            # NaN/Infæ£€æµ‹ï¼šåœ¨åå‘ä¼ æ’­å‰æ£€æŸ¥æŸå¤± â­ä¿®å¤NaN + è¯¦ç»†è¯Šæ–­
            if torch.isnan(total_batch_loss) or torch.isinf(total_batch_loss):
                logger.error(f"ğŸ”´ğŸ”´ğŸ”´ æ£€æµ‹åˆ°å¼‚å¸¸æ€»æŸå¤±: {total_batch_loss.item()} ğŸ”´ğŸ”´ğŸ”´")
                logger.error(f"=" * 70)
                logger.error(f"ã€NaNè¯Šæ–­æŠ¥å‘Šã€‘Epoch {epoch}, Batch size={current_batch_size}")
                logger.error(f"=" * 70)
                
                # è¯¦ç»†è®°å½•å„é¡¹æŸå¤±
                logger.error(f"ğŸ“Š å„é¡¹æŸå¤±å€¼:")
                logger.error(f"   - å¯¹æ¯”æŸå¤±(InfoNCE): {contrastive_loss.item()}")
                logger.error(f"   - ç›¸ä¼¼æ€§æŸå¤±:        {similarity_loss.item()}")
                logger.error(f"   - å¤šæ ·æ€§æŸå¤±:        {diversity_loss.item()}")
                logger.error(f"   - äºŒå€¼åŒ–æŸå¤±:        {binary_loss.item()}")
                logger.error(f"   - æ€»æŸå¤±:            {total_batch_loss.item()}")
                
                # è®°å½•æŸå¤±æƒé‡
                logger.error(f"")
                logger.error(f"âš–ï¸ å½“å‰æŸå¤±æƒé‡:")
                logger.error(f"   - contrastive:        {weights_dyn['supcon']:.3f}")
                logger.error(f"   - similarity:         {weights_dyn['proto']:.3f}")
                logger.error(f"   - diversity:          {weights_dyn['diversity']:.3f}")
                logger.error(f"   - binary_consistency: {weights_dyn['binary']:.3f}")
                
                # è¯Šæ–­å“ªä¸ªæŸå¤±æ˜¯NaN
                logger.error(f"")
                logger.error(f"ğŸ” å¼‚å¸¸æ¥æºè¯Šæ–­:")
                nan_sources = []
                if torch.isnan(contrastive_loss) or torch.isinf(contrastive_loss):
                    nan_sources.append("å¯¹æ¯”æŸå¤±(InfoNCE)")
                    logger.error(f"   âŒ å¯¹æ¯”æŸå¤±å¼‚å¸¸ â†’ å¯èƒ½åŸå› : ç›¸ä¼¼åº¦çŸ©é˜µexpæº¢å‡ºã€æ‰¹æ¬¡è¿‡å°å¯¼è‡´æ— æ­£æ ·æœ¬å¯¹")
                if torch.isnan(similarity_loss) or torch.isinf(similarity_loss):
                    nan_sources.append("ç›¸ä¼¼æ€§æŸå¤±")
                    logger.error(f"   âŒ ç›¸ä¼¼æ€§æŸå¤±å¼‚å¸¸ â†’ å¯èƒ½åŸå› : ç‰¹å¾å‘é‡å…¨0æˆ–ç‰¹å¾èŒƒæ•°å¼‚å¸¸")
                if torch.isnan(diversity_loss) or torch.isinf(diversity_loss):
                    nan_sources.append("å¤šæ ·æ€§æŸå¤±")
                    logger.error(f"   âŒ å¤šæ ·æ€§æŸå¤±å¼‚å¸¸ â†’ å¯èƒ½åŸå› : ç‰¹å¾åå¡Œã€åæ–¹å·®çŸ©é˜µè®¡ç®—é—®é¢˜")
                if torch.isnan(proto_loss) or torch.isinf(proto_loss):
                    nan_sources.append("åŸå‹å¯¹æ¯”æŸå¤±")
                    logger.error(f"   âŒ åŸå‹å¯¹æ¯”æŸå¤±å¼‚å¸¸ â†’ å¯èƒ½åŸå› : åŸå‹è®¡ç®—é—®é¢˜ã€äº¤å‰ç†µæº¢å‡º")
                if torch.isnan(binary_loss) or torch.isinf(binary_loss):
                    nan_sources.append("äºŒå€¼åŒ–æŸå¤±")
                    logger.error(f"   âŒ äºŒå€¼åŒ–æŸå¤±å¼‚å¸¸ â†’ å¯èƒ½åŸå› : ä¸­ä½æ•°è®¡ç®—å¼‚å¸¸ã€æ¸©åº¦é€€ç«é—®é¢˜")
                
                logger.error(f"   ğŸ¯ å¼‚å¸¸æŸå¤±é¡¹: {', '.join(nan_sources) if nan_sources else 'æ€»æŸå¤±è®¡ç®—è¿‡ç¨‹'}")
                
                # è®°å½•æ¶‰åŠçš„å›¾
                logger.error(f"")
                logger.error(f"ğŸ“ æ¶‰åŠçš„å›¾æ•°æ®:")
                if batch_graph_names:
                    for i, name in enumerate(batch_graph_names):
                        logger.error(f"   [{i+1}] {name}")
                else:
                    logger.error(f"   ï¼ˆæœªè®°å½•å›¾åç§°ï¼‰")
                
                # è®°å½•ç‰¹å¾ç»Ÿè®¡
                logger.error(f"")
                logger.error(f"ğŸ“ˆ ç‰¹å¾ç»Ÿè®¡:")
                logger.error(f"   åŸå§‹ç‰¹å¾ - min: {batch_original.min().item():.4f}, max: {batch_original.max().item():.4f}, mean: {batch_original.mean().item():.4f}")
                logger.error(f"   æ”»å‡»ç‰¹å¾ - min: {batch_attacked.min().item():.4f}, max: {batch_attacked.max().item():.4f}, mean: {batch_attacked.mean().item():.4f}")
                if torch.isnan(batch_original).any():
                    logger.error(f"   âš ï¸ åŸå§‹ç‰¹å¾åŒ…å« {torch.isnan(batch_original).sum().item()} ä¸ªNaN")
                if torch.isnan(batch_attacked).any():
                    logger.error(f"   âš ï¸ æ”»å‡»ç‰¹å¾åŒ…å« {torch.isnan(batch_attacked).sum().item()} ä¸ªNaN")
                
                logger.error(f"=" * 70)
                logger.error(f"âœ… å¤„ç†: è·³è¿‡æ­¤batchï¼Œç»§ç»­è®­ç»ƒ")
                logger.error(f"=" * 70)
                
                # æ¸…ç†GPUå†…å­˜å¹¶è·³è¿‡æ­¤batch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # è¿”å›é›¶æŸå¤±ï¼Œé¿å…æ±¡æŸ“è®­ç»ƒ
                return {
                    'total': 0.0,
                    'contrastive': 0.0,
                    'similarity': 0.0,
                    'diversity': 0.0,
                    'uniqueness': 0.0,
                    'binary': 0.0,
                    'align': 0.0
                }, 0, True  # Trueè¡¨ç¤ºè·³è¿‡äº†æ­¤batch
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            if self.use_amp:
                self.scaler.scale(total_batch_loss).backward()
                self.scaler.unscale_(self.optimizer)
                
                # æ¢¯åº¦è£å‰ª + NaN/Infæ£€æµ‹ â­å¢å¼ºç¨³å®šæ€§
                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å¼‚å¸¸ â­è¯Šæ–­å¢å¼º
                if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                    logger.error(f"ğŸ”´ æ£€æµ‹åˆ°å¼‚å¸¸æ¢¯åº¦èŒƒæ•°(AMP): {grad_norm}")
                    logger.error(f"   Epoch {epoch}, Batch size={current_batch_size}")
                    logger.error(f"   æŸå¤±å€¼: {total_batch_loss.item():.4f}")
                    if batch_graph_names:
                        logger.error(f"   æ¶‰åŠå›¾: {batch_graph_names}")
                    logger.error(f"   â†’ è·³è¿‡ä¼˜åŒ–å™¨æ›´æ–°ï¼Œé¿å…æ¨¡å‹å‚æ•°æ±¡æŸ“")
                    
                    # â­å…³é”®ä¿®å¤ï¼šé‡ç½®scalerçŠ¶æ€ï¼Œé¿å…"unscale_() has already been called"é”™è¯¯
                    self.scaler.update()
                    
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    return {
                        'total': total_batch_loss.item(),
                        'contrastive': contrastive_loss.item(),
                        'similarity': similarity_loss.item(),
                        'diversity': diversity_loss.item(),
                        'uniqueness': uniqueness_loss.item(),
                        'binary': binary_loss.item()
                    }, grad_norm.item() if not torch.isnan(grad_norm) else 0.0, True
                
                self.scaler.step(self.optimizer)
                self.scaler.update()
            else:
                total_batch_loss.backward()
                
                # æ¢¯åº¦è£å‰ª + NaN/Infæ£€æµ‹ â­å¢å¼ºç¨³å®šæ€§
                grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å¼‚å¸¸ â­è¯Šæ–­å¢å¼º
                if torch.isnan(grad_norm) or torch.isinf(grad_norm):
                    logger.error(f"ğŸ”´ æ£€æµ‹åˆ°å¼‚å¸¸æ¢¯åº¦èŒƒæ•°: {grad_norm}")
                    logger.error(f"   Epoch {epoch}, Batch size={current_batch_size}")
                    logger.error(f"   æŸå¤±å€¼: {total_batch_loss.item():.4f}")
                    if batch_graph_names:
                        logger.error(f"   æ¶‰åŠå›¾: {batch_graph_names}")
                    logger.error(f"   â†’ è·³è¿‡ä¼˜åŒ–å™¨æ›´æ–°ï¼Œé¿å…æ¨¡å‹å‚æ•°æ±¡æŸ“")
                    
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    return {
                        'total': total_batch_loss.item(),
                        'contrastive': contrastive_loss.item(),
                        'similarity': similarity_loss.item(),
                        'diversity': diversity_loss.item(),
                        'uniqueness': uniqueness_loss.item(),
                        'binary': binary_loss.item(),
                        'align': align_loss.item()
                    }, grad_norm.item() if not torch.isnan(grad_norm) else 0.0, True
                
                self.optimizer.step()
            
            # â­å…³é”®ä¿®å¤ï¼šæ›´æ–°å­¦ä¹ ç‡ï¼ˆOneCycleLRæ¯æ­¥æ›´æ–°ï¼‰
            # æ³¨æ„ï¼šOOMé‡è¯•æ—¶ï¼Œscheduler.step()åªåœ¨æœ€å¤–å±‚batchæˆåŠŸæ—¶è°ƒç”¨ä¸€æ¬¡
            # é¿å…åœ¨å­batchå¤„ç†æ—¶é‡å¤è°ƒç”¨å¯¼è‡´æ­¥æ•°è¶…è¿‡é¢„è®¾å€¼
            if self.scheduler is not None and not is_oom_retry:
                try:
                    self.scheduler.step()
                    self._apply_lr_multiplier()
                except ValueError as e:
                    if "Tried to step" in str(e) and "times" in str(e):
                        # OneCycleLRæ­¥æ•°è¶…é™ï¼Œè¯´æ˜OOMé‡è¯•å¯¼è‡´å®é™…stepæ•°è¶…è¿‡é¢„è®¾
                        # è¿™ç§æƒ…å†µä¸‹ä¸å†è°ƒç”¨stepï¼Œé¿å…é”™è¯¯
                        logger.warning(f"âš ï¸ OneCycleLRæ­¥æ•°è¶…é™ï¼ˆOOMé‡è¯•å¯¼è‡´ï¼‰ï¼Œè·³è¿‡æœ¬æ¬¡step: {e}")
                    else:
                        raise
            
            # æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            losses = {
                'total': total_batch_loss.item(),
                'contrastive': contrastive_loss.item(),
                'similarity': similarity_loss.item(),
                'diversity': diversity_loss.item(),
                'uniqueness': uniqueness_loss.item(),
                'binary': binary_loss.item(),
                'align': align_loss.item()
            }
            
            # è¿”å›ï¼šlosseså­—å…¸, æ¢¯åº¦èŒƒæ•°, æ˜¯å¦è·³è¿‡ï¼ˆFalseè¡¨ç¤ºæ­£å¸¸å¤„ç†ï¼‰
            return losses, grad_norm.item(), False
            
        except RuntimeError as e:
            if 'out of memory' in str(e).lower():
                # OOMé”™è¯¯ï¼Œå°è¯•æ‹†åˆ†batch
                if batch_graph_names:
                    logger.warning(f"ğŸ”´ OOM! batch_size={current_batch_size}, æ¶‰åŠå›¾: {batch_graph_names}")
                else:
                    logger.warning(f"ğŸ”´ OOM! batch_size={current_batch_size}")
                
                # âœ… å¢å¼ºOOMæ¢å¤ï¼šå½»åº•é‡ç½®CUDAçŠ¶æ€
                self.optimizer.zero_grad(set_to_none=True)  # é‡Šæ”¾æ¢¯åº¦å†…å­˜
                
                if torch.cuda.is_available():
                    try:
                        # åŒæ­¥CUDAæ“ä½œï¼Œç¡®ä¿æ‰€æœ‰pendingæ“ä½œå®Œæˆ
                        torch.cuda.synchronize()
                        # æ¸…ç©ºç¼“å­˜
                        torch.cuda.empty_cache()
                        # é‡ç½®å†…å­˜ç»Ÿè®¡ï¼ˆé¿å…ç´¯ç§¯é”™è¯¯ï¼‰
                        torch.cuda.reset_peak_memory_stats()
                        torch.cuda.reset_accumulated_memory_stats()
                    except Exception as cuda_err:
                        logger.error(f"  âš ï¸ CUDAé‡ç½®è­¦å‘Š: {cuda_err}")
                
                # é‡ç½®AMP scalerçŠ¶æ€ï¼ˆé¿å…"unscale already called"é”™è¯¯ï¼‰
                if self.use_amp:
                    self.scaler = amp.GradScaler(enabled=True)
                
                # å¦‚æœbatchåªæœ‰1ä¸ªæ ·æœ¬ï¼Œæ— æ³•å†æ‹†åˆ†
                if current_batch_size <= 1:
                    oom_graph_name = None
                    if batch_graph_names and batch_pairs:
                        graph_name = batch_graph_names[0]
                        oom_graph_name = graph_name
                        # è·å–åŸå›¾èŠ‚ç‚¹æ•°å’Œè¾¹æ•°
                        try:
                            original_graph = batch_pairs[0][0]  # (original_graph, attacked_graph)
                            num_nodes = original_graph.x.shape[0]
                            num_edges = original_graph.edge_index.shape[1]
                            logger.error(f"  âŒ å•ä¸ªå›¾OOMï¼Œè·³è¿‡: {graph_name}")
                            logger.error(f"     èŠ‚ç‚¹æ•°: {num_nodes:,}, è¾¹æ•°: {num_edges:,}")
                            logger.error(f"     ğŸ’¡ è¯¥å›¾å°†è¢«åŠ å…¥åŠ¨æ€é»‘åå•ï¼Œåç»­epochå°†è·³è¿‡")
                        except:
                            logger.error(f"  âŒ å•ä¸ªå›¾OOMï¼Œè·³è¿‡: {graph_name}")
                            logger.error(f"     ğŸ’¡ è¯¥å›¾å°†è¢«åŠ å…¥åŠ¨æ€é»‘åå•ï¼Œåç»­epochå°†è·³è¿‡")
                    else:
                        logger.error(f"  âŒ å•ä¸ªæ ·æœ¬OOMï¼Œè·³è¿‡")
                    
                    # âœ… OOMæ¢å¤ï¼šæœ€åä¸€æ¬¡æ¸…ç†
                    if torch.cuda.is_available():
                        try:
                            torch.cuda.synchronize()
                            torch.cuda.empty_cache()
                        except:
                            pass
                    
                    # è¿”å›ç‰¹æ®Šæ ‡è®°ï¼šNoneè¡¨ç¤ºOOMä¸”éœ€è¦åŠ å…¥é»‘åå•
                    return ('OOM', oom_graph_name)
                
                # æ™ºèƒ½é™çº§ç­–ç•¥ï¼š8â†’4â†’2â†’1
                # æ ¹æ®å½“å‰batch_sizeå†³å®šä¸‹ä¸€ä¸ªå°è¯•å¤§å°
                if current_batch_size > 4:
                    next_size = 4
                elif current_batch_size > 2:
                    next_size = 2
                elif current_batch_size > 1:
                    next_size = 1
                else:
                    next_size = 1  # æœ€å°ä¸º1
                
                logger.warning(f"  âš¡ å°è¯•é™ä½åˆ° batch_size={next_size}")
                
                # æ‹†åˆ†batchï¼ˆå°½å¯èƒ½å‡åŒ€åˆ†é…ï¼‰
                num_splits = (current_batch_size + next_size - 1) // next_size
                sub_batches = []
                
                for i in range(num_splits):
                    start_idx = i * next_size
                    end_idx = min((i + 1) * next_size, current_batch_size)
                    
                    sub_pairs = batch_pairs[start_idx:end_idx]
                    sub_labels = batch_labels[start_idx:end_idx]
                    sub_names = batch_graph_names[start_idx:end_idx] if batch_graph_names else None
                    sub_size = end_idx - start_idx
                    
                    sub_batches.append((sub_pairs, sub_labels, sub_names, sub_size))
                
                logger.warning(f"  ğŸ“¦ æ‹†åˆ†ä¸º {num_splits} ä¸ªå°æ‰¹æ¬¡ï¼Œæ¯æ‰¹çº¦ {next_size} ä¸ªæ ·æœ¬")
                
                # å¤„ç†æ‰€æœ‰å­batchï¼ˆä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼‰
                all_results = []
                oom_graph_names = set()  # âœ… æ”¶é›†æ‰€æœ‰OOMçš„å›¾å
                for idx, (sub_pairs, sub_labels, sub_names, sub_size) in enumerate(sub_batches):
                    try:
                        # â­å…³é”®ä¿®å¤ï¼šOOMé‡è¯•çš„å­batchæ ‡è®°ä¸ºis_oom_retry=Trueï¼Œé¿å…é‡å¤è°ƒç”¨scheduler.step()
                        result = self.process_batch_with_retry(sub_pairs, sub_labels, 
                                                              weights, epoch, sub_size, sub_names, is_oom_retry=True)
                        if result is not None:
                            # âœ… æ£€æŸ¥æ˜¯å¦æ˜¯OOMæ ‡è®°
                            if isinstance(result, tuple) and len(result) == 2 and result[0] == 'OOM':
                                logger.warning(f"    å­æ‰¹æ¬¡ {idx+1}/{num_splits} OOMï¼Œè·³è¿‡")
                                # âœ… æ”¶é›†OOMçš„å›¾å
                                if result[1]:
                                    oom_graph_names.add(result[1])
                                continue
                            
                            losses, grad_norm, is_skipped = result
                            # è·³è¿‡è¢«æ ‡è®°ä¸ºå¼‚å¸¸çš„batch
                            if not is_skipped:
                                all_results.append(result)
                    except Exception as sub_err:
                        logger.error(f"    å­æ‰¹æ¬¡ {idx+1}/{num_splits} å¤„ç†å¤±è´¥: {sub_err}")
                        # âœ… å­æ‰¹æ¬¡å¤±è´¥æ—¶ä¹Ÿæ¸…ç†CUDA
                        if torch.cuda.is_available():
                            try:
                                torch.cuda.synchronize()
                                torch.cuda.empty_cache()
                            except:
                                pass
                        continue
                
                # åˆå¹¶æ‰€æœ‰æˆåŠŸçš„ç»“æœ
                if len(all_results) == 0:
                    # âœ… å¦‚æœæœ‰OOMçš„å›¾ï¼Œè¿”å›OOMæ ‡è®°è€Œä¸æ˜¯None
                    if len(oom_graph_names) > 0:
                        # è¿”å›ç¬¬ä¸€ä¸ªOOMçš„å›¾åï¼ˆé€šå¸¸batchå†…éƒ½æ˜¯åŒä¸€ä¸ªå›¾ï¼‰
                        oom_graph_name = list(oom_graph_names)[0]
                        logger.error(f"  âš ï¸ æ‰€æœ‰å­æ‰¹æ¬¡éƒ½å¤±è´¥ï¼Œæ£€æµ‹åˆ°OOMå›¾: {oom_graph_name}")
                        return ('OOM', oom_graph_name)
                    return None
                elif len(all_results) == 1:
                    return all_results[0]
                else:
                    # å¹³å‡æ‰€æœ‰ç»“æœ
                    avg_losses = {
                        'total': sum(r[0]['total'] for r in all_results) / len(all_results),
                        'contrastive': sum(r[0]['contrastive'] for r in all_results) / len(all_results),
                        'similarity': sum(r[0]['similarity'] for r in all_results) / len(all_results),
                        'diversity': sum(r[0]['diversity'] for r in all_results) / len(all_results),
                        'uniqueness': sum(r[0]['uniqueness'] for r in all_results) / len(all_results),
                        'binary': sum(r[0]['binary'] for r in all_results) / len(all_results),
                        'align': sum(r[0]['align'] for r in all_results) / len(all_results)
                    }
                    avg_grad_norm = sum(r[1] for r in all_results) / len(all_results)
                    
                    # â­å…³é”®ä¿®å¤ï¼šOOMé‡è¯•åˆå¹¶åï¼Œåœ¨æœ€å¤–å±‚è°ƒç”¨ä¸€æ¬¡scheduler.step()
                    # å› ä¸ºå­batchå¤„ç†æ—¶æ²¡æœ‰è°ƒç”¨scheduler.step()ï¼ˆis_oom_retry=Trueï¼‰
                    if self.scheduler is not None:
                        try:
                            self.scheduler.step()
                            self._apply_lr_multiplier()
                        except ValueError as e:
                            if "Tried to step" in str(e) and "times" in str(e):
                                logger.warning(f"âš ï¸ OneCycleLRæ­¥æ•°è¶…é™ï¼Œè·³è¿‡æœ¬æ¬¡step: {e}")
                            else:
                                raise
                    
                    logger.info(f"  âœ… æˆåŠŸåˆå¹¶ {len(all_results)}/{num_splits} ä¸ªå­æ‰¹æ¬¡")
                    return avg_losses, avg_grad_norm, False  # Falseè¡¨ç¤ºæœªè·³è¿‡
            else:
                logger.error(f"[process_batch] RuntimeErrorï¼ˆéOOMï¼‰: {e}")
                logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                self.optimizer.zero_grad()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                return None
        except Exception as e:
            logger.error(f"[process_batch] æœªçŸ¥å¼‚å¸¸: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            self.optimizer.zero_grad()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            return None
    
    def train_epoch(self, train_orig, train_attack, epoch):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆè‡ªé€‚åº”batch sizeï¼‰"""
        try:
            self.model.train()
            
            # â­â­â­ V9é˜¶æ®µæ€§Memory Bankåˆ·æ–°ï¼šåœ¨epoch 15å’Œ22æ—¶æ¸…ç©º50%æœ€æ—§æ ·æœ¬ï¼ŒE25+ä¿æŒéš¾æ ·æœ¬ä¸åˆ·æ–°
            # â­ä¿®å¤ï¼šå®Œå…¨é‡æ–°è®¾è®¡åˆ·æ–°é€»è¾‘ï¼Œé¿å…å¼ é‡å¤§å°ä¸åŒ¹é…
            if self._should_refresh_memory(epoch):
                keep_ratio = self.schedule.robust_memory_keep_ratio if getattr(self, "current_stage", None) == "late" else 0.5
                self._refresh_memory_bank(epoch, keep_ratio=keep_ratio)
            
            total_loss = 0.0
            total_contrastive_loss = 0.0
            total_similarity_loss = 0.0
            total_diversity_loss = 0.0
            total_uniqueness_loss = 0.0  # âœ… ç´¯ç§¯å”¯ä¸€æ€§æŸå¤±
            total_binary_loss = 0.0
            total_grad_norm = 0.0
            num_batches = 0  # âœ… ç»Ÿè®¡æˆåŠŸå¤„ç†çš„batchæ•°
            num_oom_retries = 0  # ç»Ÿè®¡OOMé‡è¯•æ¬¡æ•°
            num_skipped_batches = 0  # ç»Ÿè®¡å› NaN/Infè·³è¿‡çš„batchæ•° â­ä¿®å¤NaN
            
            # âœ… åˆ†ç»„é‡‡æ ·ç­–ç•¥ï¼šæŒ‰åŸå›¾ç»„ç»‡æ•°æ®ï¼Œç¡®ä¿batchå†…æœ‰æ­£æ ·æœ¬å¯¹
            # æ•°æ®ç»“æ„ï¼š{graph_name: [(orig, attack1), (orig, attack2), ...]}
            grouped_data = {}
            graph_name_to_label = {}  # åŸå›¾åç§°åˆ°labelçš„æ˜ å°„
            blacklisted_count = 0  # è¢«åŠ¨æ€é»‘åå•è¿‡æ»¤çš„å›¾æ•°é‡
            
            for i, (graph_name, original_graph) in enumerate(train_orig.items()):
                # âœ… æ£€æŸ¥åŠ¨æ€é»‘åå•
                if graph_name in self.dynamic_blacklist:
                    blacklisted_count += 1
                    continue
                
                if graph_name in train_attack and len(train_attack[graph_name]) > 0:
                    grouped_data[graph_name] = [
                        (original_graph, attacked_graph) 
                        for attacked_graph in train_attack[graph_name]
                    ]
                    graph_name_to_label[graph_name] = i
            
            if len(grouped_data) == 0:
                logger.warning("æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®å¯¹")
                return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_samples = sum(len(pairs) for pairs in grouped_data.values())
            logger.info(f"æ··åˆé‡‡æ ·ï¼š{len(grouped_data)}ä¸ªåŸå›¾ï¼Œå…±{total_samples}ä¸ªè®­ç»ƒæ ·æœ¬")

            full_chain_global_pool = []
            for gname, pairs in grouped_data.items():
                for (og, ak) in pairs:
                    if is_full_chain_attack(get_attack_name(ak)):
                        full_chain_global_pool.append((og, ak, graph_name_to_label[gname], gname))
            
            # âœ… å¦‚æœæœ‰åŠ¨æ€é»‘åå•ï¼Œç²¾ç®€æ—¥å¿—
            if len(self.dynamic_blacklist) > 0:
                logger.warning(f"åŠ¨æ€é»‘åå•å¯ç”¨ï¼šå·²è¿‡æ»¤{blacklisted_count}ä¸ªå›¾ï¼›å½“å‰é»‘åå•æ•°={len(self.dynamic_blacklist)}")
            
        except Exception as e:
            logger.error(f"[train_epoch] å‡†å¤‡æ•°æ®æ—¶å‡ºé”™: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0
        
        # âœ… æ–¹æ¡ˆAï¼šä½¿ç”¨å›ºå®šæƒé‡ï¼ˆæ¨¡ä»¿VGCNï¼‰
        loss_weights = FIXED_LOSS_WEIGHTS
        
        # ç¬¬ä¸€ä¸ªepochæ—¶æ‰“å°æƒé‡ä¿¡æ¯
        if epoch == 0:
            logger.info("")
            logger.info("âœ… ä½¿ç”¨å›ºå®šæŸå¤±æƒé‡ï¼ˆæ¨¡ä»¿VGCNï¼‰ï¼š")
            logger.info(f"   - contrastive:        {loss_weights['contrastive']:.1f}")
            logger.info(f"   - similarity:         {loss_weights['similarity']:.1f}")
            logger.info(f"   - diversity:          {loss_weights['diversity']:.1f}")
            logger.info(f"   - binary_consistency: {loss_weights['binary_consistency']:.1f} â­")
            logger.info("")
        
        # âœ… æ··åˆé‡‡æ ·ï¼šæ¯ä¸ªbatchåŒ…å«å¤šä¸ªä¸åŒåŸå›¾ï¼ˆæä¾›è´Ÿæ ·æœ¬ï¼‰â­æ–°ç­–ç•¥
        try:
            import random
            
            # è®¡ç®—æ€»batchæ•°
            total_samples = sum(len(pairs) for pairs in grouped_data.values())
            
            # âœ… æ··åˆé‡‡æ ·å‚æ•°ï¼ˆå¹³è¡¡æ•ˆæœä¸æ˜¾å­˜ï¼šbatch_size=6æ¨èï¼‰
            num_graphs_per_batch = 2  # æ¯ä¸ªbatchåŒ…å«2ä¸ªä¸åŒåŸå›¾
            samples_per_graph = 3  # æ¯ä¸ªå›¾è´¡çŒ®3ä¸ªæ ·æœ¬
            effective_batch_size = num_graphs_per_batch * samples_per_graph  # 2Ã—3=6
            
            num_total_batches = (total_samples + effective_batch_size - 1) // effective_batch_size
            
            # åˆ›å»ºåŸå›¾é‡‡æ ·æ± ï¼ˆæ”¯æŒå¤šè½®é‡‡æ ·ï¼‰
            graph_names_list = list(grouped_data.keys())
            random.shuffle(graph_names_list)
            graph_pool = graph_names_list.copy()  # å¯é‡å¤çš„é‡‡æ ·æ± 
            
            logger.info(f"âœ… æ··åˆé‡‡æ ·ç­–ç•¥ï¼šæ¯batch {num_graphs_per_batch}ä¸ªåŸå›¾ï¼Œæ¯å›¾{samples_per_graph}ä¸ªæ ·æœ¬ï¼Œæœ‰æ•ˆbatch_size={effective_batch_size}")
            logger.info(f"   é¢„è®¡æ€»batchæ•°: {num_total_batches}ï¼ˆéå†æ‰€æœ‰{total_samples}ä¸ªæ ·æœ¬ï¼‰")
            
            batch_num = 0
            processed_samples = 0
            
            # éå†æ„å»ºbatchï¼ˆç›´åˆ°å¤„ç†å®Œæ‰€æœ‰æ ·æœ¬ï¼‰
            while processed_samples < total_samples:
                batch_pairs = []
                batch_labels = []
                batch_graph_names = []
                
                # âœ… ä»å¤šä¸ªåŸå›¾é‡‡æ ·æ„å»ºä¸€ä¸ªbatch
                graphs_in_batch = []
                attempts = 0
                max_attempts = len(grouped_data) * 2  # é˜²æ­¢æ­»å¾ªç¯
                
                while len(graphs_in_batch) < num_graphs_per_batch and attempts < max_attempts:
                    attempts += 1
                    
                    # å¦‚æœé‡‡æ ·æ± ç”¨å®Œï¼Œé‡æ–°æ‰“ä¹±
                    if len(graph_pool) == 0:
                        graph_pool = graph_names_list.copy()
                        random.shuffle(graph_pool)
                    
                    # ä»æ± ä¸­å–å‡ºä¸€ä¸ªåŸå›¾
                    current_graph_name = graph_pool.pop(0)
                    
                    # æ£€æŸ¥åŠ¨æ€é»‘åå•
                    if current_graph_name in self.dynamic_blacklist:
                        continue
                    
                    if current_graph_name not in grouped_data:
                        continue
                    
                    current_pairs = grouped_data[current_graph_name]
                    current_label = graph_name_to_label[current_graph_name]
                    
                    if len(current_pairs) == 0:
                        continue
                    
                    # ä»å½“å‰åŸå›¾é‡‡æ ·samples_per_graphä¸ªæ ·æœ¬ï¼ˆæ”»å‡»æ„ŸçŸ¥çš„åŠ æƒé‡‡æ ·ï¼‰
                    num_samples = min(samples_per_graph, len(current_pairs))
                    attack_weights = np.array(
                        [attack_sample_weight(get_attack_name(atk_g)) for (_og, atk_g) in current_pairs],
                        dtype=np.float64
                    )
                    if attack_weights.size == 0:
                        graph_batch_pairs = random.choices(current_pairs, k=num_samples)
                    else:
                        attack_weights = np.clip(attack_weights, 1e-6, None)
                        full_chain_indices = [
                            idx_local for idx_local, (_og, atk_g) in enumerate(current_pairs)
                            if is_full_chain_attack(get_attack_name(atk_g))
                        ]
                        selected_indices: List[int] = []
                        if full_chain_indices:
                            selected_indices.append(int(random.choice(full_chain_indices)))
                            attack_weights[selected_indices[-1]] = 0.0  # é˜²æ­¢é‡å¤æŠ½åˆ°åŒä¸€ä¸ªfull-chain
                        remaining = max(0, num_samples - len(selected_indices))
                        if remaining > 0:
                            if attack_weights.sum() <= 0:
                                attack_weights = np.ones_like(attack_weights)
                            p = attack_weights / attack_weights.sum()
                            replace_flag = remaining > np.count_nonzero(p)
                            sampled_indices = np.random.choice(
                                len(current_pairs),
                                size=remaining,
                                replace=replace_flag,
                                p=p
                            )
                            selected_indices.extend(sampled_indices.tolist())
                        if len(selected_indices) == 0:
                            selected_indices = random.choices(range(len(current_pairs)), k=num_samples)
                        graph_batch_pairs = [current_pairs[int(idx)] for idx in selected_indices[:num_samples]]
                    
                    # æ·»åŠ åˆ°batch
                    batch_pairs.extend(graph_batch_pairs)
                    batch_labels.extend([current_label] * num_samples)
                    batch_graph_names.extend([current_graph_name] * num_samples)
                    graphs_in_batch.append(current_graph_name)
                    processed_samples += num_samples
                
                # å¦‚æœbatchä¸ºç©ºï¼ˆæ‰€æœ‰å›¾éƒ½è¢«é»‘åå•è¿‡æ»¤ï¼‰ï¼Œè·³è¿‡
                if len(batch_pairs) == 0:
                    continue
                
                batch_num += 1
                current_batch_size = len(batch_pairs)
                stage_for_batch = getattr(self, "current_stage", "mid")
                target_full_chain = self.min_full_chain_per_batch if stage_for_batch in ("early", "mid") else self.max_full_chain_per_batch
                full_chain_indices_in_batch = [idx for idx, (_og, _atk) in enumerate(batch_pairs) if is_full_chain_attack(get_attack_name(_atk))]
                if len(full_chain_indices_in_batch) < target_full_chain and len(full_chain_global_pool) > 0:
                    needed = min(target_full_chain - len(full_chain_indices_in_batch), len(full_chain_global_pool))
                    for replace_iter in range(needed):
                        rep = random.choice(full_chain_global_pool)
                        replace_pos = (len(batch_pairs) - 1 - replace_iter) % len(batch_pairs)
                        batch_pairs[replace_pos] = (rep[0], rep[1])
                        batch_labels[replace_pos] = rep[2]
                        batch_graph_names[replace_pos] = rep[3]
                has_composite = any(is_composite_attack(get_attack_name(_atk)) for (_og, _atk) in batch_pairs)
                if not has_composite:
                    try:
                        composite_candidates = []
                        for gname, pairs in grouped_data.items():
                            for (og, ak) in pairs:
                                if is_composite_attack(get_attack_name(ak)):
                                    composite_candidates.append((og, ak, graph_name_to_label[gname], gname))
                        if len(composite_candidates) > 0:
                            rep = random.choice(composite_candidates)
                            batch_pairs[-1] = (rep[0], rep[1])
                            batch_labels[-1] = rep[2]
                            batch_graph_names[-1] = rep[3]
                            current_batch_size = len(batch_pairs)
                    except Exception:
                        pass

                # è½¯çº¦æŸï¼šè‹¥batchä¸­æ— å¼±æ”»å‡»ç±»å‹ï¼ˆnoise/add/crop/rotate/flipï¼‰ï¼Œè¡¥é½1ä¸ª
                has_weak = any(has_weak_perturbation(get_attack_name(_atk)) for (_og, _atk) in batch_pairs)
                if not has_weak:
                    try:
                        weak_candidates = []
                        for gname, pairs in grouped_data.items():
                            for (og, ak) in pairs:
                                if has_weak_perturbation(get_attack_name(ak)):
                                    weak_candidates.append((og, ak, graph_name_to_label[gname], gname))
                        if len(weak_candidates) > 0:
                            repw = random.choice(weak_candidates)
                            batch_pairs[0] = (repw[0], repw[1])
                            batch_labels[0] = repw[2]
                            batch_graph_names[0] = repw[3]
                    except Exception:
                        pass
                
                # âœ… è¯Šæ–­ä¿¡æ¯ï¼šç¬¬ä¸€ä¸ªbatchéªŒè¯æ··åˆé‡‡æ ·
                if batch_num == 1:
                    logger.info(f"")
                    logger.info(f"ğŸ” æ··åˆé‡‡æ ·éªŒè¯ï¼ˆç¬¬1ä¸ªbatchï¼‰ï¼š")
                    logger.info(f"   åŒ…å«åŸå›¾: {graphs_in_batch}")
                    logger.info(f"   Batchå¤§å°: {current_batch_size}")
                    logger.info(f"   Labelåˆ†å¸ƒ: {dict(zip(*np.unique(batch_labels, return_counts=True)))}")
                    
                    # è®¡ç®—æ­£è´Ÿæ ·æœ¬å¯¹æ•°é‡
                    num_positive = sum((np.array(batch_labels) == label).sum() * ((np.array(batch_labels) == label).sum() - 1) 
                                      for label in set(batch_labels))
                    total_pairs = current_batch_size * (current_batch_size - 1)
                    num_negative = total_pairs - num_positive
                    
                    logger.info(f"   âœ… æ­£æ ·æœ¬å¯¹: {num_positive} ä¸ªï¼ˆåŒä¸€åŸå›¾çš„ä¸åŒæ”»å‡»ï¼‰")
                    logger.info(f"   âœ… è´Ÿæ ·æœ¬å¯¹: {num_negative} ä¸ªï¼ˆä¸åŒåŸå›¾ï¼‰â­å…³é”®")
                    logger.info(f"")
                
                # ä½¿ç”¨å¸¦é‡è¯•çš„batchå¤„ç†
                result = self.process_batch_with_retry(batch_pairs, batch_labels, loss_weights, epoch, current_batch_size, batch_graph_names)
                
                # âœ… æ£€æŸ¥æ˜¯å¦æ˜¯OOMå¯¼è‡´çš„å¤±è´¥
                if result is not None and isinstance(result, tuple) and len(result) == 2 and result[0] == 'OOM':
                    # OOMå›¾ï¼ŒåŠ å…¥åŠ¨æ€é»‘åå•
                    oom_graph_name = result[1]
                    if oom_graph_name:
                        self.dynamic_blacklist.add(oom_graph_name)
                        logger.error(f"åŠ å…¥åŠ¨æ€é»‘åå•: {oom_graph_name}ï¼ˆæ€»æ•°={len(self.dynamic_blacklist)}ï¼‰")
                        
                        # âœ… ä»å¯é€‰åˆ—è¡¨ä¸­ç§»é™¤ï¼Œé¿å…é‡å¤é€‰ä¸­
                        if oom_graph_name in grouped_data:
                            # è®¡ç®—è¯¥å›¾çš„æ ·æœ¬æ•°
                            removed_samples = len(grouped_data[oom_graph_name])
                            total_samples -= removed_samples  # è°ƒæ•´æ€»æ ·æœ¬æ•°
                            del grouped_data[oom_graph_name]
                            logger.info(f"  ğŸ“‰ ä»æ€»æ ·æœ¬æ•°ä¸­å‡å» {removed_samples} ä¸ªæ ·æœ¬")
                        if oom_graph_name in graph_names_list:
                            graph_names_list.remove(oom_graph_name)
                        logger.info(f"  âœ… å·²ä»å½“å‰epochçš„å€™é€‰åˆ—è¡¨ä¸­ç§»é™¤è¯¥å›¾")
                        
                        # å¦‚æœæ²¡æœ‰å‰©ä½™å›¾å¯è®­ç»ƒï¼Œæå‰é€€å‡º
                        if len(graph_names_list) == 0:
                            logger.error(f"")
                            logger.error(f"âš ï¸ æ‰€æœ‰å›¾éƒ½å·²åŠ å…¥é»‘åå•ï¼Œæå‰ç»“æŸå½“å‰epoch")
                            logger.error(f"")
                            break
                    continue
                
                if result is not None:
                    losses, grad_norm, is_skipped = result
                    
                    # å¦‚æœbatchè¢«è·³è¿‡ï¼ˆNaNï¼‰ï¼Œè®°å½•ä½†ä¸ç´¯ç§¯åˆ°æŸå¤±ä¸­ â­è¯Šæ–­
                    if is_skipped:
                        num_skipped_batches += 1
                        logger.warning(f"")
                        logger.warning(f"âš ï¸ ========== Batch {batch_num}/{num_total_batches} è¢«è·³è¿‡ ==========")
                        logger.warning(f"   åŸå› : NaN/Infæ£€æµ‹è§¦å‘")
                        logger.warning(f"   å½“å‰è·³è¿‡æ€»æ•°: {num_skipped_batches}")
                        logger.warning(f"   è¯¦æƒ…è¯·æŸ¥çœ‹ä¸Šæ–¹çš„ã€NaNè¯Šæ–­æŠ¥å‘Šã€‘")
                        logger.warning(f"=" * 60)
                        logger.warning(f"")
                        continue
                    
                    # ç´¯ç§¯æŸå¤±
                    total_loss += losses['total']
                    total_contrastive_loss += losses['contrastive']
                    total_similarity_loss += losses['similarity']
                    total_diversity_loss += losses['diversity']
                    total_uniqueness_loss += losses['uniqueness']  # â­â­â­ ç´¯ç§¯å”¯ä¸€æ€§æŸå¤±
                    total_binary_loss += losses['binary']
                    total_grad_norm += grad_norm
                    num_batches += 1
                    
                    # å¦‚æœbatchå¤§å°ä¸åŸå§‹ä¸åŒï¼Œè¯´æ˜å‘ç”Ÿäº†OOMé‡è¯•
                    if current_batch_size != self.batch_size:
                        num_oom_retries += 1
                    
                    # æ¯50ä¸ªbatchæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    if batch_num % 50 == 0:
                        avg_loss_so_far = total_loss / num_batches
                        logger.info(f"  ğŸ“Š Batch {batch_num}/{num_total_batches} | "
                                  f"æ€»æŸå¤±: {avg_loss_so_far:.4f}")
                        logger.info(f"      SupCon: {losses['contrastive']:.4f} | "
                                  f"Proto: {losses['similarity']:.4f} | "
                                  f"Binary: {losses['binary']:.4f} | "
                                  f"Div: {losses['diversity']:.4f}")
                        logger.info(f"      â­Unique: {losses['uniqueness']:.4f} | "
                                  f"Align: {losses.get('align', 0.0):.4f}")
                else:
                    # å®Œå…¨å¤±è´¥ï¼Œè·³è¿‡æ­¤batch
                    graph_info = f"æ¶‰åŠå›¾: {batch_graph_names}" if batch_graph_names else ""
                    logger.warning(f"  âš ï¸ Batch {batch_num} å¤±è´¥ï¼Œè·³è¿‡ {graph_info}")
        except Exception as e:
            logger.error(f"[train_epoch] åˆ†æ‰¹è®­ç»ƒæ—¶å‡ºé”™: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            # âœ… å¢å¼ºOOMæ¢å¤ï¼šepochçº§åˆ«çš„CUDAçŠ¶æ€æ¢å¤
            if 'out of memory' in str(e).lower():
                logger.error(f"")
                logger.error(f"ğŸ”´ æ£€æµ‹åˆ°Epochçº§åˆ«OOMé”™è¯¯ï¼Œå°è¯•æ¢å¤CUDAçŠ¶æ€...")
                if torch.cuda.is_available():
                    try:
                        # æ¸…ç†æ‰€æœ‰æ¢¯åº¦
                        self.optimizer.zero_grad(set_to_none=True)
                        # åŒæ­¥å¹¶æ¸…ç†CUDA
                        torch.cuda.synchronize()
                        torch.cuda.empty_cache()
                        torch.cuda.reset_peak_memory_stats()
                        torch.cuda.reset_accumulated_memory_stats()
                        # é‡ç½®AMP scaler
                        if self.use_amp:
                            self.scaler = amp.GradScaler(enabled=True)
                        logger.error(f"âœ… CUDAçŠ¶æ€å·²é‡ç½®ï¼Œè®­ç»ƒå°†ç»§ç»­")
                    except Exception as cuda_err:
                        logger.error(f"âŒ CUDAé‡ç½®å¤±è´¥: {cuda_err}")
                logger.error(f"")
            
            # ä¸ç«‹å³è¿”å›ï¼Œå°è¯•è®¡ç®—å·²æœ‰çš„å¹³å‡å€¼
        
        # è®¡ç®—å¹³å‡å€¼
        try:
            avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
            avg_contrastive_loss = total_contrastive_loss / num_batches if num_batches > 0 else 0.0
            avg_similarity_loss = total_similarity_loss / num_batches if num_batches > 0 else 0.0
            avg_diversity_loss = total_diversity_loss / num_batches if num_batches > 0 else 0.0
            avg_uniqueness_loss = total_uniqueness_loss / num_batches if num_batches > 0 else 0.0  # â­â­â­ è®¡ç®—å¹³å‡å”¯ä¸€æ€§æŸå¤±
            avg_binary_loss = total_binary_loss / num_batches if num_batches > 0 else 0.0
            avg_grad_norm = total_grad_norm / num_batches if num_batches > 0 else 0.0
            
            # è¾“å‡ºOOMé‡è¯•å’ŒNaNè·³è¿‡ç»Ÿè®¡ â­ä¿®å¤NaN + è¯Šæ–­å¢å¼º
            logger.info(f"")
            logger.info(f"ğŸ“Š æœ¬Epochç»Ÿè®¡:")
            logger.info(f"   - æˆåŠŸå¤„ç†batchæ•°: {num_batches}")
            logger.info(f"   - OOMè‡ªé€‚åº”é‡è¯•: {num_oom_retries}æ¬¡{'ï¼ˆå·²è‡ªåŠ¨å¤„ç†ï¼‰' if num_oom_retries > 0 else ''}")
            if len(self.dynamic_blacklist) > 0:
                logger.warning(f"   - ğŸš« åŠ¨æ€é»‘åå•å›¾æ•°: {len(self.dynamic_blacklist)}ä¸ª (å·²è·³è¿‡)")
            if num_skipped_batches > 0:
                skip_rate = (num_skipped_batches / (num_batches + num_skipped_batches)) * 100 if (num_batches + num_skipped_batches) > 0 else 0
                logger.warning(f"   - âš ï¸ è·³è¿‡å¼‚å¸¸batch: {num_skipped_batches}æ¬¡ (è·³è¿‡ç‡: {skip_rate:.2f}%)")
                logger.warning(f"   - ğŸ’¡ å»ºè®®: å¦‚æœè·³è¿‡ç‡>5%, è¯·æ£€æŸ¥æ•°æ®è´¨é‡æˆ–è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡")
            else:
                logger.info(f"   - âœ… æ— å¼‚å¸¸batchï¼Œè®­ç»ƒç¨³å®š")
            
            # â­â­â­â­â­ V7æ–°å¢ï¼šMemory Bankå’ŒåŸå‹çŠ¶æ€æ—¥å¿—
            if self.memory_initialized:
                valid_memory_size = min(self.memory_seen_count, self.memory_bank_size)
                memory_fill_rate = (valid_memory_size / self.memory_bank_size) * 100
                logger.info(f"")
                logger.info(f"ğŸ¦ Memory BankçŠ¶æ€:")
                logger.info(f"   - å·²å­˜å‚¨æ ·æœ¬æ•°: {valid_memory_size}/{self.memory_bank_size} ({memory_fill_rate:.1f}%)")
                logger.info(f"   - åŸå‹æ•°é‡: {len(self.prototypes)}ä¸ªåŸå›¾")
                logger.info(f"   - è´Ÿæ ·æœ¬æ± æ‰©å¤§: {len(self.prototypes)}å€ï¼ˆä»batchå†…2-3ä¸ªåŸå›¾ â†’ å…¨éƒ¨{len(self.prototypes)}ä¸ªåŸå›¾ï¼‰")
            else:
                logger.info(f"")
                logger.info(f"ğŸ¦ Memory BankçŠ¶æ€: æœªåˆå§‹åŒ–ï¼ˆå°†åœ¨ç¬¬ä¸€ä¸ªbatchååˆå§‹åŒ–ï¼‰")
            
            return avg_loss, avg_contrastive_loss, avg_similarity_loss, avg_diversity_loss, avg_uniqueness_loss, avg_binary_loss, avg_grad_norm, num_oom_retries, num_batches
        except Exception as e:
            logger.error(f"[train_epoch] è®¡ç®—å¹³å‡å€¼æ—¶å‡ºé”™: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0
    
    def train(self, original_graphs, attacked_graphs, num_epochs=40, resume_from_checkpoint=None):
        """
        è®­ç»ƒæ¨¡å‹ï¼ˆå®Œå…¨æ¨¡ä»¿VGCNï¼Œä¸ä½¿ç”¨éªŒè¯é›†ï¼‰
        
        Args:
            original_graphs: åŸå§‹å›¾æ•°æ®
            attacked_graphs: æ”»å‡»åçš„å›¾æ•°æ®
            num_epochs: æ€»è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤40ï¼Œé…åˆæ—©åœæœºåˆ¶ï¼‰
            resume_from_checkpoint: checkpointè·¯å¾„ï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
        """
        logger.info("="*70)
        logger.info("å¼€å§‹è®­ç»ƒæ”¹è¿›çš„GATæ¨¡å‹")
        logger.info("="*70)
        logger.info(f"è®­ç»ƒè½®æ•°: {num_epochs}")
        logger.info(f"âœ… è®­ç»ƒç­–ç•¥: ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼ŒåŸºäºæ€»æŸå¤±ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆæ¨¡ä»¿VGCNï¼‰")
        logger.info("")
        
        # æ›´æ–°è‡ªé€‚åº”æ¸©åº¦çš„æ€»è½®æ•°
        self.adaptive_temp.total_epochs = num_epochs
        logger.info(f"è‡ªé€‚åº”æ¸©åº¦å·²æ›´æ–°: æ€»è½®æ•°={num_epochs}")
        logger.info("")
        
        # âœ… æ–¹æ¡ˆAï¼šä¸åˆ’åˆ†éªŒè¯é›†ï¼Œä½¿ç”¨å…¨éƒ¨æ•°æ®è®­ç»ƒï¼ˆåƒVGCNï¼‰
        train_orig = original_graphs
        train_attack = attacked_graphs
        
        val_orig = train_orig
        val_attack = train_attack
        metric_eval_interval = self.schedule.metric_eval_interval
        metric_patience = self.schedule.metric_patience
        metric_patience_counter = 0
        best_nc = -float('inf')
        best_distinction = -float('inf')
        best_fig12_nc = -float('inf')  # å†…éƒ¨éªŒè¯é›†Fig12-likeæŒ‡æ ‡
        best_fig12_epoch = 0
        best_fig12_nc_real = -float('inf')  # â­æ–°å¢ï¼šçœŸå®Fig12.pyè¯„ä¼°çš„æœ€ä½³NC
        best_fig12_epoch_real = 0
        nc_improve_tol = self.schedule.nc_improve_tol
        distinction_improve_tol = self.schedule.distinction_improve_tol
        min_epoch_for_metric_stop = self.schedule.min_epoch_for_metric_stop
        metric_has_valid = False
        
        # è®¡ç®—æ¯ä¸ªepochçš„batchæ•°ï¼ˆç”¨äºOneCycleLRï¼‰
        num_pairs = sum(len(train_attack.get(k, [])) for k in train_orig.keys())
        steps_per_epoch = max(1, num_pairs // self.batch_size)
        
        logger.info(f"æ•°æ®é›†ç»Ÿè®¡:")
        logger.info(f"  è®­ç»ƒå›¾æ•°: {len(train_orig)} ä¸ªåŸå›¾")
        logger.info(f"  è®­ç»ƒæ ·æœ¬: {num_pairs} ä¸ªå›¾å¯¹")
        logger.info(f"  æ¯epochæ­¥æ•°: {steps_per_epoch}")
        logger.info("")
        
        # åˆå§‹åŒ–OneCycleLRï¼ˆé™ä½å­¦ä¹ ç‡é¿å…NaNï¼‰â­æä½åˆå§‹lr
        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=self.schedule.onecycle_max_lr,
            epochs=num_epochs,
            steps_per_epoch=steps_per_epoch,
            pct_start=self.schedule.onecycle_pct_start,
            anneal_strategy='cos',
            div_factor=self.schedule.onecycle_div_factor,
            final_div_factor=self.schedule.onecycle_final_div
        )
        logger.info(f"OneCycleLRåˆå§‹åŒ–:")
        init_lr = self.schedule.onecycle_max_lr / self.schedule.onecycle_div_factor
        logger.info(f"  ä¼˜åŒ–å™¨: AdamW (åˆå§‹lr={self.base_lr:.6f}, weight_decay=0.01)")
        logger.info(f"  åˆå§‹å­¦ä¹ ç‡: {init_lr:.6f} (æä½å¯åŠ¨ï¼Œé˜²æ­¢åˆå§‹NaN) ")
        logger.info(f"  æ¯epochæ­¥æ•°: {steps_per_epoch}")
        logger.info(f"  Warmupæ¯”ä¾‹: {self.schedule.onecycle_pct_start*100:.0f}% (è‡ªé€‚åº”Reachedå³°å€¼)")
        logger.info("")
        
        # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€ï¼ˆæ¨¡ä»¿VGCNï¼‰
        start_epoch = 0
        self.total_epochs = num_epochs
        best_loss = float('inf')       
        try:
            patience = int(os.environ.get('VGAT_EARLY_STOP_PATIENCE', '15'))
        except Exception:
            patience = 15  
        patience_counter = 0
        self.patience = patience       
        self.best_epoch = 0       
        
        # å°è¯•ä»checkpointæ¢å¤
        if resume_from_checkpoint:
            logger.info(f"å°è¯•ä»checkpointæ¢å¤è®­ç»ƒ: {resume_from_checkpoint}")
            checkpoint = self.load_checkpoint(resume_from_checkpoint)
            if checkpoint:
                start_epoch = checkpoint['epoch'] + 1
                # å…¼å®¹æ—§checkpointï¼ˆå¯èƒ½è¿˜æœ‰best_val_ncï¼‰
                best_loss = checkpoint.get('best_loss', checkpoint.get('best_val_nc', float('inf')))
                patience_counter = checkpoint['patience_counter']
                self.training_history = checkpoint['training_history']
                logger.info(f"ä»Epoch {start_epoch}æ¢å¤è®­ç»ƒ")
                logger.info(f"   æœ€ä½³æŸå¤±: {best_loss:.6f}")
            else:
                logger.warning("  æ— æ³•åŠ è½½checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
        
        # Checkpointä¿å­˜è·¯å¾„ï¼ˆä½¿ç”¨è‡ªå®šä¹‰åç§°ï¼‰
        script_dir = os.path.dirname(os.path.abspath(__file__))
        checkpoint_path = os.path.join(script_dir, 'checkpoints', self.checkpoint_name)
        
        for epoch in tqdm(range(start_epoch, num_epochs), desc="è®­ç»ƒè¿›åº¦", initial=start_epoch, total=num_epochs):
            try:
                logger.info(f"\nå¼€å§‹ Epoch {epoch+1}/{num_epochs}...")
                # é˜¶æ®µåˆ‡æ¢æ—¶é‡ç½®æ—©åœè€å¿ƒï¼ˆE20å‰æœŸâ†’ä¸­æœŸï¼ŒE30ä¸­æœŸâ†’åæœŸï¼ŒE40è¿‡æ¸¡â†’ç¨³å®šï¼‰
                if should_reset_patience(epoch):
                    patience_counter = 0
                    self.patience = 25  
                    logger.info(f"é˜¶æ®µåˆ‡æ¢ï¼ˆepoch={epoch}ï¼‰ï¼Œé‡ç½®patienceä¸º{self.patience}")
                    # â­ Memory Bankåˆ·æ–°å·²ç§»è‡³train_epoch()å¼€å¤´ï¼Œé¿å…é‡å¤åˆ·æ–°
                
                stage, stage_progress = self._update_robust_phase_state(epoch)
                stage_desc = describe_stage(stage, stage_progress)
                
                # è®­ç»ƒ
                train_loss, contrastive_loss, similarity_loss, diversity_loss, uniqueness_loss, binary_loss, grad_norm, num_oom_retries, num_batches = \
                    self.train_epoch(train_orig, train_attack, epoch)
                
                # ç›¸å¯¹æ”¹å–„é˜ˆå€¼ï¼ˆå…è®¸3%çš„æ³¢åŠ¨ï¼‰- ä»…åœ¨æœ¬epochæœ‰æœ‰æ•ˆbatchæ—¶æ‰§è¡Œ
                if num_batches > 0:
                    tolerance = 0.03
                    if train_loss < best_loss * (1 + tolerance):
                        if train_loss < best_loss:
                            best_loss = train_loss
                            self.best_epoch = epoch
                            logger.info(f"ğŸ¯ æ›´æ–°æœ€ä½³æŸå¤±: {best_loss:.6f} (Epoch {epoch+1})")
                        patience_counter = 0
                        try:
                            script_dir = os.path.dirname(os.path.abspath(__file__))
                            model_best_path = os.path.join(script_dir, 'models', f'gat_model_{self.model_prefix}_best.pth')
                            self.save_model(model_best_path)
                            logger.info(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆæ€»æŸå¤±: {best_loss:.6f}ï¼‰-> {os.path.basename(model_best_path)}")
                        except Exception as e:
                            logger.error(f"[Epoch {epoch+1}] ä¿å­˜æœ€ä½³æ¨¡å‹å¤±è´¥: {e}")
                            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                    else:
                        patience_counter += 1
                else:
                    logger.warning(f"æœ¬Epochæœªå¤„ç†ä»»ä½•batchï¼Œè·³è¿‡æœ€ä½³/è€å¿ƒåˆ¤å®šä¸æ¨¡å‹ä¿å­˜")
                
                # è®°å½•è®­ç»ƒå†å²
                try:
                    self.training_history['epoch_losses'].append(train_loss)
                    self.training_history['contrastive_losses'].append(contrastive_loss)
                    self.training_history['similarity_losses'].append(similarity_loss)
                    self.training_history['diversity_losses'].append(diversity_loss)
                    self.training_history['uniqueness_losses'].append(uniqueness_loss)  # â­â­â­ è®°å½•å”¯ä¸€æ€§æŸå¤±
                    self.training_history['binary_consistency_losses'].append(binary_loss)
                    self.training_history['gradient_norms'].append(grad_norm)
                    self.training_history['oom_retries'].append(num_oom_retries)
                    current_lr = self.optimizer.param_groups[0]['lr']
                    self.training_history['learning_rates'].append(current_lr)
                    current_temp = self.adaptive_temp.get_temperature(epoch)
                    self.training_history['temperatures'].append(current_temp)
                except Exception as e:
                    logger.error(f"[Epoch {epoch+1}] è®°å½•è®­ç»ƒå†å²å¤±è´¥: {e}")
                    logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                    # ä½¿ç”¨é»˜è®¤å€¼
                    current_lr = 0.0
                    current_temp = 1.0
                
                # æ¯ä¸ªepochæ‰“å°ç®€è¦ä¿¡æ¯ï¼ˆç²¾ç®€æ—¥å¿—ï¼‰
                try:
                    weights_snapshot = self.get_dynamic_loss_weights(epoch, self.total_epochs)
                    supcon_stage_temp = getattr(self, "current_supcon_temperature", 0.0)
                    logger.info(
                        f"Epoch {epoch+1}/{num_epochs} | é˜¶æ®µ: {stage_desc} | æ€»æŸå¤±: {train_loss:.6f} (æœ€ä½³: {best_loss:.6f}) | "
                        f"SupCon: {contrastive_loss:.4f} | Proto: {similarity_loss:.4f} | Binary: {binary_loss:.4f} | Div: {diversity_loss:.4f} | "
                        f"LR: {current_lr:.6f} | SupCon-T: {supcon_stage_temp:.2f} | Binary-T: {current_temp:.2f} | è€å¿ƒ: {patience_counter}/{self.patience}"
                    )
                    logger.info(
                        "   âš–ï¸ Loss Weights: "
                        f"SupCon={weights_snapshot['supcon']:.2f} | Proto={weights_snapshot['proto']:.2f} | "
                        f"Binary={weights_snapshot['binary']:.2f} | Div={weights_snapshot['diversity']:.2f} | "
                        f"Unique={weights_snapshot['uniqueness']:.2f} | Align={weights_snapshot['align']:.2f}"
                    )
                    
                    # â­ V8æ–°å¢ï¼šMemory Bankéš¾æ ·æœ¬æŒ–æ˜ç»Ÿè®¡
                    if self.memory_initialized and hasattr(self, 'memory_hardness'):
                        valid_size = min(self.memory_seen_count, self.memory_bank_size)
                        if valid_size > 0:
                            avg_hardness = self.memory_hardness[:valid_size].mean().item()
                            max_hardness = self.memory_hardness[:valid_size].max().item()
                            logger.info(
                                f"   ğŸ“Š Memory Bank: {valid_size}/{self.memory_bank_size} æ ·æœ¬ | "
                                f"å¹³å‡éš¾åº¦: {avg_hardness:.4f} | æœ€å¤§éš¾åº¦: {max_hardness:.4f}"
                            )
                except Exception:
                    pass
                
                # ä¿å­˜checkpointï¼ˆæ¯3ä¸ªepochä¿å­˜ä¸€æ¬¡ï¼Œæˆ–åœ¨éªŒè¯epochï¼‰ï¼Œä»…åœ¨æœ‰æœ‰æ•ˆbatchæ—¶ä¿å­˜
                if num_batches > 0 and ((epoch + 1) % 3 == 0 or epoch == start_epoch):
                    try:
                        self.save_checkpoint(
                            checkpoint_path, 
                            epoch, 
                            best_loss,  # âœ… æ”¹ä¸ºbest_loss
                            patience_counter,
                            self.training_history
                        )
                    except Exception as e:
                        logger.error(f"ä¿å­˜checkpointå¤±è´¥: {e}")
                        logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                
                # âœ… æ¯ä¸ªepochä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆè¦†ç›–æ›´æ–°ï¼‰- ä»…åœ¨æœ‰æœ‰æ•ˆbatchæ—¶ä¿å­˜
                if num_batches > 0:
                    try:
                        script_dir = os.path.dirname(os.path.abspath(__file__))
                        model_final_path = os.path.join(script_dir, 'models', f'gat_model_{self.model_prefix}_final.pth')
                        self.save_model(model_final_path)
                        logger.debug(f"âœ… æœ€ç»ˆæ¨¡å‹å·²æ›´æ–°: epoch {epoch+1} -> {os.path.basename(model_final_path)}")
                    except Exception as e:
                        logger.error(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹å¤±è´¥: {e}")

                    # â­ä½¿ç”¨çœŸå®Fig12.pyåœ¨æ¯ä¸ªepochåè¯„ä¼°ä¸€æ¬¡é²æ£’æ€§ï¼ˆé‡æ“ä½œï¼‰
                    try:
                        fig12_nc_real = run_fig12_evaluation_for_model(model_final_path)
                        if isinstance(fig12_nc_real, (float, int)):
                            logger.info(f"[Fig12] Epoch {epoch+1}: Average NC = {fig12_nc_real:.6f}")
                            if fig12_nc_real > best_fig12_nc_real + 1e-4:
                                best_fig12_nc_real = fig12_nc_real
                                best_fig12_epoch_real = epoch
                                best_fig12_path = os.path.join(script_dir, 'models', f'gat_model_{self.model_prefix}_best_fig12.pth')
                                try:
                                    self.save_model(best_fig12_path)
                                    # â­å…³é”®ä¿®å¤ï¼šä¿å­˜åç­‰å¾…æ–‡ä»¶ç³»ç»ŸåŒæ­¥ï¼Œç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œå…¨å†™å…¥
                                    import time
                                    time.sleep(0.5)  # ç­‰å¾…0.5ç§’ç¡®ä¿æ–‡ä»¶å†™å…¥å®Œæˆ
                                    
                                    # â­å…³é”®ä¿®å¤ï¼šä¿å­˜åç«‹å³ç”¨ä¿å­˜çš„æ¨¡å‹é‡æ–°è¯„ä¼°ï¼Œç¡®ä¿æ—¥å¿—è®°å½•çš„NCå€¼ä¸å®é™…æ¨¡å‹ä¸€è‡´
                                    logger.info(f"[Fig12] å¼€å§‹éªŒè¯ä¿å­˜çš„æ¨¡å‹: {best_fig12_path}")
                                    fig12_nc_verify = run_fig12_evaluation_for_model(best_fig12_path)
                                    if isinstance(fig12_nc_verify, (float, int)):
                                        logger.info(f"ğŸ’¾ æ›´æ–°çœŸå®Fig12-bestæ¨¡å‹: {os.path.basename(best_fig12_path)} (NC={fig12_nc_verify:.6f}, éªŒè¯é€šè¿‡)")
                                        logger.info(f"[Fig12] éªŒè¯è¯¦æƒ…: åŸå§‹NC={fig12_nc_real:.6f}, éªŒè¯NC={fig12_nc_verify:.6f}, å·®å¼‚={abs(fig12_nc_real - fig12_nc_verify):.6f}")
                                        best_fig12_nc_real = fig12_nc_verify  # ä½¿ç”¨éªŒè¯åçš„NCå€¼
                                    else:
                                        logger.warning(f"[Fig12] éªŒè¯å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹NCå€¼: {best_fig12_nc_real:.6f}")
                                        logger.info(f"ğŸ’¾ æ›´æ–°çœŸå®Fig12-bestæ¨¡å‹: {os.path.basename(best_fig12_path)} (NC={best_fig12_nc_real:.6f})")
                                except Exception as e:
                                    logger.error(f"[Epoch {epoch+1}] ä¿å­˜Fig12-bestæ¨¡å‹å¤±è´¥: {e}")
                                    logger.error(f"[Epoch {epoch+1}] è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
                    except Exception as e:
                        logger.error(f"[Epoch {epoch+1}] è¿è¡ŒFig12è¯„ä¼°å¤±è´¥: {e}")

                if num_batches > 0 and ((epoch + 1) % metric_eval_interval == 0):
                    nc_results = None
                    distinction_score = None
                    try:
                        nc_results = self.evaluate_nc_on_validation(val_orig, val_attack)
                    except Exception as e:
                        logger.error(f"[Epoch {epoch+1}] evaluate_nc_on_validation å¤±è´¥: {e}")
                    try:
                        distinction_score = self.evaluate_feature_distinction(val_orig)
                    except Exception as e:
                        logger.error(f"[Epoch {epoch+1}] evaluate_feature_distinction å¤±è´¥: {e}")
                    
                    improved = False
                    has_valid = False
                    avg_nc = None
                    fig12_nc = None
                    
                    # å¤„ç†NCè¯„ä¼°ç»“æœï¼ˆç°åœ¨è¿”å›å­—å…¸ï¼‰
                    if isinstance(nc_results, dict):
                        avg_nc = nc_results.get('avg_nc')
                        fig12_nc = nc_results.get('fig12_nc')
                    elif isinstance(nc_results, (float, int)):
                        # å…¼å®¹æ—§ç‰ˆæœ¬ï¼ˆç›´æ¥è¿”å›floatï¼‰
                        avg_nc = nc_results
                    
                    if isinstance(avg_nc, (float, int)):
                        has_valid = True
                        logger.info(f"[Metrics] Epoch {epoch+1}: Avg NC = {avg_nc:.6f}")
                        if avg_nc > best_nc + nc_improve_tol:
                            best_nc = avg_nc
                            improved = True
                    
                    # â­ä¿®å¤ï¼šç¦ç”¨åŸºäºéªŒè¯é›†çš„best_fig12ä¿å­˜é€»è¾‘ï¼Œé¿å…è¦†ç›–åŸºäºçœŸå®Fig12.pyè¯„ä¼°çš„æ¨¡å‹
                    # æ³¨æ„ï¼šbest_fig12æ¨¡å‹åº”è¯¥åªç”±æ¯ä¸ªepochåçš„çœŸå®Fig12.pyè¯„ä¼°æ¥ä¿å­˜ï¼ˆè§ä¸Šé¢çš„run_fig12_evaluation_for_modelï¼‰
                    # éªŒè¯é›†çš„fig12_ncå¯èƒ½ä¸å‡†ç¡®ï¼Œä¸åº”è¯¥ç”¨æ¥ä¿å­˜best_fig12æ¨¡å‹
                    if isinstance(fig12_nc, (float, int)) and fig12_nc > 0:
                        has_valid = True
                        logger.info(f"[Metrics] Epoch {epoch+1}: Fig12 NC = {fig12_nc:.6f} (ç›®æ ‡: >0.8) [éªŒè¯é›†è¯„ä¼°ï¼Œä»…ä¾›å‚è€ƒ]")
                        # â­å·²ç¦ç”¨ï¼šä¸å†åŸºäºéªŒè¯é›†è¯„ä¼°ä¿å­˜best_fig12æ¨¡å‹
                        # çœŸå®best_fig12æ¨¡å‹ç”±æ¯ä¸ªepochåçš„run_fig12_evaluation_for_modelä¿å­˜
                        if fig12_nc > best_fig12_nc + 0.01:  # 0.01çš„å®¹å·®
                            best_fig12_nc = fig12_nc
                            best_fig12_epoch = epoch
                            improved = True
                            # â­å·²ç¦ç”¨ï¼šä¸å†ä¿å­˜ï¼Œé¿å…è¦†ç›–çœŸå®Fig12.pyè¯„ä¼°çš„æ¨¡å‹
                            logger.debug(f"[Metrics] éªŒè¯é›†Fig12 NCæå‡åˆ° {best_fig12_nc:.6f}ï¼Œä½†ä¸ä¿å­˜æ¨¡å‹ï¼ˆçœŸå®best_fig12ç”±Fig12.pyè¯„ä¼°ä¿å­˜ï¼‰")
                    
                    if isinstance(distinction_score, (float, int)):
                        has_valid = True
                        logger.info(f"[Metrics] Epoch {epoch+1}: Distinction = {distinction_score:.6f}")
                        if distinction_score > best_distinction + distinction_improve_tol:
                            best_distinction = distinction_score
                            improved = True
                    if has_valid:
                        metric_has_valid = True
                    if improved:
                        metric_patience_counter = 0
                    else:
                        metric_patience_counter += 1
                    if metric_has_valid and (epoch + 1) >= min_epoch_for_metric_stop and metric_patience_counter >= metric_patience:
                        logger.warning(f"NC/åŒºåˆ†åº¦è¿ç»­{metric_patience_counter}æ¬¡æ— æ˜æ˜¾æå‡ï¼Œè§¦å‘æŒ‡æ ‡æ—©åœ (best NC={best_nc:.6f}, best distinction={best_distinction:.6f})")
                        try:
                            self.save_checkpoint(checkpoint_path, epoch, best_loss, patience_counter, self.training_history)
                        except Exception as e:
                            logger.error(f"åŸºäºæŒ‡æ ‡æ—©åœæ—¶ä¿å­˜checkpointå¤±è´¥: {e}")
                        break
                    self.model.train()

                # åŠ¨æ€æ—©åœæ£€æŸ¥ï¼ˆä½¿ç”¨self.patienceå’Œç›¸å¯¹æ”¹å–„ï¼‰- ä»…åœ¨æœ‰æœ‰æ•ˆbatchæ—¶æ£€æŸ¥
                if num_batches > 0 and patience_counter >= self.patience:
                    logger.warning(f"è¿ç»­{patience_counter}ä¸ªepochæ€»æŸå¤±æ²¡æœ‰æ”¹å–„ï¼ˆæœ€ä½³åœ¨E{self.best_epoch+1}ï¼‰ï¼Œè§¦å‘æ—©åœæœºåˆ¶")
                    # ä¿å­˜æœ€ç»ˆcheckpoint
                    try:
                        self.save_checkpoint(checkpoint_path, epoch, best_loss, patience_counter, self.training_history)
                    except Exception as e:
                        logger.error(f"ä¿å­˜æœ€ç»ˆcheckpointå¤±è´¥: {e}")
                    break
                
                # å®šæœŸæ¸…ç†CUDAç¼“å­˜ï¼ˆæ¯2ä¸ªepochï¼Œé˜²æ­¢CUDAé”™è¯¯ï¼‰
                if (epoch + 1) % 2 == 0 and torch.cuda.is_available():
                    try:
                        logger.info("ğŸ§¹ æ¸…ç†CUDAç¼“å­˜...")
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                    except Exception as e:
                        logger.warning(f"âš ï¸ CUDAç¼“å­˜æ¸…ç†å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {e}")
                
                logger.info(f"Epoch {epoch+1}/{num_epochs} å®Œæˆï¼\n")
                
            except Exception as e:
                logger.error(f"âŒ Epoch {epoch+1} è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
                logger.error(f"è¯¦ç»†é”™è¯¯å †æ ˆ:\n{traceback.format_exc()}")
                logger.error(f"ä¿å­˜ç´§æ€¥checkpoint...")
                # ä¿å­˜ç´§æ€¥checkpoint
                try:
                    emergency_checkpoint = os.path.join(script_dir, 'checkpoints', f'gat_checkpoint_emergency_epoch{epoch+1}.pth')
                    self.save_checkpoint(emergency_checkpoint, epoch, best_loss, patience_counter, self.training_history)
                    logger.info(f"âœ… ç´§æ€¥checkpointå·²ä¿å­˜: {emergency_checkpoint}")
                except Exception as save_error:
                    logger.error(f"ä¿å­˜ç´§æ€¥checkpointå¤±è´¥: {save_error}")
                logger.error(f"å°è¯•ç»§ç»­ä¸‹ä¸€ä¸ªepoch...")
                continue
        
        # ä¿å­˜è®­ç»ƒå†å²
        try:
            logger.info("å¼€å§‹ä¿å­˜è®­ç»ƒå†å²...")
            self.save_training_history()
            logger.info("è®­ç»ƒå†å²ä¿å­˜æˆåŠŸï¼")
        except Exception as e:
            logger.error(f"ä¿å­˜è®­ç»ƒå†å²å¤±è´¥: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        # âœ… ç¡®ä¿æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ï¼ˆæ¯ä¸ªepochéƒ½ä¼šä¿å­˜ä¸€æ¬¡ï¼Œè¿™é‡Œå†æ¬¡ç¡®ä¿ï¼‰
        try:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            model_final_path = os.path.join(script_dir, 'models', f'gat_model_{self.model_prefix}_final.pth')
            if not os.path.exists(model_final_path):
                self.save_model(model_final_path)
                logger.info(f"âœ… æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {os.path.basename(model_final_path)}")
            else:
                logger.info(f"âœ… æœ€ç»ˆæ¨¡å‹: {os.path.basename(model_final_path)} (å·²åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æŒç»­æ›´æ–°)")
        except Exception as e:
            logger.error(f"æ£€æŸ¥æœ€ç»ˆæ¨¡å‹å¤±è´¥: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        
        logger.info("")
        logger.info("="*70)
        logger.info("è®­ç»ƒå®Œæˆï¼")
        logger.info("="*70)
        logger.info(f"âœ… æœ€ä½³æ€»æŸå¤±: {best_loss:.6f}")
        logger.info(f"âœ… æœ€ä½³æ¨¡å‹: models/gat_model_{self.model_prefix}_best.pth (æ€»æŸå¤±æœ€ä½)")
        if best_fig12_nc_real > 0:
            logger.info(f"âœ… çœŸå®Fig12æœ€ä½³NCå€¼: {best_fig12_nc_real:.6f} (Epoch {best_fig12_epoch_real+1})")
            logger.info(f"âœ… çœŸå®Fig12-bestæ¨¡å‹: models/gat_model_{self.model_prefix}_best_fig12.pth")
            if best_fig12_nc_real >= 0.8:
                logger.info("ğŸ‰ æ­å–œï¼çœŸå®Fig12 Average NC å·²è¾¾åˆ°ç›®æ ‡ (>=0.8)")
            else:
                logger.info(f"ğŸ’¡ æç¤ºï¼šçœŸå®Fig12 Average NC ({best_fig12_nc_real:.6f}) å°šæœªè¾¾åˆ°ç›®æ ‡ (0.8)ï¼Œå»ºè®®ç»§ç»­ä¼˜åŒ–")
        logger.info(f"âœ… æœ€ç»ˆæ¨¡å‹: models/gat_model_IMPROVED_final.pth (æ¯ä¸ªepochæŒç»­æ›´æ–°)")
        logger.info("="*70)
        return best_loss
    
    def save_model(self, model_path):
        """ä¿å­˜æ¨¡å‹ï¼ˆå¸¦å®Œæ•´CUDAé”™è¯¯æ¢å¤æœºåˆ¶ï¼‰- ä¸‰å±‚å›é€€ç­–ç•¥"""
        model_path = os.path.abspath(model_path)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        model_dir = os.path.dirname(model_path)
        if not os.path.exists(model_dir):
            os.makedirs(model_dir, exist_ok=True)
        
        # ğŸ”§ æ–¹æ³•1ï¼šç›´æ¥ä¿å­˜ï¼ˆæ­£å¸¸æƒ…å†µï¼‰
        try:
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
            }, model_path)
            logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜: {model_path}")
            return
        except Exception as e1:
            logger.warning(f"âš ï¸ æ–¹æ³•1ç›´æ¥ä¿å­˜å¤±è´¥: {e1}")
        
        # ğŸ”§ æ–¹æ³•2ï¼šå°†state_dictå¤åˆ¶åˆ°CPUåä¿å­˜
        try:
            model_state_cpu = {k: v.cpu().clone().detach() for k, v in self.model.state_dict().items()}
            try:
                optimizer_state_cpu = {
                    k: ({kk: vv.cpu() if isinstance(vv, torch.Tensor) else vv 
                         for kk, vv in v.items()} if isinstance(v, dict) else 
                        [vvv.cpu() if isinstance(vvv, torch.Tensor) else vvv for vvv in v] if isinstance(v, list) else
                        v.cpu() if isinstance(v, torch.Tensor) else v)
                    for k, v in self.optimizer.state_dict().items()
                }
            except:
                optimizer_state_cpu = None  # ä¼˜åŒ–å™¨çŠ¶æ€å¯èƒ½æŸåï¼Œæ”¾å¼ƒ
            
            torch.save({
                'model_state_dict': model_state_cpu,
                'optimizer_state_dict': optimizer_state_cpu,
            }, model_path)
            logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜ï¼ˆæ–¹æ³•2-CPUæ‹·è´ï¼‰: {model_path}")
            return
        except Exception as e2:
            logger.warning(f"âš ï¸ æ–¹æ³•2 CPUæ‹·è´ä¿å­˜å¤±è´¥: {e2}")
        
        # ğŸ”§ æ–¹æ³•3ï¼šå°†æ•´ä¸ªæ¨¡å‹ç§»åˆ°CPUï¼Œåªä¿å­˜æ¨¡å‹æƒé‡ï¼ˆä¸ä¿å­˜ä¼˜åŒ–å™¨ï¼‰
        try:
            original_device = next(self.model.parameters()).device
            self.model.cpu()  # æ•´ä¸ªæ¨¡å‹ç§»åˆ°CPU
            
            model_state = {k: v.clone() for k, v in self.model.state_dict().items()}
            torch.save({'model_state_dict': model_state}, model_path)
            
            self.model.to(original_device)  # ç§»å›åŸè®¾å¤‡
            logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜ï¼ˆæ–¹æ³•3-ä»…æ¨¡å‹ï¼‰: {model_path}")
            return
        except Exception as e3:
            logger.error(f"âŒ æ–¹æ³•3ä¹Ÿå¤±è´¥: {e3}")
            # å°è¯•æ¢å¤æ¨¡å‹ä½ç½®
            try:
                self.model.to(self.device)
            except:
                pass
        
        # ğŸ”§ æ–¹æ³•4ï¼šåˆ›å»ºæ–°æ¨¡å‹å®ä¾‹å¹¶å¤åˆ¶æƒé‡ï¼ˆæœ€åæ‰‹æ®µï¼‰
        try:
            import copy
            model_copy = copy.deepcopy(self.model).cpu()
            torch.save({'model_state_dict': model_copy.state_dict()}, model_path)
            del model_copy
            logger.info(f"âœ… æ¨¡å‹å·²ä¿å­˜ï¼ˆæ–¹æ³•4-æ·±æ‹·è´ï¼‰: {model_path}")
            return
        except Exception as e4:
            logger.error(f"âŒ æ‰€æœ‰ä¿å­˜æ–¹æ³•éƒ½å¤±è´¥äº†: {e4}")
            raise RuntimeError(f"æ— æ³•ä¿å­˜æ¨¡å‹åˆ° {model_path}ï¼Œæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥")
    
    def save_checkpoint(self, checkpoint_path, epoch, best_loss, patience_counter, training_history):
        """
        ä¿å­˜å®Œæ•´çš„è®­ç»ƒcheckpointï¼ˆç”¨äºä¸­æ–­æ¢å¤ï¼‰- å¸¦å®Œæ•´CUDAé”™è¯¯æ¢å¤æœºåˆ¶
        
        Args:
            checkpoint_path: checkpointä¿å­˜è·¯å¾„
            epoch: å½“å‰epoch
            best_loss: æœ€ä½³æ€»æŸå¤±
            patience_counter: æ—©åœè®¡æ•°å™¨
            training_history: è®­ç»ƒå†å²è®°å½•
        """
        checkpoint_path = os.path.abspath(checkpoint_path)
        checkpoint_dir = os.path.dirname(checkpoint_path)
        
        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir, exist_ok=True)
        
        # ğŸ”§ ç¬¬ä¸€æ­¥ï¼šè·å–scalerçŠ¶æ€ï¼ˆä¸ä¾èµ–GPUï¼‰
        scaler_state = None
        if self.use_amp and self.scaler is not None:
            try:
                scaler_state = self.scaler.state_dict()
            except Exception as e:
                logger.warning(f"âš ï¸ GradScalerçŠ¶æ€è¯»å–å¤±è´¥: {e}")
                try:
                    from torch.cuda.amp import GradScaler
                    scaler_state = GradScaler().state_dict()
                except:
                    scaler_state = None
        
        # ğŸ”§ ç¬¬äºŒæ­¥ï¼šå®‰å…¨è·å–æ‰€æœ‰çŠ¶æ€ï¼ˆå¤šå±‚å›é€€ï¼‰
        model_state = None
        optimizer_state = None
        scheduler_state = None
        
        # æ–¹æ³•1ï¼šç›´æ¥è·å–
        try:
            model_state = {k: v.clone().detach() for k, v in self.model.state_dict().items()}
            optimizer_state = self.optimizer.state_dict()
            scheduler_state = self.scheduler.state_dict() if self.scheduler else None
        except Exception as e1:
            logger.warning(f"âš ï¸ ç›´æ¥è·å–çŠ¶æ€å¤±è´¥: {e1}")
            # æ–¹æ³•2ï¼šç§»åˆ°CPUåè·å–
            try:
                model_state = {k: v.cpu().clone().detach() for k, v in self.model.state_dict().items()}
                optimizer_state = {
                    k: ({kk: vv.cpu() if isinstance(vv, torch.Tensor) else vv for kk, vv in v.items()} 
                        if isinstance(v, dict) else v)
                    for k, v in self.optimizer.state_dict().items()
                }
                scheduler_state = self.scheduler.state_dict() if self.scheduler else None
            except Exception as e2:
                logger.error(f"âš ï¸ CPUæ–¹å¼è·å–çŠ¶æ€ä¹Ÿå¤±è´¥: {e2}")
                # æ–¹æ³•3ï¼šå°†æ•´ä¸ªæ¨¡å‹ç§»åˆ°CPU
                try:
                    original_device = next(self.model.parameters()).device
                    self.model.cpu()
                    model_state = {k: v.clone() for k, v in self.model.state_dict().items()}
                    self.model.to(original_device)
                    optimizer_state = None  # æ”¾å¼ƒä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
                    scheduler_state = None
                except Exception as e3:
                    logger.error(f"âŒ æ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥: {e3}")
                    return  # æ”¾å¼ƒä¿å­˜checkpoint
        
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': model_state,
            'optimizer_state_dict': optimizer_state,
            'scheduler_state_dict': scheduler_state,
            'scaler_state_dict': scaler_state,
            'best_loss': best_loss,
            'patience_counter': patience_counter,
            'training_history': training_history,
            'adaptive_temp': {
                'init_temp': self.adaptive_temp.init_temp,
                'final_temp': self.adaptive_temp.final_temp,
                'total_epochs': self.adaptive_temp.total_epochs
            }
        }
        
        # ğŸ”§ ç¬¬ä¸‰æ­¥ï¼šå®‰å…¨ä¿å­˜ï¼ˆé¿å…ä»»ä½•CUDAæ“ä½œï¼‰
        try:
            torch.save(checkpoint, checkpoint_path)
            logger.info(f"âœ… Checkpointå·²ä¿å­˜: Epoch {epoch+1}, è·¯å¾„: {checkpoint_path}")
        except Exception as e:
            logger.warning(f"âš ï¸ Checkpointä¿å­˜å¤±è´¥: {e}")
            # ä¸è¦è°ƒç”¨torch.cuda.empty_cache()ï¼Œå› ä¸ºå¯èƒ½è§¦å‘CUDAé”™è¯¯
            # ç›´æ¥æ”¾å¼ƒæœ¬æ¬¡checkpointä¿å­˜
            logger.warning(f"âš ï¸ æ”¾å¼ƒæœ¬æ¬¡checkpointä¿å­˜ï¼ˆç»§ç»­è®­ç»ƒï¼‰")
    
    def load_checkpoint(self, checkpoint_path):
        """
        åŠ è½½checkpointä»¥æ¢å¤è®­ç»ƒ
        
        Args:
            checkpoint_path: checkpointè·¯å¾„
            
        Returns:
            checkpointå­—å…¸ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨åˆ™è¿”å›None
        """
        checkpoint_path = os.path.abspath(checkpoint_path)
        
        if not os.path.exists(checkpoint_path):
            logger.warning(f"Checkpointæ–‡ä»¶ä¸å­˜åœ¨: {checkpoint_path}")
            return None
        
        try:
            # PyTorch 2.6+éœ€è¦è®¾ç½®weights_only=Falseæ¥åŠ è½½åŒ…å«numpyå¯¹è±¡çš„checkpoint
            checkpoint = torch.load(checkpoint_path, map_location=self.device, weights_only=False)
            
            # âœ… æ™ºèƒ½åŠ è½½ï¼šå°è¯•ä¸¥æ ¼åŠ è½½ï¼Œå¤±è´¥åˆ™å°è¯•éƒ¨åˆ†åŠ è½½
            try:
                self.model.load_state_dict(checkpoint['model_state_dict'], strict=True)
                logger.info("âœ… å®Œæ•´åŠ è½½æ¨¡å‹å‚æ•°ï¼ˆæ¶æ„å®Œå…¨åŒ¹é…ï¼‰")
            except RuntimeError as e:
                # æ¶æ„ä¸åŒ¹é…ï¼Œå°è¯•éƒ¨åˆ†åŠ è½½
                if "size mismatch" in str(e) or "Missing key" in str(e) or "Unexpected key" in str(e):
                    logger.warning("âš ï¸  æ¨¡å‹æ¶æ„å·²æ”¹å˜ï¼Œå°è¯•éƒ¨åˆ†åŠ è½½å…¼å®¹å‚æ•°...")
                    
                    # è·å–å½“å‰æ¨¡å‹å’Œcheckpointçš„å‚æ•°
                    model_state = self.model.state_dict()
                    checkpoint_state = checkpoint['model_state_dict']
                    
                    # ç»Ÿè®¡å…¼å®¹/ä¸å…¼å®¹å‚æ•°
                    compatible_params = 0
                    incompatible_params = 0
                    new_params = 0
                    
                    for name, param in checkpoint_state.items():
                        if name in model_state:
                            if model_state[name].shape == param.shape:
                                model_state[name] = param
                                compatible_params += 1
                            else:
                                incompatible_params += 1
                                logger.debug(f"   è·³è¿‡ï¼ˆå½¢çŠ¶ä¸åŒ¹é…ï¼‰: {name}")
                        else:
                            incompatible_params += 1
                            logger.debug(f"   è·³è¿‡ï¼ˆæ—§å‚æ•°ï¼‰: {name}")
                    
                    # ç»Ÿè®¡æ–°å¢å‚æ•°
                    for name in model_state.keys():
                        if name not in checkpoint_state:
                            new_params += 1
                            logger.debug(f"   æ–°å¢å‚æ•°ï¼ˆéšæœºåˆå§‹åŒ–ï¼‰: {name}")
                    
                    # åŠ è½½å…¼å®¹å‚æ•°
                    self.model.load_state_dict(model_state, strict=True)
                    
                    logger.info(f"ğŸ“Š éƒ¨åˆ†åŠ è½½ç»Ÿè®¡:")
                    logger.info(f"   âœ… å…¼å®¹å‚æ•°: {compatible_params}")
                    logger.info(f"   âŒ ä¸å…¼å®¹å‚æ•°: {incompatible_params}")
                    logger.info(f"   ğŸ†• æ–°å¢å‚æ•°: {new_params}")
                    logger.warning("âš ï¸  æ¶æ„æ”¹å˜å¯èƒ½å½±å“æ€§èƒ½ï¼Œå»ºè®®ä»å¤´è®­ç»ƒä»¥è·å¾—æœ€ä½³æ•ˆæœ")
                else:
                    raise  # å…¶ä»–é”™è¯¯ï¼Œç»§ç»­æŠ›å‡º
            
            # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚æœå…¼å®¹ï¼‰
            try:
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
            except Exception as e:
                logger.warning(f"âš ï¸  ä¼˜åŒ–å™¨çŠ¶æ€ä¸å…¼å®¹ï¼Œä½¿ç”¨æ–°åˆå§‹åŒ–: {e}")
            
            if checkpoint.get('scheduler_state_dict') and self.scheduler:
                self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
            
            if checkpoint.get('scaler_state_dict') and self.use_amp:
                self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
            
            # æ¢å¤è‡ªé€‚åº”æ¸©åº¦å‚æ•°
            if 'adaptive_temp' in checkpoint:
                temp_config = checkpoint['adaptive_temp']
                self.adaptive_temp.init_temp = temp_config['init_temp']
                self.adaptive_temp.final_temp = temp_config['final_temp']
                self.adaptive_temp.total_epochs = temp_config['total_epochs']
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½checkpoint: Epoch {checkpoint['epoch']+1}")
            # âœ… å…¼å®¹æ–°æ—§checkpoint
            if 'best_loss' in checkpoint:
                logger.info(f"   æœ€ä½³æ€»æŸå¤±: {checkpoint['best_loss']:.6f}")
            elif 'best_val_nc' in checkpoint:
                logger.info(f"   æœ€ä½³éªŒè¯NCå€¼: {checkpoint['best_val_nc']:.4f} (æ—§checkpoint)")
            logger.info(f"   è€å¿ƒè®¡æ•°: {checkpoint['patience_counter']}")
            
            return checkpoint
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½checkpointå¤±è´¥: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        try:
            # ä½¿ç”¨ç»å¯¹è·¯å¾„
            script_dir = os.path.dirname(os.path.abspath(__file__))
            history_dir = os.path.join(script_dir, "logs")
            
            if not os.path.exists(history_dir):
                os.makedirs(history_dir, exist_ok=True)
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            history_file = os.path.join(history_dir, f"training_history_IMPROVED_{timestamp}.json")
            
            # è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
            history_data = {}
            for key, value in self.training_history.items():
                try:
                    if key == 'feature_stats':
                        history_data[key] = value
                    else:
                        history_data[key] = [float(v) if hasattr(v, 'item') else v for v in value]
                except Exception as e:
                    logger.error(f"[save_training_history] è½¬æ¢é”® '{key}' å¤±è´¥: {e}")
                    history_data[key] = []
            
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(history_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {history_file}")
            
            # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
            self.plot_training_curves(history_data, history_dir, timestamp)
        except Exception as e:
            logger.error(f"[save_training_history] ä¿å­˜å¤±è´¥: {e}")
            logger.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            raise
    
    def plot_training_curves(self, history_data, save_dir, timestamp):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆSCIå­¦æœ¯è®ºæ–‡é£æ ¼ï¼‰"""
        try:
            import matplotlib.pyplot as plt
            import matplotlib as mpl
            from matplotlib import rcParams
            
            # ============ SCIè®ºæ–‡é£æ ¼é…ç½® ============
            # è®¾ç½®å­—ä½“ä¸ºTimes New Romanï¼ˆå­¦æœ¯è®ºæ–‡æ ‡å‡†ï¼‰
            rcParams['font.family'] = 'serif'
            rcParams['font.serif'] = ['Times New Roman', 'DejaVu Serif', 'serif']
            rcParams['font.size'] = 10
            rcParams['axes.labelsize'] = 11
            rcParams['axes.titlesize'] = 12
            rcParams['xtick.labelsize'] = 10
            rcParams['ytick.labelsize'] = 10
            rcParams['legend.fontsize'] = 9
            
            # è®¾ç½®çº¿æ¡å’Œæ ‡è®°æ ·å¼
            rcParams['lines.linewidth'] = 1.5
            rcParams['lines.markersize'] = 4
            
            # è®¾ç½®åæ ‡è½´æ ·å¼
            rcParams['axes.linewidth'] = 1.0
            rcParams['axes.grid'] = True
            rcParams['grid.alpha'] = 0.3
            rcParams['grid.linestyle'] = '--'
            rcParams['grid.linewidth'] = 0.5
            
            # è®¾ç½®å›¾ä¾‹æ ·å¼
            rcParams['legend.frameon'] = True
            rcParams['legend.framealpha'] = 0.9
            rcParams['legend.edgecolor'] = 'black'
            
            # Colorblind-friendlyé¢œè‰²æ–¹æ¡ˆï¼ˆå­¦æœ¯è®ºæ–‡æ¨èï¼‰
            colors = {
                'blue': '#0173B2',      # è“è‰²
                'orange': '#DE8F05',    # æ©™è‰²
                'green': '#029E73',     # ç»¿è‰²
                'red': '#CC78BC',       # çº¢è‰²
                'cyan': '#56B4E9',      # é’è‰²
                'magenta': '#CA9161',   # æ£•è‰²
                'purple': '#949494'     # ç°è‰²
            }
            
            epochs = range(1, len(history_data['epoch_losses']) + 1)
            
            # ============ åˆ›å»ºå›¾è¡¨ï¼ˆåŒæ å¸ƒå±€ï¼š4è¡Œ2åˆ—ï¼Œå¢åŠ æ¸©åº¦å­å›¾ï¼‰============
            fig, axes = plt.subplots(4, 2, figsize=(7.0, 11.0))  # 7è‹±å¯¸å®½åº¦é€‚åˆåŒæ è®ºæ–‡
            
            # (a) æ€»æŸå¤±
            ax = axes[0, 0]
            ax.plot(epochs, history_data['epoch_losses'], 
                   color=colors['blue'], linewidth=1.5, label='Total Loss')
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Loss')
            ax.set_title('(a) Total Training Loss')
            ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
            
            # (b) å¯¹æ¯”æŸå¤±ï¼ˆInfoNCEï¼‰
            ax = axes[0, 1]
            ax.plot(epochs, history_data['contrastive_losses'], 
                   color=colors['orange'], linewidth=1.5, label='Contrastive Loss')
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Loss')
            ax.set_title('(b) Contrastive Loss (InfoNCE)')
            ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
            
            # (c) äºŒå€¼åŒ–ä¸€è‡´æ€§æŸå¤±ï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰
            ax = axes[1, 0]
            ax.plot(epochs, history_data['binary_consistency_losses'], 
                   color=colors['green'], linewidth=1.5, label='Binary Consistency Loss')
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Loss')
            ax.set_title('(c) Binary Consistency Loss')
            ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
            
            # (d) éªŒè¯NCå€¼ï¼ˆâœ… æ¯ä¸ªepochè¯„ä¼°ï¼‰
            ax = axes[1, 1]
            val_nc_values = history_data.get('val_nc_values', [])
            if val_nc_values:
                # âœ… ä¿®å¤ï¼šä½¿ç”¨å®é™…çš„val_nc_valuesé•¿åº¦ï¼ŒåŒæ—¶å…¼å®¹æ—§historyï¼ˆå¯èƒ½ä¸å­˜åœ¨è¯¥é”®ï¼‰
                val_nc_len = len(val_nc_values)
                val_epochs = range(1, val_nc_len + 1)
                ax.plot(val_epochs, val_nc_values, 
                       color=colors['red'], linewidth=1.5, 
                       marker='o', markersize=4, markerfacecolor='white',
                       markeredgewidth=1.5, label='Validation NC')
                ax.set_ylim([0, 1.0])  # NCå€¼èŒƒå›´0-1
            ax.set_xlabel('Epoch')
            ax.set_ylabel('NC Value')
            ax.set_title('(d) Validation NC Value')
            ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
            
            # (e) å­¦ä¹ ç‡ï¼ˆOneCycleLRï¼‰
            ax = axes[2, 0]
            ax.plot(epochs, history_data['learning_rates'], 
                   color=colors['cyan'], linewidth=1.5, label='Learning Rate')
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Learning Rate')
            ax.set_title('(e) Learning Rate Schedule')
            ax.set_yscale('log')  # å­¦ä¹ ç‡ç”¨å¯¹æ•°åæ ‡
            ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
            
            # (f) æ¢¯åº¦èŒƒæ•°
            ax = axes[2, 1]
            ax.plot(epochs, history_data['gradient_norms'], 
                   color=colors['magenta'], linewidth=1.5, label='Gradient Norm')
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Gradient Norm')
            ax.set_title('(f) Gradient Norm')
            ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
            
            # (g) è‡ªé€‚åº”æ¸©åº¦ â­æ–°å¢
            ax = axes[3, 0]
            if 'temperatures' in history_data and len(history_data['temperatures']) > 0:
                ax.plot(epochs, history_data['temperatures'], 
                       color='#E91E63', linewidth=2.0, label='Temperature')
                ax.axhline(y=1.0, color='gray', linestyle='--', linewidth=0.8, alpha=0.5, label='Init')
                ax.axhline(y=0.01, color='gray', linestyle='--', linewidth=0.8, alpha=0.5, label='Final')
                ax.set_yscale('log')  # å¯¹æ•°åæ ‡
            ax.set_xlabel('Epoch')
            ax.set_ylabel('Temperature')
            ax.set_title('(g) Adaptive Temperature (Annealing)')
            ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
            ax.legend(loc='upper right', fontsize=8)
            
            # (h) æŸå¤±æƒé‡å˜åŒ–ï¼ˆå¯é€‰ï¼Œæš‚æ—¶ç©ºç™½æˆ–æ·»åŠ è¯´æ˜ï¼‰
            ax = axes[3, 1]
            ax.text(0.5, 0.5, 'Adaptive Loss Weights\n(Dynamic During Training)', 
                   ha='center', va='center', fontsize=10, color='gray')
            ax.set_title('(h) Training Strategy')
            ax.axis('off')
            
            # è°ƒæ•´å­å›¾é—´è·
            plt.tight_layout()
            
            # ä¿å­˜ä¸ºé«˜åˆ†è¾¨ç‡PNGï¼ˆç”¨äºè®ºæ–‡ï¼‰
            plot_file_png = os.path.join(save_dir, f"training_curves_SCI_{timestamp}.png")
            plt.savefig(plot_file_png, dpi=300, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
            
            # åŒæ—¶ä¿å­˜ä¸ºçŸ¢é‡æ ¼å¼PDFï¼ˆç”¨äºè®ºæ–‡ç»ˆç¨¿ï¼‰
            plot_file_pdf = os.path.join(save_dir, f"training_curves_SCI_{timestamp}.pdf")
            plt.savefig(plot_file_pdf, format='pdf', bbox_inches='tight',
                       facecolor='white', edgecolor='none')
            
            plt.close()
            
            logger.info(f"SCIé£æ ¼è®­ç»ƒæ›²çº¿å·²ä¿å­˜:")
            logger.info(f"  PNG (300 DPI): {plot_file_png}")
            logger.info(f"  PDF (çŸ¢é‡å›¾): {plot_file_pdf}")
            
            # ============ é¢å¤–ï¼šç”Ÿæˆå•ç‹¬çš„NCå€¼æ›²çº¿ï¼ˆç”¨äºè®ºæ–‡é‡ç‚¹å±•ç¤ºï¼‰============
            val_nc_values = history_data.get('val_nc_values', [])
            if val_nc_values:
                fig_nc, ax_nc = plt.subplots(1, 1, figsize=(3.5, 2.8))  # å•æ å®½åº¦
                
                val_epochs = range(3, len(epochs)+1, 3)[:len(val_nc_values)]
                ax_nc.plot(val_epochs, val_nc_values, 
                          color=colors['red'], linewidth=2.0, 
                          marker='o', markersize=5, markerfacecolor='white',
                          markeredgewidth=1.5, label='Validation NC')
                
                ax_nc.set_xlabel('Epoch', fontsize=11)
                ax_nc.set_ylabel('NC Value', fontsize=11)
                ax_nc.set_title('Validation NC Value (Zero-Watermark Robustness)', fontsize=12)
                ax_nc.set_ylim([0, 1.0])
                ax_nc.grid(True, alpha=0.3, linestyle='--', linewidth=0.5)
                ax_nc.legend(loc='lower right', frameon=True)
                
                plt.tight_layout()
                
                nc_file_png = os.path.join(save_dir, f"nc_value_SCI_{timestamp}.png")
                nc_file_pdf = os.path.join(save_dir, f"nc_value_SCI_{timestamp}.pdf")
                
                plt.savefig(nc_file_png, dpi=300, bbox_inches='tight',
                           facecolor='white', edgecolor='none')
                plt.savefig(nc_file_pdf, format='pdf', bbox_inches='tight',
                           facecolor='white', edgecolor='none')
                
                plt.close()
                
                logger.info(f"  NCå€¼å•ç‹¬å›¾: {nc_file_png}, {nc_file_pdf}")
            
        except ImportError as e:
            logger.warning(f"matplotlibæœªå®‰è£…æˆ–å¯¼å…¥å¤±è´¥: {e}ï¼Œè·³è¿‡è®­ç»ƒæ›²çº¿ç»˜åˆ¶")
        except Exception as e:
            logger.error(f"ç»˜åˆ¶è®­ç»ƒæ›²çº¿æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

class GraphDataLoader:
    """å›¾æ•°æ®åŠ è½½å™¨"""

    BLACKLIST = [
        'tianjin-latest-free.shp-gis_osm_landuse_a_free_1',
        'tianjin-latest-free.shp-gis_osm_traffic_free_1',
        'H51-HYDL',
        'tianjin-latest-free.shp-gis_osm_railways_free_1',
        'H51-AANP',
        'H51-LRDL'
    ]

    def __init__(self, graph_dir=None, max_nodes=30000):
        if graph_dir is None:
            script_dir = os.path.dirname(os.path.abspath(__file__))
            project_root = os.path.dirname(script_dir)
            graph_dir = os.path.join(project_root, 'convertToGraph', 'Graph', 'TrainingSet')

        self.graph_dir = os.path.abspath(graph_dir)
        self.max_nodes = max_nodes
        logger.info(f"å›¾æ•°æ®åŠ è½½è·¯å¾„: {self.graph_dir}")

    def load_graph_data(self, max_nodes=None):
        threshold = max_nodes if max_nodes is not None else self.max_nodes
        original_dir = os.path.join(self.graph_dir, 'Original')
        attacked_dir = os.path.join(self.graph_dir, 'Attacked')

        if not os.path.exists(original_dir):
            logger.warning(f"åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {original_dir}")
            return {}, {}

        logger.info(f"å¼€å§‹åŠ è½½å›¾æ•°æ®ï¼ˆè¿‡æ»¤>{threshold:,}èŠ‚ç‚¹çš„è¶…å¤§å›¾ï¼‰...")
        original_graphs, filtered_graphs = self._load_original_graphs(original_dir, threshold)
        attacked_graphs, total_attacked_loaded, total_attacked_filtered = self._load_attacked_graphs(attacked_dir, original_graphs)

        self._log_dataset_stats(len(original_graphs), total_attacked_loaded)
        self._log_filter_summary(filtered_graphs, total_attacked_filtered, len(original_graphs), threshold)

        return original_graphs, attacked_graphs

    def _load_original_graphs(self, original_dir, max_nodes):
        original_graphs = {}
        filtered_graphs = []

        for filename in os.listdir(original_dir):
            if not filename.endswith('_graph.pkl'):
                continue
            graph_name = filename.replace('_graph.pkl', '')
            filepath = os.path.join(original_dir, filename)
            graph_data = self._load_graph_pickle(filepath)

            if graph_name in self.BLACKLIST:
                logger.warning(f"  â›” é»‘åå•è¿‡æ»¤: {graph_name} (æ‰‹åŠ¨æ’é™¤)")
                self._record_filtered_graph(filtered_graphs, graph_name, graph_data, 'é»‘åå•')
                continue

            num_nodes = graph_data.x.shape[0]
            if num_nodes > max_nodes:
                logger.warning(f"  è¿‡æ»¤è¶…å¤§å›¾: {graph_name} ({num_nodes:,}èŠ‚ç‚¹, {graph_data.edge_index.shape[1]:,}è¾¹)")
                self._record_filtered_graph(filtered_graphs, graph_name, graph_data, 'è¶…å¤§å›¾')
                continue

            original_graphs[graph_name] = graph_data

        return original_graphs, filtered_graphs

    def _load_attacked_graphs(self, attacked_dir, original_graphs):
        attacked_graphs = {}
        total_attacked_loaded = 0
        total_attacked_filtered = 0

        if not os.path.exists(attacked_dir):
            return attacked_graphs, total_attacked_loaded, total_attacked_filtered

        for subdir in os.listdir(attacked_dir):
            subdir_path = os.path.join(attacked_dir, subdir)
            if not os.path.isdir(subdir_path):
                continue

            attack_files = [f for f in os.listdir(subdir_path) if f.endswith('_graph.pkl')]
            if subdir not in original_graphs:
                total_attacked_filtered += len(attack_files)
                continue

            attacked_graphs[subdir] = []
            for filename in attack_files:
                attack_path = os.path.join(subdir_path, filename)
                graph_data = self._load_graph_pickle(attack_path)
                attack_name = filename.replace('_graph.pkl', '')
                if 'compound_seq' in attack_name.lower() and 'full_chain' not in attack_name.lower():
                    attack_name = f"{attack_name}_full_chain"
                graph_data.attack_type = attack_name
                attacked_graphs[subdir].append(graph_data)
                total_attacked_loaded += 1

        return attacked_graphs, total_attacked_loaded, total_attacked_filtered

    def _record_filtered_graph(self, filtered_graphs, graph_name, graph_data, reason):
        filtered_graphs.append({
            'name': graph_name,
            'nodes': graph_data.x.shape[0],
            'edges': graph_data.edge_index.shape[1],
            'reason': reason
        })

    @staticmethod
    def _load_graph_pickle(filepath):
        with open(filepath, 'rb') as f:
            return pickle.load(f)

    def _log_dataset_stats(self, original_count, attacked_loaded):
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {original_count} ä¸ªåŸå§‹å›¾")
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {attacked_loaded} ä¸ªè¢«æ”»å‡»çš„å›¾")

    def _log_filter_summary(self, filtered_graphs, total_attacked_filtered, original_count, max_nodes):
        if not filtered_graphs:
            return

        logger.warning("")
        blacklist_count = sum(1 for fg in filtered_graphs if fg.get('reason') == 'é»‘åå•')
        large_count = sum(1 for fg in filtered_graphs if fg.get('reason') == 'è¶…å¤§å›¾')
        logger.warning(f"âš ï¸  è¿‡æ»¤äº† {len(filtered_graphs)} ä¸ªåŸå§‹å›¾")
        logger.warning(f"   â”œâ”€ é»‘åå•: {blacklist_count} ä¸ª")
        logger.warning(f"   â””â”€ è¶…å¤§å›¾(>{max_nodes:,}èŠ‚ç‚¹): {large_count} ä¸ª")
        logger.warning(f"âš ï¸  è¿‡æ»¤äº† {total_attacked_filtered} ä¸ªå¯¹åº”çš„æ”»å‡»å›¾")

        total_graphs = original_count + len(filtered_graphs)
        if total_graphs > 0:
            retention = original_count / total_graphs * 100
            logger.warning(f"âš ï¸  æ•°æ®ä¿ç•™ç‡: {retention:.1f}%")

        logger.warning("")
        logger.warning("è¢«è¿‡æ»¤çš„å›¾åˆ—è¡¨ï¼š")
        for fg in filtered_graphs:
            reason = fg.get('reason', 'æœªçŸ¥')
            logger.warning(f"   - {fg['name']}: {fg['nodes']:,} èŠ‚ç‚¹, {fg['edges']:,} è¾¹ [{reason}]")
        logger.warning("")
        logger.info("âœ… è¿‡æ»¤é—®é¢˜å›¾åï¼Œé¿å…è®­ç»ƒè¿‡ç¨‹ä¸­OOM")
        logger.warning("")


def main():
    """ä¸»å‡½æ•°"""
    # â­ è®­ç»ƒæ¨¡å¼ï¼šè®¾ç½®å®Œæ•´çš„æ–‡ä»¶æ—¥å¿—
    global logger
    logger = setup_logging()
    
    logger.info("="*70)
    logger.info("ç¬¬ä¸‰æ­¥ï¼šæ”¹è¿›çš„GATæ¨¡å‹è®­ç»ƒ - çŸ¢é‡åœ°å›¾é›¶æ°´å°é²æ£’ç‰¹å¾æå–")
    logger.info("="*70)
    log_training_overview()
    device = log_device_info()
    
    # åŠ è½½æ•°æ®
    data_loader = GraphDataLoader()
    original_graphs, attacked_graphs = data_loader.load_graph_data()
    
    if len(original_graphs) == 0:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°åŸå§‹å›¾æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œç¬¬äºŒæ­¥")
        return
    
    if len(attacked_graphs) == 0:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°è¢«æ”»å‡»çš„å›¾æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œç¬¬äºŒæ­¥")
        return
    
    input_dim = infer_input_dim(original_graphs)
    log_feature_profile(input_dim)
    
    # åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹
    model = ImprovedGATModel(
        input_dim=input_dim,
        hidden_dim=256,  # å¢åŠ åˆ°256ï¼ˆåŸæ¥128ï¼‰
        output_dim=1024,
        num_heads=8,  # å¢åŠ åˆ°8ï¼ˆåŸæ¥4ï¼‰
        dropout=0.3
    )
    
    log_model_summary(model)
    configured_bs = resolve_batch_size()
    logger.info(f"âœ… æ•°æ®åŠ è½½æ—¶å·²è¿‡æ»¤>30000èŠ‚ç‚¹çš„è¶…å¤§å›¾")
    logger.info("")
    
    # åˆ›å»ºæ”¹è¿›çš„è®­ç»ƒå™¨ï¼ˆæ¸©åº¦å‚æ•°æé«˜ä»¥å¢å¼ºæ•°å€¼ç¨³å®šæ€§ï¼‰â­ä¿®å¤NaN
    trainer = ImprovedContrastiveTrainer(
        model,
        device,
        temperature=0.1,  # ä»0.07æå‡è‡³0.1ï¼Œå‡å°‘expæº¢å‡ºé£é™©
        use_amp=(device=='cuda'),
        batch_size=configured_bs
    )
    
    logger.info("")
    logger.info("="*70)
    logger.info("å¼€å§‹è®­ç»ƒ...")
    logger.info("="*70)
    logger.info("")
    
    script_dir = os.path.dirname(os.path.abspath(__file__))
    checkpoint_path = os.path.join(script_dir, 'checkpoints', 'gat_checkpoint_latest.pth')
    resume_from = resolve_checkpoint_choice(checkpoint_path)
    
    # è®­ç»ƒæ¨¡å‹ï¼ˆâœ… è°ƒæ•´ä¸º20ä¸ªepochï¼šå‰æœŸå”¯ä¸€æ€§â†’ä¸­æœŸå¹³è¡¡â†’åæœŸé²æ£’æ€§ï¼‰
    best_loss = trainer.train(
        original_graphs,
        attacked_graphs,
        num_epochs=12,  # â­ä¼˜åŒ–ï¼šä»20å‡å°‘åˆ°12ï¼Œå› ä¸ºEpoch 1å°±è¾¾åˆ°æœ€ä½³é²æ£’æ€§ï¼Œä¸éœ€è¦é‚£ä¹ˆå¤šepoch
        resume_from_checkpoint=resume_from
    )
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    script_dir = os.path.dirname(os.path.abspath(__file__))
    final_model_path = os.path.join(script_dir, 'models', 'gat_model_IMPROVED.pth')
    best_model_path = os.path.join(script_dir, 'models', 'gat_model_IMPROVED_best.pth')
    
    trainer.save_model(final_model_path)
    
    logger.info("")
    logger.info("="*70)
    logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    logger.info("="*70)
    logger.info(f"âœ… æœ€ä½³æ€»æŸå¤±: {best_loss:.6f}")
    logger.info(f"âœ… æœ€ç»ˆæ¨¡å‹ä¿å­˜åˆ°: {final_model_path}")
    logger.info(f"âœ… æœ€ä½³æ¨¡å‹ä¿å­˜åˆ°: {best_model_path} (æ€»æŸå¤±æœ€ä½)")
    logger.info("")
    logger.info("æ¨¡å‹å°†ç”¨äºï¼š")
    logger.info("  1. ä»åŸå§‹çŸ¢é‡åœ°å›¾æå–1024ç»´é²æ£’ç‰¹å¾")
    logger.info("  2. äºŒå€¼åŒ–åä¸ç‰ˆæƒå›¾åƒXORç”Ÿæˆé›¶æ°´å°")
    logger.info("  3. éªŒè¯é˜¶æ®µæå–ç‰¹å¾å¹¶æ¢å¤ç‰ˆæƒå›¾åƒ")
    logger.info("="*70)

if __name__ == "__main__":
    main()

