#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬¬ä¸‰æ­¥ï¼šGCNæ¨¡å‹è®­ç»ƒ - çŸ¢é‡åœ°å›¾é›¶æ°´å°é²æ£’ç‰¹å¾æå–
ä½¿ç”¨GCNç»“åˆå¯¹æ¯”å­¦ä¹ è®­ç»ƒæ¨¡å‹ï¼Œæå–æŠµæŠ—RSTæ”»å‡»çš„é²æ£’ç‰¹å¾
"""

import os
import pickle
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, global_mean_pool, global_max_pool
from torch.cuda import amp
import numpy as np
from tqdm import tqdm
import json
import logging
from datetime import datetime
import glob

# è®¾ç½®æ—¥å¿—
def setup_logging():
    """è®¾ç½®æ—¥å¿—è®°å½•ï¼ˆæŒ‰æ—¶é—´æˆ³+PIDç”Ÿæˆå”¯ä¸€æ–‡ä»¶ï¼Œå¹¶ç»´æŠ¤ latest æ–‡ä»¶ï¼‰"""
    # å°†æ—¥å¿—è¾“å‡ºåˆ°VGCNæ–‡ä»¶å¤¹ä¸‹
    base_dir = os.path.dirname(__file__)
    log_dir = os.path.join(base_dir, "logs")
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    pid = os.getpid()
    log_file = os.path.join(log_dir, f"step3_training_{timestamp}_{pid}.log")
    latest_file = os.path.join(log_dir, "step3_training_latest.log")

    # æ¸…ç†å¯èƒ½å­˜åœ¨çš„é‡å¤ handlerï¼ˆé€‚é…é‡å¤åˆå§‹åŒ–çš„åœºæ™¯ï¼‰
    root_logger = logging.getLogger()
    if root_logger.handlers:
        for h in list(root_logger.handlers):
            root_logger.removeHandler(h)

    # é…ç½®ä¸¤ä¸ªæ–‡ä»¶è¾“å‡ºï¼šå”¯ä¸€æ—¥å¿—æ–‡ä»¶ + latest å¿«ç…§
    file_handler_unique = logging.FileHandler(log_file, encoding='utf-8')
    file_handler_latest = logging.FileHandler(latest_file, mode='w', encoding='utf-8')
    console_handler = logging.StreamHandler()

    for h in (file_handler_unique, file_handler_latest, console_handler):
        h.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))

    root_logger.setLevel(logging.INFO)
    root_logger.addHandler(file_handler_unique)
    root_logger.addHandler(file_handler_latest)
    root_logger.addHandler(console_handler)

    # å°†å½“å‰æ—¥å¿—è·¯å¾„æš´éœ²ä¸ºæ¨¡å—çº§å˜é‡ï¼Œä¾¿äºå¤–éƒ¨è¯»å–
    globals()["CURRENT_LOG_FILE"] = log_file
    globals()["CURRENT_LATEST_LOG"] = latest_file
    os.environ["VGCN_CURRENT_LOG"] = log_file
    os.environ["VGCN_CURRENT_LOG_LATEST"] = latest_file

    logger = logging.getLogger(__name__)
    logger.info(f"æ—¥å¿—æ–‡ä»¶: {log_file}")
    logger.info(f"æœ€æ–°æ—¥å¿—(è¦†ç›–): {latest_file}")
    return logger

logger = setup_logging()

class GCNModel(nn.Module):
    """ä½¿ç”¨GCNæ¨¡å‹æå–çŸ¢é‡åœ°å›¾çš„é²æ£’ç‰¹å¾"""
    
    def __init__(self, input_dim, hidden_dim=128, output_dim=1024, dropout=0.2):
        super(GCNModel, self).__init__()
        
        # GCNå±‚ï¼šæå–å›¾ç»“æ„ç‰¹å¾ï¼ˆä½¿ç”¨æ”¹è¿›çš„å¯¹ç§°å½’ä¸€åŒ–ï¼‰
        self.gcn1 = GCNConv(input_dim, hidden_dim, improved=True, add_self_loops=True)
        self.gcn2 = GCNConv(hidden_dim, hidden_dim, improved=True, add_self_loops=True)
        self.gcn3 = GCNConv(hidden_dim, hidden_dim, improved=True, add_self_loops=True)
        
        # ç‰¹å¾èåˆå±‚ï¼šå°†èŠ‚ç‚¹ç‰¹å¾èåˆä¸ºå›¾çº§ç‰¹å¾
        self.fusion = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim * 2),  # è¾“å…¥æ˜¯2*hidden_dim
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, output_dim),
            nn.Tanh()  # è¾“å‡ºèŒƒå›´[-1,1]ï¼Œä¾¿äºäºŒå€¼åŒ–
        )
        
        self.dropout = nn.Dropout(dropout)
        
        # æƒé‡åˆå§‹åŒ–
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æ¨¡å‹æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight, gain=0.5)  # ä½¿ç”¨è¾ƒå°çš„gain
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x, edge_index, batch=None, debug=False):
        """å‰å‘ä¼ æ’­ï¼šæå–å›¾çº§é²æ£’ç‰¹å¾
        
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, input_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            batch: æ‰¹æ¬¡ç´¢å¼•ï¼Œç”¨äºåŒºåˆ†ä¸åŒå›¾çš„èŠ‚ç‚¹ [num_nodes]ï¼Œå¦‚æœä¸ºNoneåˆ™å‡è®¾å•å›¾
            debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        """
        # æ£€æŸ¥è¾“å…¥
        if torch.isnan(x).any() or torch.isinf(x).any():
            if debug:
                logger.error(f"âŒ è¾“å…¥ç‰¹å¾xåŒ…å«NaN/Infï¼èŒƒå›´=[{x.min():.4f}, {x.max():.4f}]")
            return torch.full((1, 1024), float('nan'), device=x.device)
        
        # GCNç‰¹å¾æå–
        x1 = self.gcn1(x, edge_index)
        if torch.isnan(x1).any() or torch.isinf(x1).any():
            if debug:
                logger.error(f"âŒ GCN1è¾“å‡ºåŒ…å«NaN/Infï¼")
            return torch.full((1, 1024), float('nan'), device=x.device)
        
        x1 = F.relu(x1)
        x1 = self.dropout(x1)
        
        x2 = self.gcn2(x1, edge_index)
        if torch.isnan(x2).any() or torch.isinf(x2).any():
            if debug:
                logger.error(f"âŒ GCN2è¾“å‡ºåŒ…å«NaN/Infï¼")
            return torch.full((1, 1024), float('nan'), device=x.device)
        
        x2 = F.relu(x2)
        x2 = self.dropout(x2)
        
        x3 = self.gcn3(x2, edge_index)
        if torch.isnan(x3).any() or torch.isinf(x3).any():
            if debug:
                logger.error(f"âŒ GCN3è¾“å‡ºåŒ…å«NaN/Infï¼")
            return torch.full((1, 1024), float('nan'), device=x.device)
        
        x3 = F.relu(x3)
        
        # å…¨å±€æ± åŒ–ï¼šå°†èŠ‚ç‚¹ç‰¹å¾èåˆä¸ºå›¾çº§ç‰¹å¾
        # å¦‚æœæ²¡æœ‰æä¾›batchï¼Œåˆ›å»ºä¸€ä¸ªå…¨0çš„batchï¼ˆå•å›¾æƒ…å†µï¼‰
        if batch is None:
            batch = torch.zeros(x3.size(0), dtype=torch.long, device=x3.device)
        
        # ä½¿ç”¨PyGçš„å…¨å±€æ± åŒ–å‡½æ•°ï¼Œæ­£ç¡®å¤„ç†æ‰¹å›¾
        mean_pool = global_mean_pool(x3, batch)
        max_pool = global_max_pool(x3, batch)
        
        if torch.isnan(mean_pool).any() or torch.isnan(max_pool).any():
            if debug:
                logger.error(f"âŒ æ± åŒ–è¾“å‡ºåŒ…å«NaNï¼")
            return torch.full((1, 1024), float('nan'), device=x.device)
        
        graph_features = torch.cat([mean_pool, max_pool], dim=1)  # [batch_size, hidden_dim*2]
        
        # é€šè¿‡èåˆå±‚å¾—åˆ°æœ€ç»ˆç‰¹å¾
        output = self.fusion(graph_features)
        
        if torch.isnan(output).any() or torch.isinf(output).any():
            if debug:
                logger.error(f"âŒ Fusionå±‚è¾“å‡ºåŒ…å«NaN/Infï¼èŒƒå›´=[{output.min():.4f}, {output.max():.4f}]")
            return torch.full((1, 1024), float('nan'), device=x.device)
        
        return output

class ContrastiveTrainer:
    """å¯¹æ¯”å­¦ä¹ è®­ç»ƒå™¨"""
    
    def __init__(self, model, device='cpu', temperature=0.1, use_amp=False, batch_size=8):
        self.model = model.to(device)
        self.device = device
        self.temperature = temperature  # å¢å¤§åˆ°0.1ï¼Œæé«˜æ•°å€¼ç¨³å®šæ€§
        self.use_amp = use_amp  # é»˜è®¤ç¦ç”¨AMPï¼Œé¿å…FP16ç²¾åº¦å¯¼è‡´çš„NaN
        self.batch_size = batch_size
        self.initial_batch_size = batch_size  # ä¿å­˜åˆå§‹batch_size
        self.min_batch_size = 1  # æœ€å°batch_size
        
        # ä¼˜åŒ–å™¨ï¼ˆé™ä½å­¦ä¹ ç‡æé«˜ç¨³å®šæ€§ï¼‰
        self.optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.01)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=50)
        # AMPç¼©æ”¾å™¨
        self.scaler = amp.GradScaler(enabled=self.use_amp)
        
        # è®­ç»ƒå†å²è®°å½•
        self.training_history = {
            'epoch_losses': [],
            'contrastive_losses': [],
            'similarity_losses': [],
            'diversity_losses': [],
            'learning_rates': [],
            'gradient_norms': [],
            'feature_stats': []
        }
    
    def contrastive_loss(self, features_original, features_attacked, labels):
        """
        å¯¹æ¯”å­¦ä¹ æŸå¤±å‡½æ•°
        - æ­£æ ·æœ¬ï¼šåŒä¸€åŸå›¾çš„ä¸åŒæ”»å‡»ç‰ˆæœ¬ï¼ˆåº”è¯¥ç›¸ä¼¼ï¼‰
        - è´Ÿæ ·æœ¬ï¼šä¸åŒåŸå›¾çš„ä»»ä½•ç‰ˆæœ¬ï¼ˆåº”è¯¥åŒºåˆ†ï¼‰
        """
        # æ£€æŸ¥è¾“å…¥ç‰¹å¾æ˜¯å¦æœ‰NaN
        if torch.isnan(features_original).any():
            logger.error(f"âš ï¸ features_originalåŒ…å«NaNï¼")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        if torch.isnan(features_attacked).any():
            logger.error(f"âš ï¸ features_attackedåŒ…å«NaNï¼")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        
        # L2å½’ä¸€åŒ–ï¼ˆæ·»åŠ epsé˜²æ­¢é™¤é›¶ï¼‰
        features_original = F.normalize(features_original, p=2, dim=1, eps=1e-8)
        features_attacked = F.normalize(features_attacked, p=2, dim=1, eps=1e-8)
        
        sim_matrix = torch.matmul(features_original, features_attacked.T) / self.temperature
        
        batch_size = features_original.size(0)
        labels_matrix = labels.unsqueeze(1) == labels.unsqueeze(0)
        
        total_loss = 0
        valid_samples = 0
        
        for i in range(batch_size):
            # æ­£æ ·æœ¬ï¼šåŒä¸€åŸå›¾çš„æ”»å‡»ç‰ˆæœ¬
            positive_mask = labels_matrix[i]
            positive_scores = sim_matrix[i][positive_mask]
            
            # è´Ÿæ ·æœ¬ï¼šä¸åŒåŸå›¾çš„ä»»ä½•ç‰ˆæœ¬ï¼ˆåŒ…æ‹¬åŸå›¾å’Œè¢«æ”»å‡»ç‰ˆæœ¬ï¼‰
            negative_mask = ~labels_matrix[i]
            negative_scores = sim_matrix[i][negative_mask]
            
            if len(positive_scores) > 0 and len(negative_scores) > 0:
                # è®¡ç®—å¯¹æ¯”æŸå¤±ï¼šæ­£æ ·æœ¬å¾—åˆ†åº”è¯¥é«˜ï¼Œè´Ÿæ ·æœ¬å¾—åˆ†åº”è¯¥ä½
                # ä½¿ç”¨æ•°å€¼ç¨³å®šçš„InfoNCEæŸå¤±ï¼š-log(exp(pos)/sum(exp(all)))
                pos_score = positive_scores.mean()  # å¹³å‡æ­£æ ·æœ¬å¾—åˆ†
                all_scores = torch.cat([positive_scores, negative_scores])
                
                # æ•°å€¼ç¨³å®šç‰ˆæœ¬ï¼šlog(exp(a)/sum(exp(b))) = a - log_sum_exp(b)
                # ä½¿ç”¨PyTorchçš„logsumexpå‡½æ•°ï¼Œå†…éƒ¨å®ç°äº†æ•°å€¼ç¨³å®šçš„æŠ€å·§
                loss = -pos_score + torch.logsumexp(all_scores, dim=0)
                
                # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaNæˆ–Inf
                if torch.isnan(loss) or torch.isinf(loss):
                    logger.warning(f"âš ï¸ Batch {i}: loss={loss.item()}, pos_score={pos_score.item()}, all_scoresèŒƒå›´=[{all_scores.min().item():.2f}, {all_scores.max().item():.2f}]")
                    continue  # è·³è¿‡è¿™ä¸ªæ ·æœ¬
                
                total_loss += loss
                valid_samples += 1
        
        return total_loss / valid_samples if valid_samples > 0 else torch.tensor(0.0, device=self.device)
    
    def similarity_loss(self, features_original, features_attacked):
        """ç›¸ä¼¼æ€§æŸå¤±ï¼šç¡®ä¿åŒä¸€åŸå›¾çš„æ”»å‡»ç‰ˆæœ¬ç‰¹å¾ç›¸ä¼¼"""
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆæ·»åŠ epsé˜²æ­¢æ•°å€¼é—®é¢˜ï¼‰
        similarity = F.cosine_similarity(features_original, features_attacked, dim=1, eps=1e-8)
        # æœ€å¤§åŒ–ç›¸ä¼¼åº¦ï¼ˆæœ€å°åŒ–1-ç›¸ä¼¼åº¦ï¼‰
        loss = torch.mean(1 - similarity)
        
        # æ£€æŸ¥æŸå¤±
        if torch.isnan(loss) or torch.isinf(loss):
            logger.error(f"âš ï¸ similarity_loss is NaN/Inf: {loss.item()}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        return loss
    
    def diversity_loss(self, features):
        """å¤šæ ·æ€§æŸå¤±ï¼šé˜²æ­¢ç‰¹å¾åå¡Œï¼Œç¡®ä¿ä¸åŒå›¾æœ‰ä¸åŒç‰¹å¾"""
        # è®¡ç®—ç‰¹å¾çŸ©é˜µçš„æ–¹å·®
        feature_var = torch.var(features, dim=0, unbiased=False)
        # é¼“åŠ±æ¯ä¸ªç»´åº¦éƒ½æœ‰è¶³å¤Ÿçš„æ–¹å·®
        diversity_loss = torch.mean(torch.relu(0.1 - feature_var))
        
        # æ£€æŸ¥æŸå¤±
        if torch.isnan(diversity_loss) or torch.isinf(diversity_loss):
            logger.error(f"âš ï¸ diversity_loss is NaN/Inf: {diversity_loss.item()}")
            return torch.tensor(0.0, device=self.device, requires_grad=True)
        return diversity_loss
    
    def train_epoch(self, original_graphs, attacked_graphs, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        # Epochå¼€å§‹æ—¶æ¸…ç†CUDAç¼“å­˜å’ŒåŒæ­¥
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()  # ç¡®ä¿ä¹‹å‰çš„æ“ä½œå®Œæˆ
        
        total_loss = 0.0
        total_contrastive_loss = 0.0
        total_similarity_loss = 0.0
        total_diversity_loss = 0.0
        num_batches = 0
        
        # è®°å½•æ¢¯åº¦èŒƒæ•°
        total_grad_norm = 0.0
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        all_pairs = []
        all_labels = []
        
        for i, (graph_name, original_graph) in enumerate(original_graphs.items()):
            if graph_name in attacked_graphs:
                for attacked_graph in attacked_graphs[graph_name]:
                    all_pairs.append((original_graph, attacked_graph))
                    all_labels.append(i)  # åŒä¸€åŸå›¾ä½¿ç”¨ç›¸åŒæ ‡ç­¾
        
        if len(all_pairs) == 0:
            logger.warning("æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒæ•°æ®å¯¹")
            return 0.0, 0.0, 0.0, 0.0, 0.0
        
        # éšæœºæ‰“ä¹±æ•°æ®ï¼Œç¡®ä¿æ¯ä¸ªbatchéƒ½æœ‰ä¸åŒçš„æ ‡ç­¾åˆ†å¸ƒ
        import random
        combined = list(zip(all_pairs, all_labels))
        random.shuffle(combined)
        all_pairs, all_labels = zip(*combined)
        
        # ç»Ÿè®¡è®­ç»ƒæ•°æ®ä¿¡æ¯
        unique_labels = set(all_labels)
        label_counts = {}
        for label in all_labels:
            label_counts[label] = label_counts.get(label, 0) + 1
        
        logger.info(f"è®­ç»ƒæ•°æ®ç»Ÿè®¡:")
        logger.info(f"  æ€»æ ·æœ¬å¯¹æ•°: {len(all_pairs)}")
        logger.info(f"  åŸå›¾ç±»å‹æ•°: {len(unique_labels)}")
        logger.info(f"  å„åŸå›¾æ ·æœ¬æ•°: {label_counts}")
        
        # è®¡ç®—æ­£è´Ÿæ ·æœ¬æ•°é‡
        total_positive_pairs = sum(count * (count - 1) // 2 for count in label_counts.values())
        total_negative_pairs = len(all_pairs) * (len(all_pairs) - 1) // 2 - total_positive_pairs
        logger.info(f"  æ­£æ ·æœ¬å¯¹æ•°: {total_positive_pairs} (åŒä¸€åŸå›¾çš„ä¸åŒæ”»å‡»ç‰ˆæœ¬)")
        logger.info(f"  è´Ÿæ ·æœ¬å¯¹æ•°: {total_negative_pairs} (ä¸åŒåŸå›¾çš„ä»»ä½•ç‰ˆæœ¬)")
        logger.info("")
        
        # è®¡ç®—æ—¥å¿—é—´éš”ï¼ˆæ¯ä¸ªepochè®°å½•çº¦20ä¸ªbatchï¼‰
        total_batches = (len(all_pairs) + self.batch_size - 1) // self.batch_size
        log_interval = max(1, total_batches // 20)
        logger.info(f"  æ€»batchæ•°: {total_batches}, æ—¥å¿—é—´éš”: æ¯{log_interval}ä¸ªbatch")
        logger.info("")
        
        # åˆ†æ‰¹è®­ç»ƒ
        for i in range(0, len(all_pairs), self.batch_size):
            batch_pairs = all_pairs[i:i + self.batch_size]
            batch_labels = all_labels[i:i + self.batch_size]
            
            try:
                # å‡†å¤‡batchæ•°æ®
                batch_original_features = []
                batch_attacked_features = []
                
                for original_graph, attacked_graph in batch_pairs:
                    # ç§»åŠ¨åˆ°è®¾å¤‡
                    original_graph_gpu = original_graph.to(self.device)
                    attacked_graph_gpu = attacked_graph.to(self.device)
                    
                    # æå–ç‰¹å¾ï¼ˆAMPï¼‰
                    with amp.autocast(enabled=self.use_amp):
                        features_original = self.model(original_graph_gpu.x, original_graph_gpu.edge_index)
                        features_attacked = self.model(attacked_graph_gpu.x, attacked_graph_gpu.edge_index)
                    
                    # æ£€æŸ¥ç‰¹å¾æ˜¯å¦æœ‰NaNï¼ˆå‰5ä¸ªbatchå¼€å¯è°ƒè¯•ï¼‰
                    debug_mode = (i // self.batch_size + 1) <= 5
                    if debug_mode and (torch.isnan(features_original).any() or torch.isnan(features_attacked).any()):
                        # é‡æ–°è¿è¡Œforwardå¼€å¯è°ƒè¯•
                        logger.error(f"\nâš ï¸âš ï¸âš ï¸ Batch {i // self.batch_size + 1} æ£€æµ‹åˆ°NaNï¼å¼€å§‹è¯¦ç»†è°ƒè¯•...")
                        logger.error(f"åŸå§‹å›¾èŠ‚ç‚¹æ•°: {original_graph_gpu.x.shape[0]}, è¾¹æ•°: {original_graph_gpu.edge_index.shape[1]}")
                        logger.error(f"æ”»å‡»å›¾èŠ‚ç‚¹æ•°: {attacked_graph_gpu.x.shape[0]}, è¾¹æ•°: {attacked_graph_gpu.edge_index.shape[1]}")
                        logger.error(f"åŸå§‹å›¾ç‰¹å¾èŒƒå›´: [{original_graph_gpu.x.min():.4f}, {original_graph_gpu.x.max():.4f}]")
                        logger.error(f"æ”»å‡»å›¾ç‰¹å¾èŒƒå›´: [{attacked_graph_gpu.x.min():.4f}, {attacked_graph_gpu.x.max():.4f}]")
                        
                        # é‡æ–°forwardå¼€å¯debug
                        with torch.no_grad():
                            _ = self.model(original_graph_gpu.x, original_graph_gpu.edge_index, debug=True)
                            _ = self.model(attacked_graph_gpu.x, attacked_graph_gpu.edge_index, debug=True)
                        
                        logger.error(f"è·³è¿‡æ­¤batchç»§ç»­è®­ç»ƒ...\n")
                        continue
                    
                    # ä¿å­˜ç‰¹å¾ï¼ˆä¸è¦detachï¼Œéœ€è¦ä¿ç•™æ¢¯åº¦ï¼ï¼‰
                    batch_original_features.append(features_original)
                    batch_attacked_features.append(features_attacked)
                    
                    # åˆ é™¤GPUä¸Šçš„å›¾æ•°æ®ï¼ˆè®¡ç®—å›¾å·²å»ºç«‹ï¼Œå¯ä»¥å®‰å…¨åˆ é™¤ï¼‰
                    del original_graph_gpu, attacked_graph_gpu
                    
                # æ¸…ç†GPUç¼“å­˜ï¼ˆåœ¨ç‰¹å¾æå–å¾ªç¯åï¼‰
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                
                # å †å ç‰¹å¾ï¼ˆæ¯ä¸ªfeaturesæ˜¯[1, 1024]ï¼Œstackåæ˜¯[batch_size, 1, 1024]ï¼Œéœ€è¦squeezeï¼‰
                batch_original = torch.cat(batch_original_features, dim=0)  # [batch_size, 1024]
                batch_attacked = torch.cat(batch_attacked_features, dim=0)  # [batch_size, 1024]
                batch_labels = torch.tensor(batch_labels, device=self.device)
                
                # è®¡ç®—æŸå¤±ï¼ˆAMPï¼‰
                with amp.autocast(enabled=self.use_amp):
                    contrastive_loss = self.contrastive_loss(batch_original, batch_attacked, batch_labels)
                    similarity_loss = self.similarity_loss(batch_original, batch_attacked)
                    diversity_loss = self.diversity_loss(torch.cat([batch_original, batch_attacked], dim=0))
                
                # è®¡ç®—å½“å‰batchçš„æ­£è´Ÿæ ·æœ¬å¯¹æ•°
                batch_labels_np = batch_labels.cpu().numpy()
                unique_batch_labels = set(batch_labels_np)
                
                # è®¡ç®—æ­£æ ·æœ¬å¯¹ï¼ˆåŒä¸€åŸå›¾çš„æ”»å‡»ç‰ˆæœ¬ï¼‰
                batch_positive_pairs = 0
                batch_negative_pairs = 0
                
                for j in range(len(batch_labels_np)):
                    for k in range(j+1, len(batch_labels_np)):
                        if batch_labels_np[j] == batch_labels_np[k]:
                            batch_positive_pairs += 1
                        else:
                            batch_negative_pairs += 1
                
                # æ ¹æ®æ—¥å¿—é—´éš”è®°å½•batchä¿¡æ¯
                batch_idx = i // self.batch_size + 1
                if batch_idx % log_interval == 0 or batch_idx == 1 or batch_idx == total_batches:
                    logger.info(f"  Batch {batch_idx}/{total_batches}: æ ‡ç­¾åˆ†å¸ƒ={dict(zip(*np.unique(batch_labels_np, return_counts=True)))}, æ­£æ ·æœ¬å¯¹={batch_positive_pairs}, è´Ÿæ ·æœ¬å¯¹={batch_negative_pairs}")
                
                # æ€»æŸå¤±
                total_batch_loss = contrastive_loss + 0.5 * similarity_loss + 0.1 * diversity_loss
                
                # æ£€æŸ¥æ€»æŸå¤±æ˜¯å¦æœ‰æ•ˆï¼ˆåœ¨åå‘ä¼ æ’­å‰æ£€æŸ¥ï¼Œé¿å…ç ´åscalerçŠ¶æ€ï¼‰
                if torch.isnan(total_batch_loss) or torch.isinf(total_batch_loss):
                    logger.warning(f"âš ï¸ Batch {batch_idx}: total_batch_loss={total_batch_loss.item()}, è·³è¿‡æ­¤batch")
                    continue  # è·³è¿‡æ­¤batchï¼Œä¸è¿›è¡Œåå‘ä¼ æ’­
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                if self.use_amp:
                    self.scaler.scale(total_batch_loss).backward()
                    # AMPä¸‹éœ€è¦å…ˆåç¼©æ”¾å†è£å‰ª
                    self.scaler.unscale_(self.optimizer)
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    total_grad_norm += grad_norm.item()
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    total_batch_loss.backward()
                    grad_norm = torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    total_grad_norm += grad_norm.item()
                    self.optimizer.step()
                
                # ç´¯ç§¯æŸå¤±ï¼ˆä½¿ç”¨.item()ç«‹å³è½¬ä¸ºPythonæ ‡é‡ï¼Œé‡Šæ”¾tensorï¼‰
                total_loss += total_batch_loss.detach().item()
                total_contrastive_loss += contrastive_loss.detach().item()
                total_similarity_loss += similarity_loss.detach().item()
                total_diversity_loss += diversity_loss.detach().item()
                num_batches += 1
                
                # å¼ºåˆ¶æ¸…ç†ä¸­é—´å˜é‡å’ŒGPUå†…å­˜
                del batch_original_features, batch_attacked_features
                del batch_original, batch_attacked
                del contrastive_loss, similarity_loss, diversity_loss, total_batch_loss
                # æ¸…ç†batchå˜é‡çš„å¼•ç”¨
                del batch_pairs, batch_labels
                
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    
                    # æ¯50ä¸ªbatchå¼ºåˆ¶åŒæ­¥å’Œæ·±åº¦æ¸…ç†
                    if batch_idx % 50 == 0:
                        torch.cuda.synchronize()  # ç¡®ä¿æ‰€æœ‰æ“ä½œå®Œæˆ
                        torch.cuda.empty_cache()
                        allocated = torch.cuda.memory_allocated() / 1024**3
                        reserved = torch.cuda.memory_reserved() / 1024**3
                        logger.info(f"  ğŸ’¾ GPUæ˜¾å­˜: å·²åˆ†é…={allocated:.2f}GB, å·²ä¿ç•™={reserved:.2f}GB")
                    
                    # æ¯100ä¸ªbatchä¹Ÿæ˜¾ç¤ºï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ï¼‰
                    elif batch_idx % 100 == 0:
                        allocated = torch.cuda.memory_allocated() / 1024**3
                        reserved = torch.cuda.memory_reserved() / 1024**3
                        logger.info(f"  ğŸ’¾ GPUæ˜¾å­˜: å·²åˆ†é…={allocated:.2f}GB, å·²ä¿ç•™={reserved:.2f}GB")
                
            except RuntimeError as e:
                # CUDAé”™è¯¯ç‰¹æ®Šå¤„ç†
                if 'cuda' in str(e).lower():
                    logger.error(f"å¤„ç†batch {i//self.batch_size + 1} æ—¶å‡ºé”™: {e}")
                    logger.error("ğŸ›‘ æ£€æµ‹åˆ°CUDAé”™è¯¯ï¼Œå°è¯•æ¸…ç†å¹¶åŒæ­¥...")
                    if torch.cuda.is_available():
                        try:
                            torch.cuda.synchronize()  # åŒæ­¥æ‰€æœ‰æµ
                            torch.cuda.empty_cache()  # æ¸…ç†ç¼“å­˜
                            torch.cuda.reset_peak_memory_stats()  # é‡ç½®å³°å€¼ç»Ÿè®¡
                        except:
                            logger.error("âš ï¸ CUDAæ¸…ç†å¤±è´¥ï¼Œå¯èƒ½éœ€è¦é‡å¯è®­ç»ƒ")
                    # é‡ç½®AMP scalerçŠ¶æ€
                    if self.use_amp:
                        self.scaler.update()
                    continue
                # OOMç‰¹æ®Šå¤„ç†
                elif 'out of memory' in str(e).lower():
                    logger.error(f"å¤„ç†batch {i//self.batch_size + 1} æ—¶å‡ºé”™: {e}")
                    logger.error("ğŸš¨ æ˜¾å­˜ä¸è¶³ï¼ˆOOMï¼‰ï¼Œæ¸…ç†æ˜¾å­˜...")
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                    if self.use_amp:
                        self.scaler.update()
                    continue
                else:
                    logger.error(f"å¤„ç†batch {i//self.batch_size + 1} æ—¶å‡ºé”™: {e}")
                    if self.use_amp:
                        self.scaler.update()
                    continue
            except Exception as e:
                logger.error(f"å¤„ç†batch {i//self.batch_size + 1} æ—¶å‡ºé”™: {e}")
                if self.use_amp:
                    self.scaler.update()
                continue
        
        # æ›´æ–°å­¦ä¹ ç‡
        self.scheduler.step()
        
        # è®¡ç®—å¹³å‡æŸå¤±å’Œæ¢¯åº¦èŒƒæ•°
        avg_loss = total_loss / num_batches if num_batches > 0 else 0.0
        avg_contrastive_loss = total_contrastive_loss / num_batches if num_batches > 0 else 0.0
        avg_similarity_loss = total_similarity_loss / num_batches if num_batches > 0 else 0.0
        avg_diversity_loss = total_diversity_loss / num_batches if num_batches > 0 else 0.0
        avg_grad_norm = total_grad_norm / num_batches if num_batches > 0 else 0.0
        
        # è®°å½•è®­ç»ƒå†å²
        self.training_history['epoch_losses'].append(avg_loss)
        self.training_history['contrastive_losses'].append(avg_contrastive_loss)
        self.training_history['similarity_losses'].append(avg_similarity_loss)
        self.training_history['diversity_losses'].append(avg_diversity_loss)
        self.training_history['gradient_norms'].append(avg_grad_norm)
        self.training_history['learning_rates'].append(self.optimizer.param_groups[0]['lr'])
        
        return avg_loss, avg_contrastive_loss, avg_similarity_loss, avg_diversity_loss, avg_grad_norm
    
    def _train_epoch_with_adaptive_batch_size(self, original_graphs, attacked_graphs, epoch):
        """å¸¦è‡ªé€‚åº”batch_sizeçš„è®­ç»ƒï¼ˆOOMæ—¶è‡ªåŠ¨é™ä½batch_sizeå¹¶é‡è¯•ï¼‰"""
        max_retries = 3  # æœ€å¤šé‡è¯•3æ¬¡
        retry_count = 0
        
        while retry_count <= max_retries:
            try:
                # å°è¯•ä½¿ç”¨å½“å‰batch_sizeè®­ç»ƒ
                return self.train_epoch(original_graphs, attacked_graphs, epoch)
            
            except RuntimeError as e:
                error_str = str(e)
                # æ£€æŸ¥æ˜¯å¦æ˜¯OOMé”™è¯¯
                if "out of memory" in error_str.lower() or "cuda" in error_str.lower():
                    retry_count += 1
                    
                    # æ¸…ç†GPUå†…å­˜
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                    
                    # è®¡ç®—æ–°çš„batch_sizeï¼ˆå‡åŠï¼Œä½†ä¸ä½äºmin_batch_sizeï¼‰
                    old_batch_size = self.batch_size
                    new_batch_size = max(self.min_batch_size, self.batch_size // 2)
                    
                    if new_batch_size == old_batch_size:
                        # å·²ç»æ˜¯æœ€å°batch_sizeï¼Œæ— æ³•å†é™ä½
                        logger.error(f"âŒ æ˜¾å­˜ä¸è¶³ä¸”batch_sizeå·²æ˜¯æœ€å°å€¼({self.min_batch_size})ï¼Œæ— æ³•ç»§ç»­è®­ç»ƒï¼")
                        logger.error(f"   é”™è¯¯ä¿¡æ¯: {error_str}")
                        logger.error(f"   å»ºè®®: å…³é—­å…¶ä»–GPUç¨‹åºæˆ–å‡çº§æ˜¾å¡")
                        raise e
                    
                    self.batch_size = new_batch_size
                    logger.warning("")
                    logger.warning("âš ï¸" * 40)
                    logger.warning(f"âš ï¸ æ£€æµ‹åˆ°æ˜¾å­˜ä¸è¶³ï¼")
                    logger.warning(f"âš ï¸ è‡ªåŠ¨é™ä½ batch_size: {old_batch_size} â†’ {new_batch_size}")
                    logger.warning(f"âš ï¸ é‡è¯•ç¬¬ {retry_count}/{max_retries} æ¬¡...")
                    logger.warning("âš ï¸" * 40)
                    logger.warning("")
                    
                    # ç­‰å¾…ä¸€æ®µæ—¶é—´è®©GPUå†…å­˜å®Œå…¨é‡Šæ”¾
                    import time
                    time.sleep(2)
                    
                    # é‡æ–°è®¡ç®—batchæ•°é‡å¹¶è®°å½•
                    logger.info(f"ğŸ”„ ä½¿ç”¨æ–°çš„batch_size={self.batch_size}é‡æ–°è®­ç»ƒepoch {epoch+1}")
                    
                else:
                    # ä¸æ˜¯OOMé”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
                    raise e
        
        # è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°
        logger.error(f"âŒ å·²é‡è¯•{max_retries}æ¬¡ï¼Œä»ç„¶å¤±è´¥ï¼")
        raise RuntimeError("è‡ªé€‚åº”batch_sizeæœºåˆ¶å¤±è´¥ï¼Œè®­ç»ƒç»ˆæ­¢")
    
    def train(self, original_graphs, attacked_graphs, num_epochs=50):
        """è®­ç»ƒæ¨¡å‹ï¼ˆæ”¯æŒè‡ªé€‚åº”batch_sizeï¼‰"""
        logger.info(f"å¼€å§‹è®­ç»ƒGCNæ¨¡å‹ï¼ˆ{num_epochs}ä¸ªepochï¼‰...")
        logger.info("è®­ç»ƒç›®æ ‡ï¼šæå–çŸ¢é‡åœ°å›¾çš„é²æ£’ç‰¹å¾ï¼ŒæŠµæŠ—RSTæ”»å‡»")
        logger.info(f"è‡ªé€‚åº”batch_sizeç­–ç•¥: åˆå§‹={self.initial_batch_size}, æœ€å°={self.min_batch_size}")
        
        best_loss = float('inf')
        patience = 10
        patience_counter = 0
        
        # åˆ›å»ºCSVæ–‡ä»¶è®°å½•æŸå¤±
        loss_csv_path = os.path.join(os.path.dirname(__file__), 'logs', 'training_loss.csv')
        os.makedirs(os.path.dirname(loss_csv_path), exist_ok=True)
        with open(loss_csv_path, 'w', encoding='utf-8') as f:
            f.write('epoch,total_loss,contrastive_loss,similarity_loss,diversity_loss,grad_norm,learning_rate\n')
        logger.info(f"æŸå¤±è®°å½•æ–‡ä»¶: {loss_csv_path}")
        
        import time
        for epoch in tqdm(range(num_epochs), desc="è®­ç»ƒè¿›åº¦"):
            # Epochå¼€å§‹æ ‡è®°
            epoch_start_time = time.time()
            logger.info("")
            logger.info("=" * 80)
            logger.info(f"ğŸ“Š Epoch {epoch+1}/{num_epochs} å¼€å§‹")
            logger.info("=" * 80)
            
            # è®­ç»ƒï¼ˆå¸¦è‡ªé€‚åº”batch_sizeæœºåˆ¶ï¼‰
            train_loss, contrastive_loss, similarity_loss, diversity_loss, grad_norm = self._train_epoch_with_adaptive_batch_size(original_graphs, attacked_graphs, epoch)
            
            # Epochç»“æŸï¼Œè®¡ç®—è€—æ—¶
            epoch_time = time.time() - epoch_start_time
            
            # Epochç»“æŸæ—¶æ¸…ç†GPUå†…å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                allocated = torch.cuda.memory_allocated() / 1024**3
                max_allocated = torch.cuda.max_memory_allocated() / 1024**3
                logger.info(f"ğŸ’¾ Epochç»“æŸæ˜¾å­˜: å½“å‰={allocated:.2f}GB, å³°å€¼={max_allocated:.2f}GB")
                torch.cuda.reset_peak_memory_stats()
            
            # è®°å½•æŸå¤±åˆ°CSV
            current_lr = self.optimizer.param_groups[0]['lr']
            with open(loss_csv_path, 'a', encoding='utf-8') as f:
                f.write(f'{epoch+1},{train_loss:.6f},{contrastive_loss:.6f},{similarity_loss:.6f},{diversity_loss:.6f},{grad_norm:.6f},{current_lr:.8f}\n')
            
            # æ—©åœæœºåˆ¶
            if train_loss < best_loss:
                best_loss = train_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹åˆ°VGCNæ–‡ä»¶å¤¹
                model_best_path = os.path.join(os.path.dirname(__file__), 'models', 'gcn_model_best.pth')
                self.save_model(model_best_path)
                logger.info("ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
            else:
                patience_counter += 1
            
            # æ¯ä¸ªepochç»“æŸæ—¶éƒ½æ‰“å°æŸå¤±æ±‡æ€»
            logger.info("")
            logger.info("â”€" * 80)
            logger.info(f"âœ… Epoch {epoch+1}/{num_epochs} å®Œæˆ | è€—æ—¶: {epoch_time/60:.2f}åˆ†é’Ÿ")
            logger.info("â”€" * 80)
            logger.info(f"ğŸ“‰ æ€»æŸå¤±    : {train_loss:.6f} (æœ€ä½³: {best_loss:.6f})")
            logger.info(f"ğŸ“‰ å¯¹æ¯”æŸå¤±  : {contrastive_loss:.6f}")
            logger.info(f"ğŸ“‰ ç›¸ä¼¼æ€§æŸå¤±: {similarity_loss:.6f}")
            logger.info(f"ğŸ“‰ å¤šæ ·æ€§æŸå¤±: {diversity_loss:.6f}")
            logger.info(f"ğŸ“ æ¢¯åº¦èŒƒæ•°  : {grad_norm:.6f}")
            logger.info(f"ğŸ“š å­¦ä¹ ç‡    : {current_lr:.8f}")
            logger.info(f"ğŸ“¦ Batchå¤§å° : {self.batch_size} (åˆå§‹: {self.initial_batch_size})")
            logger.info(f"â¸ï¸  è€å¿ƒè®¡æ•°  : {patience_counter}/{patience}")
            
            # æ¯3ä¸ªepochæ‰“å°é¢å¤–çš„ç‰¹å¾ç»Ÿè®¡
            if (epoch + 1) % 3 == 0:
                # è®°å½•è¯¦ç»†çš„ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
                if hasattr(self, 'model') and self.model is not None:
                    with torch.no_grad():
                        # éšæœºé€‰æ‹©ä¸€ä¸ªbatchè®¡ç®—ç‰¹å¾ç»Ÿè®¡
                        sample_features = []
                        for graph_name, original_graph in list(original_graphs.items())[:2]:
                            original_graph = original_graph.to(self.device)
                            features = self.model(original_graph.x, original_graph.edge_index)
                            sample_features.append(features.cpu().numpy())
                        
                        if sample_features:
                            sample_features = np.concatenate(sample_features, axis=0)
                            feature_mean = np.mean(sample_features)
                            feature_std = np.std(sample_features)
                            feature_min = np.min(sample_features)
                            feature_max = np.max(sample_features)
                            
                            logger.info(f"ğŸ” ç‰¹å¾ç»Ÿè®¡: å‡å€¼={feature_mean:.4f}, æ ‡å‡†å·®={feature_std:.4f}, èŒƒå›´=[{feature_min:.4f}, {feature_max:.4f}]")
                            
                            # è®°å½•åˆ°è®­ç»ƒå†å²
                            self.training_history['feature_stats'].append({
                                'mean': feature_mean,
                                'std': feature_std,
                                'min': feature_min,
                                'max': feature_max
                            })
            
            # æ—©åœ
            if patience_counter >= patience:
                logger.warning(f"è¿ç»­{patience}ä¸ªepochæ²¡æœ‰æ”¹å–„ï¼Œæå‰åœæ­¢è®­ç»ƒ")
                break
        
        # ä¿å­˜è®­ç»ƒå†å²
        self.save_training_history()
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self.plot_training_curves(loss_csv_path)
        
        logger.info(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³æŸå¤±å€¼: {best_loss:.6f}")
        return best_loss
    
    def save_model(self, model_path):
        """ä¿å­˜æ¨¡å‹"""
        if not os.path.exists(os.path.dirname(model_path)):
            os.makedirs(os.path.dirname(model_path))
        
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, model_path)
        logger.info(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        # å°†è®­ç»ƒå†å²ä¿å­˜åˆ°VGCNæ–‡ä»¶å¤¹
        history_dir = os.path.join(os.path.dirname(__file__), "logs")
        if not os.path.exists(history_dir):
            os.makedirs(history_dir)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        history_file = os.path.join(history_dir, f"training_history_{timestamp}.json")
        
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        history_data = {}
        for key, value in self.training_history.items():
            if key == 'feature_stats':
                # è½¬æ¢feature_statsä¸­çš„numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹
                converted_stats = []
                for stat_dict in value:
                    converted_dict = {}
                    for stat_key, stat_value in stat_dict.items():
                        # å°†numpyç±»å‹è½¬æ¢ä¸ºfloat
                        if hasattr(stat_value, 'item'):
                            converted_dict[stat_key] = float(stat_value.item())
                        else:
                            converted_dict[stat_key] = float(stat_value)
                    converted_stats.append(converted_dict)
                history_data[key] = converted_stats
            else:
                # ç¡®ä¿æ‰€æœ‰æ•°å€¼éƒ½è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
                history_data[key] = [float(v) if hasattr(v, 'item') else v for v in value]
        
        with open(history_file, 'w', encoding='utf-8') as f:
            json.dump(history_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"è®­ç»ƒå†å²å·²ä¿å­˜åˆ°: {history_file}")
    
    def plot_training_curves(self, csv_path):
        """ç»˜åˆ¶SCIé£æ ¼çš„è®­ç»ƒæ›²çº¿"""
        try:
            import matplotlib.pyplot as plt
            import pandas as pd
            
            # è¯»å–CSVæ•°æ®
            df = pd.read_csv(csv_path)
            
            # è®¾ç½®SCIé£æ ¼
            plt.rcParams['font.family'] = 'sans-serif'
            plt.rcParams['font.size'] = 10
            plt.rcParams['axes.linewidth'] = 1.2
            plt.rcParams['grid.alpha'] = 0.3
            
            # åˆ›å»º2x3å­å›¾
            fig, axes = plt.subplots(2, 3, figsize=(15, 10))
            
            # 1. æ€»æŸå¤±æ›²çº¿
            axes[0, 0].plot(df['epoch'], df['total_loss'], '-o', linewidth=2, markersize=4, alpha=0.8, color='#2E86AB')
            axes[0, 0].set_xlabel('Epoch', fontsize=11, fontweight='bold')
            axes[0, 0].set_ylabel('Total Loss', fontsize=11, fontweight='bold')
            axes[0, 0].set_title('(a) Total Loss', fontsize=12, fontweight='bold')
            axes[0, 0].grid(True, alpha=0.3, linestyle='--')
            axes[0, 0].tick_params(labelsize=9)
            
            # 2. å¯¹æ¯”æŸå¤±æ›²çº¿
            axes[0, 1].plot(df['epoch'], df['contrastive_loss'], '-o', linewidth=2, markersize=4, alpha=0.8, color='#A23B72')
            axes[0, 1].set_xlabel('Epoch', fontsize=11, fontweight='bold')
            axes[0, 1].set_ylabel('Contrastive Loss', fontsize=11, fontweight='bold')
            axes[0, 1].set_title('(b) Contrastive Loss', fontsize=12, fontweight='bold')
            axes[0, 1].grid(True, alpha=0.3, linestyle='--')
            axes[0, 1].tick_params(labelsize=9)
            
            # 3. ç›¸ä¼¼æ€§æŸå¤±æ›²çº¿
            axes[0, 2].plot(df['epoch'], df['similarity_loss'], '-o', linewidth=2, markersize=4, alpha=0.8, color='#F18F01')
            axes[0, 2].set_xlabel('Epoch', fontsize=11, fontweight='bold')
            axes[0, 2].set_ylabel('Similarity Loss', fontsize=11, fontweight='bold')
            axes[0, 2].set_title('(c) Similarity Loss', fontsize=12, fontweight='bold')
            axes[0, 2].grid(True, alpha=0.3, linestyle='--')
            axes[0, 2].tick_params(labelsize=9)
            
            # 4. å¤šæ ·æ€§æŸå¤±æ›²çº¿
            axes[1, 0].plot(df['epoch'], df['diversity_loss'], '-o', linewidth=2, markersize=4, alpha=0.8, color='#C73E1D')
            axes[1, 0].set_xlabel('Epoch', fontsize=11, fontweight='bold')
            axes[1, 0].set_ylabel('Diversity Loss', fontsize=11, fontweight='bold')
            axes[1, 0].set_title('(d) Diversity Loss', fontsize=12, fontweight='bold')
            axes[1, 0].grid(True, alpha=0.3, linestyle='--')
            axes[1, 0].tick_params(labelsize=9)
            
            # 5. æ¢¯åº¦èŒƒæ•°æ›²çº¿
            axes[1, 1].plot(df['epoch'], df['grad_norm'], '-o', linewidth=2, markersize=4, alpha=0.8, color='#6A994E')
            axes[1, 1].set_xlabel('Epoch', fontsize=11, fontweight='bold')
            axes[1, 1].set_ylabel('Gradient Norm', fontsize=11, fontweight='bold')
            axes[1, 1].set_title('(e) Gradient Norm', fontsize=12, fontweight='bold')
            axes[1, 1].grid(True, alpha=0.3, linestyle='--')
            axes[1, 1].tick_params(labelsize=9)
            
            # 6. å­¦ä¹ ç‡æ›²çº¿
            axes[1, 2].plot(df['epoch'], df['learning_rate'], '-o', linewidth=2, markersize=4, alpha=0.8, color='#BC4B51')
            axes[1, 2].set_xlabel('Epoch', fontsize=11, fontweight='bold')
            axes[1, 2].set_ylabel('Learning Rate', fontsize=11, fontweight='bold')
            axes[1, 2].set_title('(f) Learning Rate', fontsize=12, fontweight='bold')
            axes[1, 2].grid(True, alpha=0.3, linestyle='--')
            axes[1, 2].tick_params(labelsize=9)
            axes[1, 2].ticklabel_format(style='scientific', axis='y', scilimits=(0,0))
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾ç‰‡
            save_dir = os.path.dirname(csv_path)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            plot_file = os.path.join(save_dir, f"training_curves_{timestamp}.png")
            plt.savefig(plot_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜åˆ°: {plot_file}")
            
        except ImportError:
            logger.warning("matplotlibæœªå®‰è£…ï¼Œè·³è¿‡è®­ç»ƒæ›²çº¿ç»˜åˆ¶")
        except Exception as e:
            logger.error(f"ç»˜åˆ¶è®­ç»ƒæ›²çº¿æ—¶å‡ºé”™: {e}")

class GraphDataLoader:
    """å›¾æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, graph_dir=os.path.join('..', 'convertToGraph', 'Graph', 'TrainingSet')):
        self.graph_dir = graph_dir
    
    def load_graph_data(self):
        """åŠ è½½å›¾æ•°æ®ï¼šåŸå§‹å›¾ä¸å…¶å¯¹åº”çš„è¢«æ”»å‡»å›¾"""
        original_dir = os.path.join(self.graph_dir, 'Original')
        attacked_dir = os.path.join(self.graph_dir, 'Attacked')
        
        # åŠ è½½åŸå§‹å›¾
        original_graphs = {}
        if not os.path.exists(original_dir):
            logger.warning(f"åŸå§‹æ•°æ®ç›®å½•ä¸å­˜åœ¨: {original_dir}")
            return {}, {}
        for filename in os.listdir(original_dir):
            if filename.endswith('_graph.pkl'):
                graph_name = filename.replace('_graph.pkl', '')
                with open(os.path.join(original_dir, filename), 'rb') as f:
                    graph_data = pickle.load(f)
                    original_graphs[graph_name] = graph_data
        
        # åŠ è½½è¢«æ”»å‡»çš„å›¾
        attacked_graphs = {}
        if os.path.exists(attacked_dir):
            for subdir in os.listdir(attacked_dir):
                subdir_path = os.path.join(attacked_dir, subdir)
                if os.path.isdir(subdir_path):
                    attacked_graphs[subdir] = []
                    for filename in os.listdir(subdir_path):
                        if filename.endswith('_graph.pkl'):
                            with open(os.path.join(subdir_path, filename), 'rb') as f:
                                graph_data = pickle.load(f)
                                attacked_graphs[subdir].append(graph_data)
        
        logger.info(f"åŠ è½½äº† {len(original_graphs)} ä¸ªåŸå§‹å›¾")
        total_attacked = sum(len(graphs) for graphs in attacked_graphs.values())
        logger.info(f"åŠ è½½äº† {total_attacked} ä¸ªè¢«æ”»å‡»çš„å›¾")
        
        return original_graphs, attacked_graphs

def main():
    """ä¸»å‡½æ•°"""
    logger.info("ï¼ï¼ï¼ ç¬¬ä¸‰æ­¥ï¼šGCNæ¨¡å‹è®­ç»ƒ - çŸ¢é‡åœ°å›¾é›¶æ°´å°é²æ£’ç‰¹å¾æå– ï¼ï¼ï¼")
    
    # å°è¯•ç¼“è§£CUDAæ˜¾å­˜ç¢ç‰‡ï¼ˆä»…å½“å‰è¿›ç¨‹ç”Ÿæ•ˆï¼‰
    try:
        os.environ.setdefault('PYTORCH_CUDA_ALLOC_CONF', 'expandable_segments:True')
        logger.info(f"PYTORCH_CUDA_ALLOC_CONF={os.environ.get('PYTORCH_CUDA_ALLOC_CONF')}")
    except Exception:
        pass

    # è®¾ç½®è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½æ•°æ®
    data_loader = GraphDataLoader()
    original_graphs, attacked_graphs = data_loader.load_graph_data()
    
    if len(original_graphs) == 0:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°åŸå§‹å›¾æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œç¬¬äºŒæ­¥")
        return
    
    if len(attacked_graphs) == 0:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°è¢«æ”»å‡»çš„å›¾æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œç¬¬äºŒæ­¥")
        return
    
    # è·å–è¾“å…¥ç»´åº¦
    first_graph = list(original_graphs.values())[0]
    input_dim = first_graph.x.shape[1]
    logger.info(f"è¾“å…¥ç‰¹å¾ç»´åº¦: {input_dim}")
    logger.info(f"ç›®æ ‡è¾“å‡ºç»´åº¦: 1024 (32x32)")
    
    # åˆ›å»ºGCNæ¨¡å‹
    model = GCNModel(input_dim=input_dim, hidden_dim=128, output_dim=1024, dropout=0.2)
    logger.info(f"æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    
    # è¯»å–å¯é€‰batchå¤§å°ï¼ˆé»˜è®¤4ï¼Œé€‚é…16GBæ˜¾å­˜ï¼‰
    try:
        configured_bs = int(os.environ.get('VGAT_BATCH_SIZE', '4'))
    except Exception:
        configured_bs = 4
    
    # åˆ›å»ºè®­ç»ƒå™¨ï¼ˆç¦ç”¨AMPä½¿ç”¨FP32å®Œæ•´ç²¾åº¦ï¼Œé¿å…FP16æ•°å€¼ä¸ç¨³å®šï¼‰
    trainer = ContrastiveTrainer(model, device, use_amp=False, batch_size=configured_bs)
    
    # è®­ç»ƒæ¨¡å‹
    train_loss = trainer.train(original_graphs, attacked_graphs, num_epochs=50)
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°VGCNæ–‡ä»¶å¤¹
    final_model_path = os.path.join(os.path.dirname(__file__), 'models', 'gcn_model.pth')
    trainer.save_model(final_model_path)
    
    logger.info("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    logger.info("æ¨¡å‹å°†ç”¨äºï¼š")
    logger.info("  1. ä»åŸå§‹çŸ¢é‡åœ°å›¾æå–é²æ£’ç‰¹å¾")
    logger.info("  2. ä¸ç‰ˆæƒå›¾åƒç»“åˆç”Ÿæˆé›¶æ°´å°")
    logger.info("  3. éªŒè¯é˜¶æ®µæå–ç‰¹å¾å¹¶ä¸é›¶æ°´å°ç»“åˆæ¢å¤ç‰ˆæƒå›¾åƒ")

if __name__ == "__main__":
    main()