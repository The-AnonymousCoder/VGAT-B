#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç¬¬äºŒæ­¥ï¼šè®­ç»ƒé›†å›¾ç»“æ„è½¬æ¢ï¼ˆKNN + Delaunay ç»Ÿä¸€å›¾æ„å»ºç‰ˆæœ¬ï¼‰
å°†vector_dataå’Œvector_data_attackedä¸‹çš„è®­ç»ƒé›†çŸ¢é‡æ•°æ®è½¬æ¢ä¸ºGATå¯å¤„ç†çš„å›¾ç»“æ„
ä½¿ç”¨ KNN + Delaunay ç»Ÿä¸€å›¾æ„å»ºæ–¹å¼

ã€æ ¸å¿ƒæ”¹è¿›ã€‘ï¼š
1. å¼•å…¥20ç»´æœ€ä¼˜å‡ ä½•ä¸å˜ç‰¹å¾ï¼ˆæ–¹æ¡ˆDï¼šè‡ªé€‚åº”ç‰ˆï¼Œæ›¿ä»£åŸ19ç»´ï¼‰
   - ç»´åº¦0-2:  å‡ ä½•ç±»å‹ç¼–ç ï¼ˆone-hotï¼‰
   - ç»´åº¦3:    Huä¸å˜çŸ©Ï†1ï¼ˆå®Œå…¨å‡ ä½•ä¸å˜ï¼‰â­
   - ç»´åº¦4:    è¾¹ç•Œå¤æ‚åº¦ï¼ˆç¼©æ”¾ä¸å˜ï¼‰
   - ç»´åº¦5-7:  å½“å‰åœ°å›¾ç›¸å¯¹ä½ç½®ï¼ˆå®è§‚ç©ºé—´ä¿¡æ¯ï¼‰
   - ç»´åº¦8-10: å±€éƒ¨ç›¸å¯¹ä½ç½®ï¼ˆå¾®è§‚ç©ºé—´ä¿¡æ¯ï¼ŒæŠ—è£å‰ªï¼‰â­æ ¸å¿ƒ
   - ç»´åº¦11-12: é•¿å®½æ¯” + çŸ©å½¢åº¦ï¼ˆæ—‹è½¬ä¸å˜ï¼‰
   - ç»´åº¦13:   Solidityï¼ˆå½¢çŠ¶å¤æ‚åº¦ï¼‰
   - ç»´åº¦14:   å¯¹æ•°é¡¶ç‚¹æ•°ï¼ˆå¤æ‚åº¦æŒ‡æ ‡ï¼‰
   - ç»´åº¦15-17: æ‹“æ‰‘é‚»åŸŸç‰¹å¾ï¼ˆå›¾ç»“æ„ç›¸å…³ï¼‰
   - ç»´åº¦18:   å­”æ´æ•°é‡ï¼ˆæ‹“æ‰‘ç‰¹å¾ï¼‰
   - ç»´åº¦19:   èŠ‚ç‚¹æ•°ç¼–ç ï¼ˆå›¾è§„æ¨¡ä¿¡æ¯ï¼‰â­æ–°å¢
2. å¤šå°ºåº¦ä½ç½®è¡¨è¾¾ï¼šå…¨å±€+å±€éƒ¨å¹¶å­˜ï¼ŒGATè‡ªåŠ¨å­¦ä¹ æƒé‡
3. å®æ–½å…¨å±€æ ‡å‡†åŒ–ï¼ˆæ›¿ä»£é€å›¾æ ‡å‡†åŒ–ï¼‰
4. **KNN + Delaunay ç»Ÿä¸€å›¾æ„å»º**ï¼šâ­â­â­
   - è‡ªé€‚åº”Kå€¼ï¼šæ ¹æ®èŠ‚ç‚¹æ•°åŠ¨æ€è°ƒæ•´ï¼ˆKæœ€å¤§ä¸º8ï¼‰
   - KNNä¿è¯å±€éƒ¨å¯†é›†è¿æ¥ï¼šæ¯ä¸ªèŠ‚ç‚¹è‡³å¤š8ä¸ªé‚»å±…
   - Delaunayä¿è¯å…¨å±€è¿é€šï¼šè¦†ç›–æ‰€æœ‰èŠ‚ç‚¹ï¼Œå¡«è¡¥ç¨€ç–åŒºåŸŸ
   - é€‚ç”¨äºæ‰€æœ‰æ•°æ®ç±»å‹ï¼ˆç‚¹/çº¿/é¢ï¼‰ï¼Œæ— å­¤å²›èŠ‚ç‚¹
5. å¯¹å„ç§æ”»å‡»ï¼ˆç‰¹åˆ«æ˜¯è£å‰ªå’Œåˆ é™¤å¯¹è±¡ï¼‰å…·æœ‰æå¼ºé²æ£’æ€§

ã€æ€§èƒ½ä¼˜åŒ–ã€‘ï¼š
6. KD-treeåŠ é€ŸKNNæ„å»ºï¼šO(n log n) å¤æ‚åº¦
7. Delaunayä¸‰è§’å‰–åˆ†ï¼šO(n log n) å¤æ‚åº¦
8. æ— éœ€R-treeä¾èµ–ï¼ˆDelaunayè‡ªå¸¦ç©ºé—´ç´¢å¼•ï¼‰

ã€ä¾èµ–å®‰è£…ã€‘ï¼š
  pip install scipy scikit-learn
  
  æ³¨æ„ï¼šscipyç”¨äºDelaunayä¸‰è§’å‰–åˆ†ï¼Œsklearnç”¨äºKNN
"""

import os
import geopandas as gpd
import numpy as np
import pickle
from sklearn.preprocessing import StandardScaler
import torch
from torch_geometric.data import Data
from torch_geometric.utils import to_undirected  # âœ… ç”¨äºæ— å‘å›¾è½¬æ¢
from tqdm import tqdm
import shutil
from shapely.geometry import Point, LineString, Polygon, MultiPoint
from scipy.spatial import Delaunay
from collections import defaultdict
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import psutil
import gc
import time
import hashlib
import json

class ImprovedTrainSetVectorToGraphConverter:
    """æ”¹è¿›çš„è®­ç»ƒé›†çŸ¢é‡æ•°æ®è½¬å›¾ç»“æ„è½¬æ¢å™¨ï¼ˆKNN + Delaunay ç»Ÿä¸€å›¾æ„å»ºï¼‰"""
    
    @staticmethod
    def adaptive_k_for_graph(n_nodes):
        """
        â­ æ ¹æ®èŠ‚ç‚¹æ•°è‡ªé€‚åº”ç¡®å®šKå€¼ï¼ˆKæœ€å°ä¸º1ï¼Œå®Œå…¨è‡ªé€‚åº”ï¼‰
        
        å…¬å¼ï¼šK = min(round(2 * log10(n)) + 2, n-1)
        - KéšèŠ‚ç‚¹æ•°å¯¹æ•°å¢é•¿
        - é™åˆ¶èŒƒå›´ï¼š[1, min(12, n-1)]
        
        ç¤ºä¾‹ï¼š
        - n=1      â†’ K=1   (n-1)
        - n=2      â†’ K=1   (n-1)
        - n=3-7    â†’ K=min(4, n-1)
        - n=8-999  â†’ K=min(è®¡ç®—å€¼, n-1)
        - n=1,000  â†’ K=8   (2*3 + 2 = 8)
        - n=10,000 â†’ K=10  (2*4 + 2 = 10)
        - nâ‰¥100,000â†’ K=12  (è¾¾åˆ°ä¸Šé™)
        
        ä¼˜åŠ¿ï¼š
        - æå°å›¾ï¼ˆn<3ï¼‰ï¼šK=n-1ï¼Œèƒ½æ„å»ºåŸºæœ¬è¿æ¥
        - å°å›¾ï¼šKè‡ªé€‚åº”ï¼Œä¸ä¼šè¶…è¿‡èŠ‚ç‚¹æ•°
        - ä¸­å¤§å›¾ï¼šKé€‚ä¸­ï¼Œä¿æŒå±€éƒ¨è¿æ¥
        - æœ€å¤§Ké™åˆ¶ä¸º12ï¼Œé¿å…è¿‡å¯†
        
        Args:
            n_nodes: å›¾çš„èŠ‚ç‚¹æ€»æ•°
            
        Returns:
            int: æ¨èçš„Kå€¼ï¼ˆèŒƒå›´1-12ï¼‰
        """
        if n_nodes < 2:
            return 1  # å•èŠ‚ç‚¹å›¾ï¼ŒK=1
        
        if n_nodes == 2:
            return 1  # 2ä¸ªèŠ‚ç‚¹ï¼ŒK=1
        
        # K = 2 * log10(n) + 2ï¼Œä½†ä¸è¶…è¿‡ n-1
        k = int(round(2 * np.log10(n_nodes) + 2))
        
        # é™åˆ¶èŒƒå›´ [1, min(12, n-1)]
        k = max(1, min(min(12, n_nodes - 1), k))
        
        return k
    
    @staticmethod
    def hilbert_distance(x, y, order=16):
        """
        â­ è®¡ç®—2Dç‚¹åœ¨Hilbertæ›²çº¿ä¸Šçš„è·ç¦»ï¼ˆç”¨äºç©ºé—´æ’åºï¼‰
        
        Hilbertæ›²çº¿æ˜¯ä¸€ç§ç©ºé—´å¡«å……æ›²çº¿ï¼Œèƒ½ä¿æŒç©ºé—´å±€éƒ¨æ€§ï¼š
        - åœ¨æ›²çº¿ä¸Šç›¸é‚»çš„ç‚¹ï¼Œåœ¨2Dç©ºé—´ä¸­ä¹Ÿç›¸é‚»
        - ç”¨äºå¯¹ç‚¹æ’åºï¼Œä½¿å¾—ç›¸é‚»çš„ç‚¹åœ¨åŒä¸€åˆ†å—ä¸­
        
        Args:
            x, y: å½’ä¸€åŒ–åæ ‡ [0, 1]
            order: Hilbertæ›²çº¿é˜¶æ•°ï¼ˆé»˜è®¤16ï¼Œæ”¯æŒ2^16=65536ä¸ªç‚¹ï¼‰
            
        Returns:
            int: Hilbertè·ç¦»
        """
        # å°†[0,1]åæ ‡æ˜ å°„åˆ°[0, 2^order-1]æ•´æ•°ç©ºé—´
        max_coord = (1 << order) - 1
        xi = int(x * max_coord)
        yi = int(y * max_coord)
        
        # Hilbertæ›²çº¿ç¼–ç ï¼ˆè¿­ä»£ç‰ˆæœ¬ï¼‰
        d = 0
        s = 1 << (order - 1)
        
        while s > 0:
            rx = 1 if (xi & s) > 0 else 0
            ry = 1 if (yi & s) > 0 else 0
            d += s * s * ((3 * rx) ^ ry)
            
            # æ—‹è½¬åæ ‡
            if ry == 0:
                if rx == 1:
                    xi = max_coord - xi
                    yi = max_coord - yi
                xi, yi = yi, xi
            
            s >>= 1
        
        return d
    
    @staticmethod
    def partition_by_hilbert(centroids, block_size=2000):
        """
        â­ ä½¿ç”¨Hilbertæ›²çº¿å°†èŠ‚ç‚¹åˆ†å—ï¼ˆä¿æŒç©ºé—´å±€éƒ¨æ€§ï¼‰
        
        ä¼˜åŠ¿ï¼š
        - æ¯ä¸ªå—å†…çš„èŠ‚ç‚¹åœ¨ç©ºé—´ä¸Šèšé›†
        - å—é—´è¿æ¥æ•°é‡å°‘ï¼Œå‡å°‘è·¨å—è¾¹
        - Delaunayå¤æ‚åº¦ï¼šO(n_block * log n_block) << O(n_total^2)
        
        Args:
            centroids: (N, 2) numpyæ•°ç»„ï¼ŒèŠ‚ç‚¹åæ ‡
            block_size: æ¯ä¸ªå—çš„æœ€å¤§èŠ‚ç‚¹æ•°ï¼ˆé»˜è®¤2000ï¼‰
            
        Returns:
            list: [block1_indices, block2_indices, ...]
        """
        n = len(centroids)
        
        if n <= block_size:
            return [list(range(n))]  # å•ä¸ªå—
        
        # å½’ä¸€åŒ–åæ ‡åˆ°[0, 1]
        min_x, min_y = centroids.min(axis=0)
        max_x, max_y = centroids.max(axis=0)
        
        # é˜²æ­¢é™¤é›¶
        range_x = max_x - min_x if max_x > min_x else 1.0
        range_y = max_y - min_y if max_y > min_y else 1.0
        
        norm_x = (centroids[:, 0] - min_x) / range_x
        norm_y = (centroids[:, 1] - min_y) / range_y
        
        # è®¡ç®—æ¯ä¸ªç‚¹çš„Hilbertè·ç¦»
        hilbert_distances = np.array([
            ImprovedTrainSetVectorToGraphConverter.hilbert_distance(x, y)
            for x, y in zip(norm_x, norm_y)
        ])
        
        # æŒ‰Hilbertè·ç¦»æ’åº
        sorted_indices = np.argsort(hilbert_distances)
        
        # åˆ†å—
        blocks = []
        for i in range(0, n, block_size):
            block_indices = sorted_indices[i:i+block_size].tolist()
            blocks.append(block_indices)
        
        print(f"  â­ Hilbertåˆ†å—ï¼š{n}èŠ‚ç‚¹ â†’ {len(blocks)}å—ï¼ˆæ¯å—â‰¤{block_size}ï¼‰")
        
        return blocks
    
    def __init__(self, vector_dir="../convertToGeoJson/GeoJson/TrainingSet", 
                 attacked_dir="../convertToGeoJson-Attacked/GeoJson-Attacked/TrainingSet", 
                 graph_dir="Graph/TrainingSet",
                 batch_size=100,
                 max_workers=None,
                 use_cache=True):
        self.vector_dir = vector_dir
        self.attacked_dir = attacked_dir
        self.graph_dir = graph_dir
        self.batch_size = batch_size
        self.max_workers = max_workers or min(8, mp.cpu_count())
        self.use_cache = use_cache
        
        self.ensure_graph_dir()
        
        # ä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–å™¨
        self.global_scaler = StandardScaler()
        self.scaler_fitted = False
        
        # å­˜å‚¨å…¨å±€ç»Ÿè®¡é‡ï¼ˆç”¨äºè®¡ç®—ç›¸å¯¹ä½ç½®ï¼‰
        self.global_bounds = None
        self.global_centroid = None
        
        # ç¼“å­˜ç›¸å…³
        self.cache_dir = os.path.join(self.graph_dir, "cache")
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
        # â­ æ–¹æ¡ˆAï¼šä»…ç”¨åŸå§‹å›¾ç”Ÿæˆscalerï¼Œç¼“å­˜æ–‡ä»¶ååŒºåˆ†
        self.features_cache_file = os.path.join(self.cache_dir, "features_cache_original_only.pkl")
        self.file_hashes_file = os.path.join(self.cache_dir, "file_hashes_original_only.json")
        
    def ensure_graph_dir(self):
        """ç¡®ä¿å›¾æ•°æ®ç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.graph_dir):
            os.makedirs(self.graph_dir)
        os.makedirs(os.path.join(self.graph_dir, 'Original'), exist_ok=True)
        os.makedirs(os.path.join(self.graph_dir, 'Attacked'), exist_ok=True)

    def clean_output_dirs(self):
        """æ¸…ç©ºè¾“å‡ºç›®å½•ï¼Œç¡®ä¿æ¯æ¬¡è¿è¡Œå¯å®Œå…¨æ›¿æ¢"""
        original_path = os.path.join(self.graph_dir, 'Original')
        attacked_path = os.path.join(self.graph_dir, 'Attacked')

        if os.path.exists(original_path):
            for name in os.listdir(original_path):
                # è·³è¿‡ macOS çš„éšè—å…ƒæ•°æ®æ–‡ä»¶
                if name.startswith('._'):
                    continue
                
                file_path = os.path.join(original_path, name)
                try:
                    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
                    if not os.path.exists(file_path):
                        continue
                    
                    if os.path.isfile(file_path):
                        os.remove(file_path)
                    elif os.path.isdir(file_path):
                        shutil.rmtree(file_path)
                except Exception as e:
                    print(f"âš ï¸  åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
                    continue
        else:
            os.makedirs(original_path, exist_ok=True)

        if os.path.exists(attacked_path):
            try:
                shutil.rmtree(attacked_path)
            except Exception as e:
                print(f"âš ï¸  åˆ é™¤ç›®å½•å¤±è´¥ {attacked_path}: {e}")
        os.makedirs(attacked_path, exist_ok=True)
    
    def get_file_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼ç”¨äºç¼“å­˜éªŒè¯"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except:
            return None

    def load_file_hashes(self):
        """åŠ è½½æ–‡ä»¶å“ˆå¸Œç¼“å­˜"""
        if os.path.exists(self.file_hashes_file):
            try:
                with open(self.file_hashes_file, 'r') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_file_hashes(self, file_hashes):
        """ä¿å­˜æ–‡ä»¶å“ˆå¸Œç¼“å­˜"""
        try:
            with open(self.file_hashes_file, 'w') as f:
                json.dump(file_hashes, f)
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶å“ˆå¸Œå¤±è´¥: {e}")
    
    def check_original_files_changed(self, old_hashes):
        """æ£€æŸ¥åŸå§‹å›¾æ–‡ä»¶æ˜¯å¦æœ‰å˜åŒ–"""
        if not os.path.exists(self.vector_dir):
            return False
        
        for filename in os.listdir(self.vector_dir):
            if filename.endswith('.geojson') and not filename.startswith('._'):
                file_path = os.path.join(self.vector_dir, filename)
                current_hash = self.get_file_hash(file_path)
                
                # å¦‚æœæ–‡ä»¶æ˜¯æ–°çš„æˆ–è€…å“ˆå¸Œå˜åŒ–äº†
                if file_path not in old_hashes or old_hashes.get(file_path) != current_hash:
                    return True
        
        return False
    
    def get_graph_output_path(self, filename, attacked_subdir=None, data_type='Original'):
        """è·å–å›¾æ–‡ä»¶çš„è¾“å‡ºè·¯å¾„"""
        graph_name = filename.replace('.geojson', '')
        
        if data_type == 'Original':
            output_dir = os.path.join(self.graph_dir, 'Original')
            output_path = os.path.join(output_dir, f"{graph_name}_graph.pkl")
        else:  # Attacked
            output_dir = os.path.join(self.graph_dir, 'Attacked', attacked_subdir)
            output_path = os.path.join(output_dir, f"{graph_name}_graph.pkl")
        
        return output_path
    
    def should_update_file(self, file_path, old_hashes, output_path):
        """åˆ¤æ–­æ–‡ä»¶æ˜¯å¦éœ€è¦æ›´æ–°ï¼ˆåŸºäºå“ˆå¸Œå’Œè¾“å‡ºæ–‡ä»¶å­˜åœ¨æ€§ï¼‰"""
        # å¦‚æœè¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå¿…é¡»æ›´æ–°
        if not os.path.exists(output_path):
            return True, "è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨"
        
        # è®¡ç®—å½“å‰æ–‡ä»¶å“ˆå¸Œ
        current_hash = self.get_file_hash(file_path)
        if current_hash is None:
            return True, "æ— æ³•è®¡ç®—å“ˆå¸Œ"
        
        # å¦‚æœæ–‡ä»¶æ˜¯æ–°çš„æˆ–å“ˆå¸Œå˜åŒ–äº†
        if file_path not in old_hashes:
            return True, "æ–°æ–‡ä»¶"
        
        if old_hashes.get(file_path) != current_hash:
            return True, "æ–‡ä»¶å·²ä¿®æ”¹"
        
        return False, "æ— å˜åŒ–"

    def monitor_system_resources(self):
        """ç›‘æ§ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        try:
            memory = psutil.virtual_memory()
            cpu = psutil.cpu_percent(interval=0.1)
            
            print(f"ğŸ’» ç³»ç»Ÿèµ„æº: CPU {cpu:.1f}%, å†…å­˜ {memory.percent:.1f}% ({memory.used // 1024**3}GB/{memory.total // 1024**3}GB)")
            
            if memory.percent > 85:
                print("âš ï¸  å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å‡å°‘batch_sizeæˆ–max_workers")
                
            return memory.percent < 90  # è¿”å›æ˜¯å¦å¯ä»¥ç»§ç»­å¤„ç†
        except:
            return True

    def get_all_file_paths(self):
        """è·å–æ‰€æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶è·¯å¾„"""
        file_paths = []
        
        # åŸå§‹æ•°æ®
        if os.path.exists(self.vector_dir):
            for filename in os.listdir(self.vector_dir):
                if filename.endswith('.geojson') and not filename.startswith('._'):
                    file_paths.append(('original', os.path.join(self.vector_dir, filename)))
        
        # æ”»å‡»æ•°æ®
        if os.path.exists(self.attacked_dir):
            for attacked_subdir in os.listdir(self.attacked_dir):
                attack_dir_path = os.path.join(self.attacked_dir, attacked_subdir)
                if os.path.isdir(attack_dir_path):
                    for filename in os.listdir(attack_dir_path):
                        if filename.endswith('.geojson') and not filename.startswith('._'):
                            file_paths.append(('attacked', os.path.join(attack_dir_path, filename)))
        
        return file_paths
    
    def calculate_global_statistics(self, all_gdfs):
        """è®¡ç®—æ‰€æœ‰å‡ ä½•è¦ç´ çš„å…¨å±€ç»Ÿè®¡é‡"""
        print("è®¡ç®—å…¨å±€ç»Ÿè®¡é‡...")
        
        # æ”¶é›†æ‰€æœ‰å‡ ä½•è¦ç´ 
        all_geometries = []
        for gdf in all_gdfs:
            all_geometries.extend(gdf.geometry.tolist())
        
        # è®¡ç®—å…¨å±€è¾¹ç•Œæ¡†
        all_bounds = [geom.bounds for geom in all_geometries]
        min_x = min(b[0] for b in all_bounds)
        min_y = min(b[1] for b in all_bounds)
        max_x = max(b[2] for b in all_bounds)
        max_y = max(b[3] for b in all_bounds)
        self.global_bounds = (min_x, min_y, max_x, max_y)
        
        # è®¡ç®—å…¨å±€è´¨å¿ƒ
        all_centroids = [geom.centroid for geom in all_geometries]
        avg_x = np.mean([c.x for c in all_centroids])
        avg_y = np.mean([c.y for c in all_centroids])
        self.global_centroid = Point(avg_x, avg_y)
        
        print(f"å…¨å±€è¾¹ç•Œæ¡†: {self.global_bounds}")
        print(f"å…¨å±€è´¨å¿ƒ: ({avg_x:.2f}, {avg_y:.2f})")
    
    def extract_improved_features(self, geometry, row, geometry_index=None, all_centroids=None, 
                                  local_bounds=None, local_centroid=None, k_neighbors_info=None, total_nodes=None):
        """
        æå–æ”¹è¿›çš„20ç»´å‡ ä½•ä¸å˜ç‰¹å¾ï¼ˆæ–¹æ¡ˆDï¼šè‡ªé€‚åº”ç‰ˆï¼Œå…¨å±€+å±€éƒ¨å¤šå°ºåº¦+èŠ‚ç‚¹æ•°ç¼–ç ï¼‰
        
        ç‰¹å¾åˆ—è¡¨ï¼š
        0-2.   å‡ ä½•ç±»å‹ç¼–ç ï¼ˆ3ç»´ï¼‰- one-hot
        3.     Huä¸å˜çŸ©Ï†1ï¼ˆ1ç»´ï¼‰- å®Œå…¨å‡ ä½•ä¸å˜ï¼Œæœ€ç»å…¸çš„å½¢çŠ¶æè¿°ç¬¦â­
        4.     è¾¹ç•Œå¤æ‚åº¦ Boundary Complexityï¼ˆ1ç»´ï¼‰- ç¼©æ”¾ä¸å˜ï¼Œå¯¹å™ªå£°é²æ£’
        5-7.   å½“å‰åœ°å›¾ç›¸å¯¹ä½ç½®ï¼ˆ3ç»´ï¼‰- å½“å‰åœ°å›¾çš„å®è§‚ç©ºé—´ä¿¡æ¯ï¼ˆç‹¬ç«‹å…¨å±€ï¼‰
        8-10.  å±€éƒ¨ç›¸å¯¹ä½ç½®ï¼ˆ3ç»´ï¼‰- å¾®è§‚ç©ºé—´ä¿¡æ¯ï¼ˆæŠ—è£å‰ªï¼‰â­æ ¸å¿ƒ
        11-12. é•¿å®½æ¯” + çŸ©å½¢åº¦ï¼ˆ2ç»´ï¼‰- æ—‹è½¬ä¸å˜
        13.    Solidityï¼ˆ1ç»´ï¼‰- å½¢çŠ¶å¤æ‚åº¦
        14.    å¯¹æ•°é¡¶ç‚¹æ•°ï¼ˆ1ç»´ï¼‰- å¤æ‚åº¦æŒ‡æ ‡
        15-17. æ‹“æ‰‘é‚»åŸŸç‰¹å¾ï¼ˆ3ç»´ï¼‰- åŸºäºæ‹“æ‰‘é‚»æ¥ï¼Œä¸å›¾ç»“æ„ä¸€è‡´
        18.    å­”æ´æ•°é‡ Holesï¼ˆ1ç»´ï¼‰- æ‹“æ‰‘ç‰¹å¾ï¼ŒæŠ—æ”»å‡»
        19.    èŠ‚ç‚¹æ•°ç¼–ç ï¼ˆ1ç»´ï¼‰- å›¾è§„æ¨¡ä¿¡æ¯â­æ–°å¢
        
        ã€æ–¹æ¡ˆDæ ¸å¿ƒæ€æƒ³ã€‘ï¼š
        - å½“å‰åœ°å›¾ä½ç½®ï¼šæè¿°èŠ‚ç‚¹åœ¨å½“å‰åœ°å›¾ä¸­çš„å®è§‚ä½ç½®ï¼ˆç‹¬ç«‹å½’ä¸€åŒ–ï¼‰
        - å±€éƒ¨ä½ç½®ï¼šæè¿°èŠ‚ç‚¹åœ¨é‚»åŸŸä¸­çš„å¾®è§‚ä½ç½®ï¼ˆè£å‰ªåä»ç¨³å®šï¼‰
        - èŠ‚ç‚¹æ•°ç¼–ç ï¼šè¡¥å¿è‡ªé€‚åº”Kå€¼å½’ä¸€åŒ–åä¸¢å¤±çš„å›¾è§„æ¨¡ä¿¡æ¯
        - GATæ³¨æ„åŠ›æœºåˆ¶ä¼šè‡ªåŠ¨å­¦ä¹ åœ¨ä¸åŒåœºæ™¯ä¸‹ä½¿ç”¨ä¸åŒç‰¹å¾
        
        Args:
            geometry: å½“å‰å‡ ä½•è¦ç´ 
            row: GeoDataFrameçš„è¡Œæ•°æ®
            geometry_index: å½“å‰å‡ ä½•ç´¢å¼•ï¼ˆç”¨äºè®¡ç®—å±€éƒ¨ä½ç½®ï¼‰
            all_centroids: æ‰€æœ‰å‡ ä½•è¦ç´ çš„è´¨å¿ƒåˆ—è¡¨ï¼ˆç”¨äºè®¡ç®—å±€éƒ¨ä½ç½®ï¼‰
            local_bounds: å½“å‰åœ°å›¾çš„è¾¹ç•Œæ¡†ï¼ˆç”¨äºå½’ä¸€åŒ–ä½ç½®ï¼‰
            local_centroid: å½“å‰åœ°å›¾çš„è´¨å¿ƒï¼ˆç”¨äºè®¡ç®—ç›¸å¯¹è·ç¦»ï¼‰
            k_neighbors_info: é¢„è®¡ç®—çš„Kè¿‘é‚»ä¿¡æ¯ï¼ˆç”¨äºåŠ é€Ÿï¼‰
            total_nodes: å½“å‰åœ°å›¾çš„æ€»èŠ‚ç‚¹æ•°ï¼ˆç”¨äºèŠ‚ç‚¹æ•°ç¼–ç ï¼‰
        """
        features = []
        
        # ===== 1-3. å‡ ä½•ç±»å‹ç¼–ç ï¼ˆ3ç»´ï¼‰=====
        geom_type = geometry.geom_type if hasattr(geometry, 'geom_type') else 'Unknown'
        if geom_type == 'Point':
            geom_features = [1, 0, 0]
        elif geom_type in ['LineString', 'MultiLineString']:
            geom_features = [0, 1, 0]
        elif geom_type in ['Polygon', 'MultiPolygon']:
            geom_features = [0, 0, 1]
        else:
            geom_features = [0, 0, 0]
        features.extend(geom_features)
        
        # è·å–åŸºæœ¬å‡ ä½•å±æ€§
        area = geometry.area if hasattr(geometry, 'area') else 0.0
        perimeter = geometry.length if hasattr(geometry, 'length') else 0.0
        
        # ===== 4. Huä¸å˜çŸ©Ï†1ï¼ˆå®Œå…¨å‡ ä½•ä¸å˜ï¼‰=====
        # HuçŸ©æ˜¯æœ€ç»å…¸çš„å½¢çŠ¶ä¸å˜é‡ï¼šå¯¹å¹³ç§»ã€ç¼©æ”¾ã€æ—‹è½¬éƒ½å®Œå…¨ä¸å˜
        # Ï†1 = Î·20 + Î·02ï¼ˆç¬¬ä¸€ä¸ªHuä¸å˜çŸ©ï¼Œæœ€ç¨³å®šï¼‰
        # æ›¿ä»£ç´§å‡‘åº¦ï¼Œæ¶ˆé™¤ä¸è¾¹ç•Œå¤æ‚åº¦çš„å†—ä½™
        
        if area > 1e-6 and geom_type in ['Polygon', 'MultiPolygon']:
            try:
                # æå–è¾¹ç•Œåæ ‡
                if geom_type == 'Polygon':
                    coords = np.array(geometry.exterior.coords[:-1])  # å»æ‰é‡å¤çš„æœ€åä¸€ç‚¹
                else:  # MultiPolygonï¼Œå–æœ€å¤§çš„å¤šè¾¹å½¢
                    largest_poly = max(geometry.geoms, key=lambda p: p.area)
                    coords = np.array(largest_poly.exterior.coords[:-1])
                
                if len(coords) >= 3:
                    # è®¡ç®—è´¨å¿ƒ
                    cx = np.mean(coords[:, 0])
                    cy = np.mean(coords[:, 1])
                    
                    # è®¡ç®—ä¸­å¿ƒçŸ© Î¼pq = Î£(x-cx)^p * (y-cy)^q
                    x_centered = coords[:, 0] - cx
                    y_centered = coords[:, 1] - cy
                    
                    mu20 = np.sum(x_centered**2) / len(coords)
                    mu02 = np.sum(y_centered**2) / len(coords)
                    mu11 = np.sum(x_centered * y_centered) / len(coords)
                    
                    # å½’ä¸€åŒ–ä¸­å¿ƒçŸ© Î·pq = Î¼pq / Î¼00^((p+q)/2+1)
                    # Î¼00 è¿‘ä¼¼ä¸º area
                    if area > 1e-6:
                        nu20 = mu20 / (area ** 1.0)  # (2+0)/2+1 = 2
                        nu02 = mu02 / (area ** 1.0)
                        
                        # ç¬¬ä¸€ä¸ªHuä¸å˜çŸ©ï¼šÏ†1 = Î·20 + Î·02
                        hu1 = nu20 + nu02
                        
                        # å¯¹æ•°å½’ä¸€åŒ–ï¼ˆHuçŸ©å€¼åŸŸå¯èƒ½å¾ˆå¤§ï¼‰
                        hu1_normalized = np.log1p(abs(hu1)) / 10.0
                    else:
                        hu1_normalized = 0.0
                else:
                    hu1_normalized = 0.0
            except Exception:
                hu1_normalized = 0.0
        else:
            # Pointå’ŒLineStringä½¿ç”¨ç®€åŒ–å€¼
            hu1_normalized = 0.0 if geom_type == 'Point' else 0.5
        
        features.append(hu1_normalized)
        
        # ===== 5. è¾¹ç•Œå¤æ‚åº¦ Boundary Complexityï¼ˆç¼©æ”¾ä¸å˜ï¼‰=====
        # å…¬å¼: perimeter / sqrt(area)
        # æ¯”å½¢çŠ¶æŒ‡æ•°æ›´ç¨³å®šï¼Œå¯¹å™ªå£°æ›´é²æ£’
        if area > 1e-6:
            boundary_complexity = perimeter / np.sqrt(area)
            # å¯¹æ•°å½’ä¸€åŒ–ï¼Œé¿å…å€¼è¿‡å¤§
            boundary_complexity = np.log1p(boundary_complexity) / 5.0  # ç»éªŒæ€§å½’ä¸€åŒ–
        else:
            boundary_complexity = 0.0
        features.append(boundary_complexity)
        
        # ===== 6-8. å½“å‰åœ°å›¾ç›¸å¯¹ä½ç½®ï¼ˆç‹¬ç«‹å…¨å±€ï¼Œå®è§‚ç©ºé—´ä¿¡æ¯ï¼‰=====
        # æè¿°èŠ‚ç‚¹åœ¨å½“å‰åœ°å›¾ä¸­çš„ä½ç½®
        # ä½¿ç”¨ç‹¬ç«‹å½’ä¸€åŒ–ï¼Œè£å‰ªæ”»å‡»åä»ä¿æŒç›¸å¯¹ç¨³å®š
        centroid = geometry.centroid
        
        if local_bounds is not None:
            # ç»´åº¦6: ç›¸å¯¹Xä½ç½®ï¼ˆå½’ä¸€åŒ–åˆ°[0,1]ï¼‰
            local_width = local_bounds[2] - local_bounds[0]
            if local_width > 1e-6:
                local_relative_x = (centroid.x - local_bounds[0]) / local_width
            else:
                local_relative_x = 0.5
            
            # ç»´åº¦7: ç›¸å¯¹Yä½ç½®ï¼ˆå½’ä¸€åŒ–åˆ°[0,1]ï¼‰
            local_height = local_bounds[3] - local_bounds[1]
            if local_height > 1e-6:
                local_relative_y = (centroid.y - local_bounds[1]) / local_height
            else:
                local_relative_y = 0.5
            
            # ç»´åº¦8: ç›¸å¯¹äºå½“å‰åœ°å›¾è´¨å¿ƒçš„è·ç¦»ï¼ˆå½’ä¸€åŒ–ï¼‰
            local_diagonal = np.sqrt(local_width**2 + local_height**2)
            if local_diagonal > 1e-6 and local_centroid is not None:
                distance_to_local_center = centroid.distance(local_centroid) / local_diagonal
            else:
                distance_to_local_center = 0.0
            
            features.extend([local_relative_x, local_relative_y, distance_to_local_center])
        else:
            # å¦‚æœæ²¡æœ‰æä¾›è¾¹ç•Œæ¡†ï¼Œä½¿ç”¨é»˜è®¤å€¼
            features.extend([0.5, 0.5, 0.0])
        
        # ===== 9-11. å±€éƒ¨ç›¸å¯¹ä½ç½®ï¼ˆå¾®è§‚ç©ºé—´ä¿¡æ¯ï¼ŒæŠ—è£å‰ªï¼‰=====
        # åŸºäºKè¿‘é‚»çš„å±€éƒ¨å‚è€ƒç³»ï¼Œå³ä½¿è£å‰ªåä¹Ÿç¨³å®š
        # è¿™æ˜¯æ–¹æ¡ˆBçš„æ ¸å¿ƒï¼šå…¨å±€+å±€éƒ¨å¤šå°ºåº¦è¡¨è¾¾
        
        if k_neighbors_info is not None:
            # ä½¿ç”¨é¢„è®¡ç®—çš„ Kè¿‘é‚»ï¼ˆKD-tree åŠ é€Ÿï¼ŒO(n log n)ï¼‰
            neighbor_centroids = k_neighbors_info['centroids']
            neighbor_distances = k_neighbors_info['distances']
            
            if len(neighbor_centroids) > 0:
                # è®¡ç®—å±€éƒ¨è´¨å¿ƒï¼ˆKè¿‘é‚»çš„å¹³å‡ä½ç½®ï¼‰
                local_centroid_x = np.mean([c.x for c in neighbor_centroids])
                local_centroid_y = np.mean([c.y for c in neighbor_centroids])
                local_centroid = Point(local_centroid_x, local_centroid_y)
                
                # è®¡ç®—å±€éƒ¨åŠå¾„ï¼ˆKè¿‘é‚»çš„å¹³å‡è·ç¦»ï¼‰
                local_radius = np.mean(neighbor_distances)
                
                # ç»´åº¦9: ç›¸å¯¹äºå±€éƒ¨è´¨å¿ƒçš„Xåç§»ï¼ˆå½’ä¸€åŒ–ï¼‰
                if local_radius > 1e-6:
                    local_relative_x = (centroid.x - local_centroid.x) / (local_radius * 2)
                    local_relative_x = np.clip(local_relative_x, -1, 1)  # é™åˆ¶åˆ°[-1, 1]
                else:
                    local_relative_x = 0.0
                
                # ç»´åº¦10: ç›¸å¯¹äºå±€éƒ¨è´¨å¿ƒçš„Yåç§»ï¼ˆå½’ä¸€åŒ–ï¼‰
                if local_radius > 1e-6:
                    local_relative_y = (centroid.y - local_centroid.y) / (local_radius * 2)
                    local_relative_y = np.clip(local_relative_y, -1, 1)
                else:
                    local_relative_y = 0.0
                
                # ç»´åº¦11: åˆ°å±€éƒ¨è´¨å¿ƒçš„è·ç¦»ï¼ˆå½’ä¸€åŒ–ï¼‰
                if local_radius > 1e-6:
                    distance_to_local_center = centroid.distance(local_centroid) / local_radius
                else:
                    distance_to_local_center = 0.0
                
                features.extend([local_relative_x, local_relative_y, distance_to_local_center])
            else:
                # å¦‚æœæ²¡æœ‰å…¶ä»–èŠ‚ç‚¹ï¼Œä½¿ç”¨é»˜è®¤å€¼
                features.extend([0.0, 0.0, 0.0])
        else:
            # å¦‚æœæ²¡æœ‰æä¾›all_centroidsï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆç¬¬ä¸€æ¬¡éå†æ—¶ï¼‰
            features.extend([0.0, 0.0, 0.0])
        
        # ===== 12-13. é•¿å®½æ¯” + çŸ©å½¢åº¦ï¼ˆæ—‹è½¬ä¸å˜ï¼‰=====
        if geom_type in ['Polygon', 'MultiPolygon'] and area > 0:
            try:
                # æœ€å°å¤–æ¥çŸ©å½¢
                min_rect = geometry.minimum_rotated_rectangle
                
                # å¤„ç†MultiPolygonçš„æƒ…å†µ
                if min_rect.geom_type == 'MultiPolygon':
                    # å¦‚æœæœ€å°å¤–æ¥çŸ©å½¢æ˜¯MultiPolygonï¼Œå–æœ€å¤§çš„é‚£ä¸ª
                    largest_rect = max(min_rect.geoms, key=lambda p: p.area)
                    rect_coords = list(largest_rect.exterior.coords)
                else:
                    rect_coords = list(min_rect.exterior.coords)
                
                # è®¡ç®—çŸ©å½¢çš„ä¸¤æ¡è¾¹é•¿
                edge1 = np.linalg.norm(
                    np.array(rect_coords[0]) - np.array(rect_coords[1])
                )
                edge2 = np.linalg.norm(
                    np.array(rect_coords[1]) - np.array(rect_coords[2])
                )
                
                # é•¿å®½æ¯”ï¼ˆå½’ä¸€åŒ–ï¼‰
                if min(edge1, edge2) > 0:
                    aspect_ratio = max(edge1, edge2) / min(edge1, edge2)
                    # å¯¹æ•°å˜æ¢ï¼Œé¿å…æç«¯å€¼
                    aspect_ratio = np.log1p(aspect_ratio) / 3.0  # ç»éªŒæ€§å½’ä¸€åŒ–
                else:
                    aspect_ratio = 0.0
                
                # çŸ©å½¢åº¦ï¼šåŸå›¾å½¢é¢ç§¯ / æœ€å°å¤–æ¥çŸ©å½¢é¢ç§¯
                rect_area = min_rect.area
                if rect_area > 0:
                    rectangularity = area / rect_area
                else:
                    rectangularity = 0.0
                
            except Exception as e:
                aspect_ratio, rectangularity = 0.0, 0.0
        else:
            aspect_ratio, rectangularity = 0.0, 1.0 if geom_type == 'Point' else 0.0
        
        features.extend([aspect_ratio, rectangularity])
        
        # ===== 14. Solidity å®å¿ƒåº¦ï¼ˆå½¢çŠ¶å¤æ‚åº¦ï¼‰=====
        # å…¬å¼: area / convex_hull.area
        # è¡¡é‡å½¢çŠ¶çš„å‡¹å‡¸ç¨‹åº¦ï¼šå‡¸å¤šè¾¹å½¢=1.0ï¼Œå‡¹è¿›å»è¶Šå¤šå€¼è¶Šå°
        if area > 0:
            try:
                convex_hull = geometry.convex_hull
                convex_area = convex_hull.area
                if convex_area > 0:
                    solidity = area / convex_area
                else:
                    solidity = 1.0
            except:
                solidity = 1.0
        else:
            solidity = 1.0 if geom_type == 'Point' else 0.0
        features.append(solidity)
        
        # ===== 15. å¯¹æ•°é¡¶ç‚¹æ•°ï¼ˆå¤æ‚åº¦æŒ‡æ ‡ï¼‰=====
        if geom_type == 'Point':
            num_vertices = 1
        elif geom_type in ['LineString', 'MultiLineString']:
            if geom_type == 'LineString':
                num_vertices = len(list(geometry.coords))
            else:  # MultiLineString
                num_vertices = sum(len(list(line.coords)) for line in geometry.geoms)
        elif geom_type in ['Polygon', 'MultiPolygon']:
            if geom_type == 'Polygon':
                num_vertices = len(list(geometry.exterior.coords))
            else:  # MultiPolygon
                num_vertices = sum(len(list(poly.exterior.coords)) for poly in geometry.geoms)
        else:
            num_vertices = 1
        
        # å¯¹æ•°å½’ä¸€åŒ–
        log_vertices = np.log1p(num_vertices) / 10.0  # ç»éªŒæ€§å½’ä¸€åŒ–
        features.append(log_vertices)
        
        # ===== 16-18. æ‹“æ‰‘é‚»åŸŸç‰¹å¾ï¼ˆå ä½ç¬¦ï¼Œåç»­æ›´æ–°ï¼‰=====
        # è¿™äº›ç‰¹å¾éœ€è¦åœ¨æ„å»ºæ‹“æ‰‘é‚»æ¥å›¾åè®¡ç®—
        # æš‚æ—¶å¡«å……0ï¼Œåœ¨update_topology_neighborhood_featuresä¸­æ›´æ–°
        features.extend([0.0, 0.0, 0.0])
        
        # ===== 19. å­”æ´æ•°é‡ Holesï¼ˆæ‹“æ‰‘ç‰¹å¾ï¼‰=====
        # å¤šè¾¹å½¢å†…éƒ¨çš„å­”æ´æ•°é‡ï¼Œå¯¹åˆ é™¤/è£å‰ªæ”»å‡»é²æ£’
        if geom_type == 'Polygon':
            try:
                num_holes = len(geometry.interiors)
            except:
                num_holes = 0
        elif geom_type == 'MultiPolygon':
            try:
                num_holes = sum(len(poly.interiors) for poly in geometry.geoms)
            except:
                num_holes = 0
        else:
            # Pointå’ŒLineStringæ²¡æœ‰å­”æ´
            num_holes = 0
        
        # å¯¹æ•°å½’ä¸€åŒ–ï¼ˆå¤§å¤šæ•°å¤šè¾¹å½¢æ²¡æœ‰å­”æ´ï¼‰
        log_holes = np.log1p(num_holes) / 5.0
        features.append(log_holes)
        
        # ===== 20. èŠ‚ç‚¹æ•°ç¼–ç ï¼ˆå›¾è§„æ¨¡ä¿¡æ¯ï¼‰â­æ–°å¢ =====
        # è¡¥å¿è‡ªé€‚åº”Kå€¼å½’ä¸€åŒ–åä¸¢å¤±çš„å›¾è§„æ¨¡ä¿¡æ¯
        # å¯¹æ•°å½’ä¸€åŒ–ï¼šlog10(n+1) / 4.0
        # èŒƒå›´ç¤ºä¾‹ï¼š
        #   n=10    -> 0.25
        #   n=100   -> 0.50
        #   n=1000  -> 0.75
        #   n=10000 -> 1.00
        if total_nodes is not None and total_nodes > 0:
            node_count_feature = np.log10(total_nodes + 1) / 4.0
        else:
            # å¦‚æœæœªæä¾›ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆå‡è®¾ä¸­ç­‰è§„æ¨¡å›¾ï¼‰
            node_count_feature = 0.5
        features.append(node_count_feature)
        
        # â­ æ–¹æ¡ˆAä¼˜åŒ–ï¼šæ·»åŠ  clip é˜²æ­¢æç«¯æ”»å‡»äº§ç”Ÿè¶…å‡ºèŒƒå›´çš„ç‰¹å¾å€¼
        features = np.array(features, dtype=np.float32)
        features = np.clip(features, -10.0, 10.0)  # é™åˆ¶ç‰¹å¾èŒƒå›´ï¼Œé˜²æ­¢æç«¯å€¼å½±å“æ ‡å‡†åŒ–
        
        return features
    
    def update_topology_neighborhood_features(self, node_features, geometries, topology_edges, local_bounds=None):
        """
        æ›´æ–°ç©ºé—´é‚»åŸŸç‰¹å¾ï¼ˆç‰¹å¾ç»´åº¦15-17ï¼‰- åŸºäºDelaunayä¸‰è§’å‰–åˆ†â­ä¼˜åŒ–ç‰ˆ
        
        ğŸ¯ æ ¸å¿ƒæ”¹è¿›ï¼šç”¨Delaunayé‚»å±…æ›¿ä»£åŸå§‹æ‹“æ‰‘è¾¹ï¼Œå½»åº•è§£å†³NaNé—®é¢˜
        
        åŸºäºDelaunayä¸‰è§’å‰–åˆ†é‚»æ¥å…³ç³»è®¡ç®—ï¼š
        - ç»´åº¦15: ä¸Delaunayé‚»å±…çš„å¹³å‡è·ç¦»ï¼ˆå½’ä¸€åŒ–ï¼Œclipé™åˆ¶ï¼‰
        - ç»´åº¦16: Delaunayé‚»å±…æ•°é‡ï¼ˆå¯¹æ•°å½’ä¸€åŒ–ï¼Œè‡ªåŠ¨åæ˜ å¯†åº¦ï¼‰â­
        - ç»´åº¦17: Delaunayé‚»åŸŸå¯†åº¦ï¼ˆé‚»å±…æ•°/é¢ç§¯ï¼‰
        
        ä¼˜åŠ¿ï¼š
        âœ… å®Œå…¨å‡ ä½•ä¸å˜ï¼šåªä¾èµ–è´¨å¿ƒåæ ‡
        âœ… æŠ—æ‹“æ‰‘æ”»å‡»ï¼šé¡¶ç‚¹æ‰“ä¹±ä¸å½±å“Delaunayç»“æ„
        âœ… é‚»å±…æ•°è‡ªé€‚åº”ï¼šå¯†é›†åŒºåŸŸå¤šé‚»å±…ï¼Œç¨€ç–åŒºåŸŸå°‘é‚»å±…
        âœ… æ•°å€¼ç¨³å®šï¼šæ·»åŠ clipé˜²æ­¢æº¢å‡ºåˆ°ä¸‡äº¿çº§åˆ«
        âœ… æ— éœ€é‡å¤è®¡ç®—ï¼šå¤ç”¨build_knn_delaunay_edgesçš„ç»“æœ
        
        Args:
            node_features: èŠ‚ç‚¹ç‰¹å¾çŸ©é˜µ
            geometries: å‡ ä½•è¦ç´ åˆ—è¡¨
            topology_edges: Delaunayè¾¹åˆ—è¡¨ [[i, j], ...]ï¼ˆä»build_knn_delaunay_edgesè·å–ï¼‰
            local_bounds: å½“å‰åœ°å›¾è¾¹ç•Œæ¡†ï¼ˆç”¨äºå½’ä¸€åŒ–è·ç¦»ï¼‰
        """
        n_samples = len(geometries)
        if n_samples < 2:
            return node_features
        
        # æ„å»ºé‚»æ¥è¡¨
        adjacency = {i: set() for i in range(n_samples)}
        for edge in topology_edges:
            if len(edge) == 2:
                i, j = edge
                adjacency[i].add(j)
                # è¾¹å·²ç»æ˜¯åŒå‘çš„ï¼Œä¸éœ€è¦é‡å¤æ·»åŠ 
        
        # æå–è´¨å¿ƒ
        centroids = [g.centroid for g in geometries]
        
        # è®¡ç®—å½“å‰åœ°å›¾å¯¹è§’çº¿ç”¨äºå½’ä¸€åŒ–
        if local_bounds is not None:
            local_width = local_bounds[2] - local_bounds[0]
            local_height = local_bounds[3] - local_bounds[1]
            local_diagonal = np.sqrt(local_width**2 + local_height**2)
        else:
            local_diagonal = 1.0
        
        # é˜²æ­¢é™¤é›¶
        if local_diagonal < 1e-6:
            local_diagonal = 1.0
        
        # æ›´æ–°æ¯ä¸ªèŠ‚ç‚¹çš„Delaunayé‚»åŸŸç‰¹å¾
        for i in range(n_samples):
            neighbors = list(adjacency[i])
            
            if len(neighbors) > 0:
                # è®¡ç®—åˆ°Delaunayé‚»å±…çš„è·ç¦»
                distances = [centroids[i].distance(centroids[j]) for j in neighbors]
                avg_dist_raw = np.mean(distances)
                
                # ç»´åº¦15: å¹³å‡è·ç¦»ï¼ˆå½’ä¸€åŒ–ï¼‰â­æ·»åŠ clipé˜²æ­¢æº¢å‡º
                avg_distance = avg_dist_raw / local_diagonal
                avg_distance = np.clip(avg_distance, 0.0, 1.0)  # â­å…³é”®ï¼šé™åˆ¶åˆ°[0,1]
                
                # ç»´åº¦16: Delaunayé‚»å±…æ•°é‡ï¼ˆå¯¹æ•°å½’ä¸€åŒ–ï¼‰â­è‡ªåŠ¨åæ˜ å¯†åº¦å·®å¼‚
                num_neighbors = np.log1p(len(neighbors)) / 5.0
                num_neighbors = np.clip(num_neighbors, 0.0, 1.0)  # â­é™åˆ¶èŒƒå›´
                
                # ç»´åº¦17: Delaunayé‚»åŸŸå¯†åº¦â­æ·»åŠ clipé˜²æ­¢æº¢å‡º
                if avg_dist_raw > 1e-10:
                    # é‚»åŸŸé¢ç§¯ï¼ˆåœ†å½¢å‡è®¾ï¼šÏ€ * rÂ²ï¼‰
                    neighborhood_area = np.pi * (avg_dist_raw ** 2)
                    density = len(neighbors) / neighborhood_area
                    
                    # å¯¹æ•°å½’ä¸€åŒ–
                    density = np.log1p(density * 1000) / 10.0
                    density = np.clip(density, 0.0, 1.0)  # â­é™åˆ¶èŒƒå›´
                else:
                    density = 0.0
            else:
                # å¦‚æœæ²¡æœ‰é‚»å±…ï¼ˆç†è®ºä¸ŠDelaunayä¸ä¼šæœ‰å­¤å²›ï¼Œä½†ä¿é™©èµ·è§ï¼‰
                avg_distance = 0.0
                num_neighbors = 0.0
                density = 0.0
            
            node_features[i, 15] = avg_distance
            node_features[i, 16] = num_neighbors
            node_features[i, 17] = density
        
        return node_features
    
    def build_knn_delaunay_edges(self, geometries):
        """
        â­ æ„å»ºKNN + Delaunayç»Ÿä¸€å›¾ï¼ˆæ”¯æŒå¤§å›¾Hilbertåˆ†å—ä¼˜åŒ–ï¼‰
        
        ç­–ç•¥ï¼š
        1. KNNä¿è¯å±€éƒ¨å¯†é›†è¿æ¥ï¼ˆæ¯ä¸ªèŠ‚ç‚¹è‡³å¤škä¸ªé‚»å±…ï¼‰
        2. Delaunayä¸‰è§’å‰–åˆ†ä¿è¯å…¨å±€è¿é€šï¼š
           - èŠ‚ç‚¹â‰¤5000ï¼šç›´æ¥Delaunay
           - èŠ‚ç‚¹>5000ï¼šâ­ Hilbertæ›²çº¿åˆ†å— + è·¨å—KNNè¿æ¥
        3. åˆå¹¶å»é‡ï¼Œè¿”å›å•å‘è¾¹åˆ—è¡¨
        
        Args:
            geometries: å‡ ä½•è¦ç´ åˆ—è¡¨
            
        Returns:
            list: è¾¹åˆ—è¡¨ [[src, dst], ...]
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        from sklearn.neighbors import NearestNeighbors
        from scipy.spatial import Delaunay
        
        n = len(geometries)
        
        # è‡ªé€‚åº”Kå€¼
        k = self.adaptive_k_for_graph(n)
        
        # é™åˆ¶Kå€¼ä¸è¶…è¿‡èŠ‚ç‚¹æ•°-1
        k = min(k, n - 1)
        
        if n < 2:
            return [], {'total_nodes': n, 'knn_k': 0, 'knn_edges': 0, 
                       'delaunay_edges': 0, 'total_edges': 0, 
                       'isolated_nodes': n, 'avg_degree': 0, 
                       'delaunay_edges_list': []}
        
        # æå–è´¨å¿ƒ
        centroids = np.array([[geom.centroid.x, geom.centroid.y] for geom in geometries])
        
        edges_set = set()
        
        # ==== ç¬¬1æ­¥ï¼šKNNè¾¹ï¼ˆå…¨å±€ï¼‰ ====
        print(f"  [1/2] æ„å»ºKNNå›¾ï¼ˆK={k}ï¼Œå…±{n}ä¸ªèŠ‚ç‚¹ï¼‰...")
        knn_start = time.time()
        nbrs = NearestNeighbors(n_neighbors=min(k+1, n), algorithm='kd_tree').fit(centroids)
        distances, indices = nbrs.kneighbors(centroids)
        
        for i in range(n):
            for j in indices[i][1:]:  # è·³è¿‡è‡ªå·±ï¼ˆç¬¬ä¸€ä¸ªæ˜¯è‡ªå·±ï¼‰
                edge = tuple(sorted([i, j]))  # æ— å‘è¾¹ï¼šç»Ÿä¸€ä¸º (min, max)
                edges_set.add(edge)
        
        knn_edges_count = len(edges_set)
        print(f"  âœ“ KNNå®Œæˆï¼Œè¾¹æ•°: {knn_edges_count}ï¼Œè€—æ—¶: {time.time()-knn_start:.2f}ç§’")
        
        # ==== ç¬¬2æ­¥ï¼šDelaunayè¾¹ï¼ˆè‡ªé€‚åº”ç­–ç•¥ï¼Œn<3æ—¶è·³è¿‡ï¼‰ ====
        delaunay_edges_list = []  # ä¿å­˜Delaunayè¾¹åˆ—è¡¨ç”¨äºç‰¹å¾è®¡ç®—
        
        if n < 3:
            # èŠ‚ç‚¹æ•°<3ï¼Œæ— æ³•æ„å»ºDelaunayä¸‰è§’å‰–åˆ†ï¼Œåªä½¿ç”¨KNN
            print(f"  [2/2] è·³è¿‡Delaunayï¼ˆèŠ‚ç‚¹æ•°={n}<3ï¼Œåªä½¿ç”¨KNNè¾¹ï¼‰")
        elif n >= 3:
            if n <= 5000:
                # ğŸ”¹ å°å›¾ï¼šç›´æ¥Delaunay
                print(f"  [2/2] æ„å»ºDelaunayä¸‰è§’å‰–åˆ†ï¼ˆ{n}ä¸ªèŠ‚ç‚¹ï¼‰...")
                delaunay_start = time.time()
                try:
                    tri = Delaunay(centroids)
                    delaunay_edges_count = 0
                    
                    for simplex in tri.simplices:
                        # ä¸‰è§’å½¢çš„ä¸‰æ¡è¾¹
                        for i in range(3):
                            v1 = simplex[i]
                            v2 = simplex[(i+1) % 3]
                            edge = tuple(sorted([v1, v2]))
                            delaunay_edges_list.append([v1, v2])
                            if edge not in edges_set:
                                delaunay_edges_count += 1
                            edges_set.add(edge)
                    
                    print(f"  âœ“ Delaunayå®Œæˆï¼Œæ–°å¢è¾¹æ•°: {delaunay_edges_count}ï¼Œè€—æ—¶: {time.time()-delaunay_start:.2f}ç§’")
                except Exception as e:
                    print(f"  âš ï¸  Delaunayå¤±è´¥: {e}ï¼Œä»…ä½¿ç”¨KNNè¾¹")
            else:
                # â­ å¤§å›¾ï¼šHilbertåˆ†å—Delaunay
                print(f"  [2/2] â­ å¤§å›¾ä¼˜åŒ–ï¼šHilbertåˆ†å—Delaunayï¼ˆ{n}ä¸ªèŠ‚ç‚¹ï¼‰...")
                delaunay_start = time.time()
                
                # åˆ†å—
                blocks = self.partition_by_hilbert(centroids, block_size=2000)
                
                total_delaunay_edges = 0
                
                # å¯¹æ¯ä¸ªå—åšDelaunay
                for block_idx, block_indices in enumerate(blocks):
                    if len(block_indices) < 3:
                        continue
                    
                    block_centroids = centroids[block_indices]
                    
                    try:
                        tri = Delaunay(block_centroids)
                        
                        for simplex in tri.simplices:
                            for i in range(3):
                                local_v1 = simplex[i]
                                local_v2 = simplex[(i+1) % 3]
                                # æ˜ å°„å›å…¨å±€ç´¢å¼•
                                global_v1 = block_indices[local_v1]
                                global_v2 = block_indices[local_v2]
                                edge = tuple(sorted([global_v1, global_v2]))
                                delaunay_edges_list.append([global_v1, global_v2])
                                if edge not in edges_set:
                                    total_delaunay_edges += 1
                                edges_set.add(edge)
                    except Exception as e:
                        print(f"    âš ï¸  å—{block_idx}çš„Delaunayå¤±è´¥: {e}")
                        continue
                
                # è·¨å—è¿æ¥ï¼šä½¿ç”¨KNNè¿æ¥ç›¸é‚»å—çš„è¾¹ç•ŒèŠ‚ç‚¹
                print(f"  â­ è·¨å—è¿æ¥ï¼ˆKNN K={k//2}ï¼‰...")
                cross_block_edges = 0
                
                for i in range(len(blocks) - 1):
                    # å–å½“å‰å—çš„æœ€å10%èŠ‚ç‚¹å’Œä¸‹ä¸€å—çš„å‰10%èŠ‚ç‚¹
                    block1_indices = blocks[i]
                    block2_indices = blocks[i+1]
                    
                    boundary1_size = max(1, len(block1_indices) // 10)
                    boundary2_size = max(1, len(block2_indices) // 10)
                    
                    boundary1 = block1_indices[-boundary1_size:]
                    boundary2 = block2_indices[:boundary2_size]
                    
                    # åœ¨è¾¹ç•ŒèŠ‚ç‚¹é—´åšKNNè¿æ¥
                    boundary_centroids = centroids[boundary1 + boundary2]
                    boundary_k = min(k//2, len(boundary_centroids)-1)
                    
                    if boundary_k >= 1:
                        nbrs_boundary = NearestNeighbors(
                            n_neighbors=boundary_k+1, algorithm='kd_tree'
                        ).fit(boundary_centroids)
                        _, indices_boundary = nbrs_boundary.kneighbors(boundary_centroids)
                        
                        for local_i, neighbors in enumerate(indices_boundary):
                            global_i = (boundary1 + boundary2)[local_i]
                            for local_j in neighbors[1:]:
                                global_j = (boundary1 + boundary2)[local_j]
                                edge = tuple(sorted([global_i, global_j]))
                                if edge not in edges_set:
                                    cross_block_edges += 1
                                edges_set.add(edge)
                
                print(f"  âœ“ åˆ†å—Delaunayå®Œæˆï¼š")
                print(f"    - å—å†…è¾¹: {total_delaunay_edges}")
                print(f"    - è·¨å—è¾¹: {cross_block_edges}")
                print(f"    - è€—æ—¶: {time.time()-delaunay_start:.2f}ç§’")
        else:
            print(f"  [2/2] èŠ‚ç‚¹æ•°<3ï¼Œè·³è¿‡Delaunay")
        
        # âœ… ç¬¬3æ­¥ï¼šè½¬æ¢ä¸ºè¾¹åˆ—è¡¨ï¼ˆå•å‘è¡¨ç¤ºï¼‰
        edges_list = [[e[0], e[1]] for e in edges_set]
        
        print(f"  âœ… å»é‡å®Œæˆï¼Œæ— å‘è¾¹æ•°: {len(edges_list)} æ¡ï¼ˆå•å‘è¡¨ç¤ºï¼‰")
        
        # ç»Ÿè®¡å­¤å²›èŠ‚ç‚¹
        connected_nodes = set()
        for edge in edges_list:
            connected_nodes.add(edge[0])
            connected_nodes.add(edge[1])
        
        isolated_count = n - len(connected_nodes)
        
        stats = {
            'total_nodes': n,
            'knn_k': k,
            'knn_edges': knn_edges_count,
            'delaunay_edges': len(delaunay_edges_list),
            'total_edges': len(edges_list),
            'isolated_nodes': isolated_count,
            'avg_degree': (len(edges_list) * 2) / n if n > 0 else 0,
            'delaunay_edges_list': delaunay_edges_list  # â­ä¿å­˜Delaunayè¾¹åˆ—è¡¨
        }
        
        return edges_list, stats
    
    def build_rng_edges(self, geometries, node_indices=None, k=5):
        """
        æ„å»ºRNGè¡¥å……è¾¹ï¼ˆKè¿‘é‚»å¿«é€Ÿç‰ˆæœ¬ï¼‰
        
        åŸç†ï¼š
        - åŸå§‹RNG: O(nÂ³) - å¯¹æ¯å¯¹èŠ‚ç‚¹æ£€æŸ¥æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹
        - ä¼˜åŒ–ç‰ˆæœ¬: O(n log n) - å¯¹å­¤å²›èŠ‚ç‚¹è¿æ¥Kä¸ªæœ€è¿‘é‚»
        
        ä¼˜åŒ–è¯´æ˜ï¼š
        - ä¸¥æ ¼çš„RNGå¯¹äºå­¤å²›èŠ‚ç‚¹è¡¥å……æ¥è¯´è¿‡äºä¸¥æ ¼ä¸”è®¡ç®—æ˜‚è´µ
        - ä½¿ç”¨Kè¿‘é‚»ä½œä¸ºRNGçš„å®ç”¨è¿‘ä¼¼ï¼šä¿è¯è¿é€šæ€§ï¼Œè®¡ç®—å¿«é€Ÿ
        - K=5é€šå¸¸è¶³å¤Ÿä¿è¯è¿é€šæ€§ï¼ŒåŒæ—¶ä¿æŒå›¾çš„ç¨€ç–æ€§
        
        Args:
            geometries: æ‰€æœ‰å‡ ä½•è¦ç´ 
            node_indices: éœ€è¦å¤„ç†çš„èŠ‚ç‚¹ç´¢å¼•åˆ—è¡¨ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨èŠ‚ç‚¹ï¼‰
            k: æ¯ä¸ªå­¤å²›èŠ‚ç‚¹è¿æ¥çš„æœ€è¿‘é‚»æ•°é‡ï¼ˆé»˜è®¤5ï¼‰
        """
        if node_indices is None:
            node_indices = list(range(len(geometries)))
        
        if len(node_indices) == 0:
            return []
        
        from sklearn.neighbors import NearestNeighbors
        
        # æå–æ‰€æœ‰èŠ‚ç‚¹çš„è´¨å¿ƒ
        all_centroids = np.array([[g.centroid.x, g.centroid.y] for g in geometries])
        
        edges = []
        n_total = len(geometries)
        
        # åŠ¨æ€è°ƒæ•´kï¼šä¸èƒ½è¶…è¿‡æ€»èŠ‚ç‚¹æ•°-1
        actual_k = min(k, n_total - 1)
        
        if actual_k < 1:
            return []
        
        print(f"  Kè¿‘é‚»è¡¥å……ï¼ˆK={actual_k}ï¼Œå¤„ç† {len(node_indices)} ä¸ªå­¤å²›èŠ‚ç‚¹ï¼‰...")
        
        # ä½¿ç”¨KDæ ‘åŠ é€Ÿæœ€è¿‘é‚»æœç´¢
        nbrs = NearestNeighbors(
            n_neighbors=actual_k + 1,  # +1å› ä¸ºåŒ…æ‹¬è‡ªå·±
            algorithm='kd_tree'
        ).fit(all_centroids)
        
        # ä¸ºæ¯ä¸ªå­¤å²›èŠ‚ç‚¹æ‰¾Kä¸ªæœ€è¿‘é‚»
        for i in node_indices:
            distances, indices = nbrs.kneighbors([all_centroids[i]])
            
            # æ’é™¤è‡ªå·±ï¼ˆç¬¬ä¸€ä¸ªæ˜¯è‡ªå·±ï¼‰ï¼Œå–Kä¸ªæœ€è¿‘é‚»
            for j in indices[0][1:actual_k+1]:
                edges.append([i, int(j)])
        
        print(f"  è¡¥å……è¾¹æ•°é‡: {len(edges)}")
        print(f"  åŠ é€Ÿæ•ˆæœ: O(n log n) vs åŸRNGçš„ O(nÂ³)")
        
        return edges
    
    def find_isolated_nodes(self, edges, n_nodes):
        """
        æ‰¾å‡ºå­¤å²›èŠ‚ç‚¹ï¼ˆæ²¡æœ‰ä»»ä½•è¾¹çš„èŠ‚ç‚¹ï¼‰
        
        Args:
            edges: è¾¹åˆ—è¡¨ [[src, dst], ...]
            n_nodes: æ€»èŠ‚ç‚¹æ•°
        
        Returns:
            set: å­¤å²›èŠ‚ç‚¹çš„ç´¢å¼•é›†åˆ
        """
        connected_nodes = set()
        for edge in edges:
            connected_nodes.add(edge[0])
            connected_nodes.add(edge[1])
        
        all_nodes = set(range(n_nodes))
        isolated = all_nodes - connected_nodes
        
        return isolated
    
    def build_level0_edges(self, geometries):
        """
        æ„å»ºç¬¬0å±‚ï¼ˆèŠ‚ç‚¹å±‚ï¼‰çš„è¾¹
        ç­–ç•¥ï¼šKNN + Delaunay ç»Ÿä¸€å›¾æ„å»º
        
        Returns:
            list: è¾¹åˆ—è¡¨
            dict: ç»Ÿè®¡ä¿¡æ¯
        """
        n = len(geometries)
        print(f"\n=== æ„å»ºå›¾ç»“æ„ï¼ˆKNN + Delaunayï¼Œ{n}ä¸ªèŠ‚ç‚¹ï¼‰===")
        
        # ç›´æ¥ä½¿ç”¨ KNN + Delaunay ç»Ÿä¸€å›¾æ„å»º
        unique_edges, stats = self.build_knn_delaunay_edges(geometries)
        
        return unique_edges, stats
    
    
    def build_knn_delaunay_graph(self, geometries, node_features):
        """
        æ„å»º KNN + Delaunay ç»Ÿä¸€å›¾ï¼ˆç®€åŒ–ç‰ˆï¼Œç§»é™¤èšç±»è®¡ç®—ï¼‰
        
        ç­–ç•¥ï¼š
        1. è‡ªé€‚åº”Kå€¼ï¼šKæœ€å¤§ä¸º20ï¼ˆæ ¹æ®èŠ‚ç‚¹æ•°åŠ¨æ€è°ƒæ•´ï¼‰
        2. KNNæ„å»ºï¼šæ¯ä¸ªèŠ‚ç‚¹è¿æ¥Kä¸ªæœ€è¿‘é‚»ï¼ˆå±€éƒ¨å¯†é›†ï¼‰
        3. Delaunayä¸‰è§’å‰–åˆ†ï¼šè¦†ç›–å…¨å›¾ï¼ˆå…¨å±€è¿é€šï¼‰
        
        Returns:
            dict: åŒ…å«å›¾ç»“æ„ä¿¡æ¯çš„å­—å…¸
        """
        n = len(geometries)
        
        print(f"\n{'='*60}")
        print(f"æ„å»º KNN + Delaunay ç»Ÿä¸€å›¾ï¼ˆ{n} ä¸ªèŠ‚ç‚¹ï¼‰")
        print(f"{'='*60}")
        
        # === æ„å»ºä¸»å›¾ç»“æ„ ===
        level0_edges, level0_stats = self.build_level0_edges(geometries)
        
        print(f"\n{'='*60}")
        print(f"KNN + Delaunay å›¾æ„å»ºå®Œæˆ")
        print(f"{'='*60}")
        print(f"èŠ‚ç‚¹æ•°: {n}")
        print(f"è¾¹æ•°: {level0_stats['total_edges']} å¯¹")
        print(f"è‡ªé€‚åº”Kå€¼: {level0_stats.get('knn_k', 'N/A')}")
        print(f"å¹³å‡åº¦æ•°: {level0_stats.get('avg_degree', 0):.2f}")
        print(f"è¿é€šæ€§: æ— å­¤å²›èŠ‚ç‚¹ï¼ˆDelaunayä¿è¯ï¼‰")
        print(f"{'='*60}\n")
        
        return {
            'edges': level0_edges,
            'stats': level0_stats
        }
    
    def build_graph_from_gdf(self, gdf, graph_name, use_global_scaler=True):
        """
        ä»GeoDataFrameæ„å»ºæ‹“æ‰‘å¢å¼ºå›¾ï¼ˆç®€åŒ–ä¼˜åŒ–ç‰ˆï¼‰
        
        Returns:
            Data: PyTorch Geometric Dataå¯¹è±¡ï¼ŒåŒ…å«èšç±»ä¿¡æ¯
        """
        print(f"\n{'='*60}")
        print(f"å¤„ç†: {graph_name}")
        print(f"{'='*60}")
        
        # æå–å‡ ä½•è¦ç´ 
        geometries = gdf.geometry.tolist()
        
        # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—å½“å‰åœ°å›¾çš„è¾¹ç•Œæ¡†å’Œè´¨å¿ƒï¼ˆç‹¬ç«‹å…¨å±€ï¼‰
        all_bounds = [geom.bounds for geom in geometries]
        local_bounds = (
            min(b[0] for b in all_bounds),  # min_x
            min(b[1] for b in all_bounds),  # min_y
            max(b[2] for b in all_bounds),  # max_x
            max(b[3] for b in all_bounds)   # max_y
        )
        
        # è®¡ç®—å½“å‰åœ°å›¾çš„è´¨å¿ƒ
        all_centroids = [Point(geom.centroid.x, geom.centroid.y) for geom in geometries]
        local_centroid = Point(
            np.mean([c.x for c in all_centroids]),
            np.mean([c.y for c in all_centroids])
        )
        
        print(f"å½“å‰åœ°å›¾è¾¹ç•Œæ¡†: {local_bounds}")
        print(f"å½“å‰åœ°å›¾è´¨å¿ƒ: ({local_centroid.x:.2f}, {local_centroid.y:.2f})")
        
        # ä½¿ç”¨ KD-tree é¢„è®¡ç®— Kè¿‘é‚»ï¼ˆå¤§å¹…åŠ é€Ÿï¼‰
        k_neighbors_dict = self.precompute_k_neighbors(all_centroids)
        
        # ç¬¬äºŒæ­¥ï¼šæå–ç‰¹å¾ï¼ˆä¼ å…¥å½“å‰åœ°å›¾çš„ç»Ÿè®¡é‡ï¼‰
        total_nodes = len(geometries)  # æ€»èŠ‚ç‚¹æ•°
        node_features = []
        for idx, row in gdf.iterrows():
            features = self.extract_improved_features(
                row.geometry, 
                row, 
                geometry_index=idx,  # ä¼ å…¥ç´¢å¼•
                all_centroids=all_centroids,  # ä¼ å…¥æ‰€æœ‰è´¨å¿ƒ
                local_bounds=local_bounds,  # ä¼ å…¥å½“å‰åœ°å›¾è¾¹ç•Œæ¡†
                local_centroid=local_centroid,  # ä¼ å…¥å½“å‰åœ°å›¾è´¨å¿ƒ
                k_neighbors_info=k_neighbors_dict.get(idx),  # ä¼ å…¥é¢„è®¡ç®—çš„Kè¿‘é‚»
                total_nodes=total_nodes  # ä¼ å…¥æ€»èŠ‚ç‚¹æ•°ï¼ˆç¬¬20ç»´ç‰¹å¾ï¼‰â­æ–°å¢
            )
            node_features.append(features)
        
        node_features = np.array(node_features, dtype=np.float32)
        
        # ç¬¬ä¸‰æ­¥ï¼šæ„å»ºKNN+Delaunayå›¾ï¼ˆç”¨äºé‚»åŸŸç‰¹å¾è®¡ç®—ï¼‰
        print("\næ„å»ºå›¾ç”¨äºé‚»åŸŸç‰¹å¾è®¡ç®—...")
        graph_edges_merged, graph_stats = self.build_knn_delaunay_edges(geometries)
        
        # â­æå–çº¯Delaunayè¾¹ç”¨äºç‰¹å¾è®¡ç®—ï¼ˆæŠ—æ”»å‡»ï¼Œæ•°å€¼ç¨³å®šï¼‰
        delaunay_edges_for_features = graph_stats.get('delaunay_edges_list', [])
        if not delaunay_edges_for_features:
            # å¦‚æœæ²¡æœ‰Delaunayè¾¹ï¼ˆèŠ‚ç‚¹<3ï¼‰ï¼Œä½¿ç”¨åˆå¹¶åçš„è¾¹
            delaunay_edges_for_features = graph_edges_merged
        
        # ç¬¬å››æ­¥ï¼šæ›´æ–°Delaunayé‚»åŸŸç‰¹å¾ï¼ˆç»´åº¦15-17ï¼‰â­ä½¿ç”¨çº¯Delaunayè¾¹
        node_features = self.update_topology_neighborhood_features(
            node_features, 
            geometries, 
            delaunay_edges_for_features,  # â­æ”¹ç”¨Delaunayè¾¹
            local_bounds=local_bounds
        )
        print(f"âœ… å·²æ›´æ–°é‚»åŸŸç‰¹å¾ï¼ˆåŸºäº {len(delaunay_edges_for_features)} æ¡Delaunayè¾¹ï¼‰")
        
        # ç¬¬äº”æ­¥ï¼šæ ‡å‡†åŒ–ç‰¹å¾
        if len(node_features) > 0:
            if use_global_scaler:
                # å¿…é¡»ä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–å™¨
                if not self.scaler_fitted:
                    raise RuntimeError(
                        f"âŒ å°è¯•ä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–å™¨ä½†æœªæ‹Ÿåˆï¼\n"
                        f"   å›¾å: {graph_name}\n"
                        f"   è¯·å…ˆè°ƒç”¨ first_pass_collect_features() æ‹Ÿåˆå…¨å±€æ ‡å‡†åŒ–å™¨"
                    )
                node_features = self.global_scaler.transform(node_features)
            else:
                # ä¸´æ—¶æ ‡å‡†åŒ–ï¼ˆä»…ç”¨äºç¬¬ä¸€éæ”¶é›†ç»Ÿè®¡é‡ï¼‰
                scaler = StandardScaler()
                node_features = scaler.fit_transform(node_features)
        
        # ç¬¬å…­æ­¥ï¼šæ„å»º KNN + Delaunay ç»Ÿä¸€å›¾
        graph_info = self.build_knn_delaunay_graph(geometries, node_features)
        
        # æå–è¾¹ä¿¡æ¯
        edges = graph_info['edges']
        
        if len(edges) > 0:
            edge_index = torch.tensor(edges, dtype=torch.long).T
            # âœ… è½¬æ¢ä¸ºæ— å‘å›¾ï¼ˆè‡ªåŠ¨æ·»åŠ åå‘è¾¹ï¼‰
            edge_index = to_undirected(edge_index)
        else:
            # å¦‚æœæ²¡æœ‰è¾¹ï¼ˆæç«¯æƒ…å†µï¼šåªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼‰ï¼Œåˆ›å»ºç©ºè¾¹ç´¢å¼•
            edge_index = torch.empty((2, 0), dtype=torch.long)
        
        # åˆ›å»ºPyTorch Geometric Dataå¯¹è±¡
        data = Data(
            x=torch.tensor(node_features, dtype=torch.float32),
            edge_index=edge_index
        )
        
        # ä¿å­˜ç®€åŒ–çš„ç»Ÿè®¡ä¿¡æ¯ï¼ˆåªä¿å­˜æ•°å€¼ï¼Œä¸ä¿å­˜å­—å…¸å¼•ç”¨ï¼‰
        data.n_nodes = graph_info['stats']['total_nodes']
        data.n_edges = graph_info['stats']['total_edges']
        data.n_isolated_nodes = graph_info['stats']['isolated_nodes']
        
        return data
    
    def save_graph_data(self, data, filename, subdir=None, data_type='Original'):
        """ä¿å­˜å›¾æ•°æ®ï¼Œå¦‚æœæ–‡ä»¶å­˜åœ¨åˆ™è¦†ç›–"""
        if data_type == 'Original':
            save_dir = os.path.join(self.graph_dir, 'Original')
        else:
            save_dir = os.path.join(self.graph_dir, 'Attacked', subdir if subdir else '')
        
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        
        save_path = os.path.join(save_dir, f"{filename}_graph.pkl")
        with open(save_path, 'wb') as f:
            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)
        
        print(f"å·²ä¿å­˜: {save_path}")
        
    def check_graph_exists(self, filename, subdir=None, data_type='Original'):
        """æ£€æŸ¥å›¾æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨"""
        if data_type == 'Original':
            save_dir = os.path.join(self.graph_dir, 'Original')
        else:
            save_dir = os.path.join(self.graph_dir, 'Attacked', subdir if subdir else '')
        
        save_path = os.path.join(save_dir, f"{filename}_graph.pkl")
        return os.path.exists(save_path)
    
    def first_pass_collect_features(self):
        """
        ä¼˜åŒ–ç‰ˆç¬¬ä¸€éï¼šæµå¼å¤„ç†æ”¶é›†ç‰¹å¾ï¼ˆç‹¬ç«‹å…¨å±€ï¼‰
        â­ ä»…ç”¨åŸå§‹å›¾ç”Ÿæˆ global_scalerï¼ˆæ–¹æ¡ˆAï¼‰
        ä½¿ç”¨ç¼“å­˜åŠ é€Ÿï¼Œæ¯ä¸ªåœ°å›¾ç‹¬ç«‹è®¡ç®—è¾¹ç•Œæ¡†
        """
        print("\n=== ğŸš€ ä¼˜åŒ–ç‰ˆç¬¬ä¸€éï¼šæ”¶é›†åŸå§‹å›¾ç‰¹å¾ï¼ˆä»…åŸå§‹å›¾ç”Ÿæˆscalerï¼‰ ===")
        print("ã€ç­–ç•¥ã€‘ä»…ç”¨åŸå§‹å›¾ç”Ÿæˆ global_scaler.pklï¼Œæ”»å‡»å›¾ä½¿ç”¨æ­¤scaleræ ‡å‡†åŒ–")
        print("ã€ä¼˜åŠ¿ã€‘é€Ÿåº¦å¿«ã€ç¬¦åˆé›¶æ°´å°é€»è¾‘ã€ç‰¹å¾ç©ºé—´ä»¥åŸå§‹å›¾ä¸ºåŸºå‡†\n")
        
        start_time = time.time()
        self.monitor_system_resources()
        
        # â­ ä»…è·å–åŸå§‹å›¾è·¯å¾„
        file_paths = []
        if os.path.exists(self.vector_dir):
            for filename in os.listdir(self.vector_dir):
                if filename.endswith('.geojson') and not filename.startswith('._'):
                    file_paths.append(('original', os.path.join(self.vector_dir, filename)))
        
        print(f"å‘ç° {len(file_paths)} ä¸ªåŸå§‹å›¾æ–‡ä»¶ï¼ˆä»…ç”¨äºç”Ÿæˆscalerï¼‰")
        
        if not file_paths:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„è®¾ç½®")
            return
        
        # æ£€æŸ¥ç¼“å­˜
        all_features = None
        if self.use_cache:
            all_features = self.try_load_cached_features(file_paths)
        
        if all_features is None:
            print("ğŸ’¾ ç¼“å­˜æ— æ•ˆæˆ–æœªå¯ç”¨ï¼Œå¼€å§‹é‡æ–°è®¡ç®—ç‰¹å¾...")
            
            # ç›´æ¥æå–ç‰¹å¾ï¼ˆä½¿ç”¨ç‹¬ç«‹å…¨å±€ï¼Œæ¯ä¸ªåœ°å›¾è‡ªå·±è®¡ç®—è¾¹ç•Œæ¡†ï¼‰
            print("ğŸ”§ æå–ç‰¹å¾ï¼ˆä½¿ç”¨ç‹¬ç«‹å…¨å±€å½’ä¸€åŒ–ï¼‰...")
            all_features = self.process_files_with_local_bounds(file_paths)
            
            if self.use_cache and all_features is not None:
                self.save_features_cache(all_features, file_paths)
        
        if all_features is None or len(all_features) == 0:
            raise RuntimeError("âŒ ç‰¹å¾æå–å¤±è´¥ï¼šæ— æ³•æ”¶é›†åˆ°æœ‰æ•ˆç‰¹å¾ï¼Œå¯èƒ½æ˜¯å†…å­˜ä¸è¶³æˆ–æ–‡ä»¶æŸå")
        
        all_features = np.array(all_features, dtype=np.float32)
        print(f"âœ… å…±æå– {len(all_features)} ä¸ªèŠ‚ç‚¹çš„ç‰¹å¾ï¼Œç‰¹å¾ç»´åº¦: {all_features.shape[1]}")
        
        # æ‹Ÿåˆå…¨å±€æ ‡å‡†åŒ–å™¨
        print("ğŸ”§ æ‹Ÿåˆå…¨å±€æ ‡å‡†åŒ–å™¨ï¼ˆä»…åŸºäºåŸå§‹å›¾ï¼‰...")
        self.global_scaler.fit(all_features)
        self.scaler_fitted = True
        
        # ä¿å­˜æ ‡å‡†åŒ–å™¨
        scaler_path = os.path.join(self.graph_dir, 'global_scaler.pkl')
        with open(scaler_path, 'wb') as f:
            pickle.dump({
                'scaler': self.global_scaler,
                'global_bounds': self.global_bounds,
                'global_centroid': self.global_centroid,
                'strategy': 'original_only'  # â­ æ ‡è®°ä½¿ç”¨æ–¹æ¡ˆA
            }, f)
        print(f"âœ… å·²ä¿å­˜å…¨å±€æ ‡å‡†åŒ–å™¨: {scaler_path}")
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        elapsed_time = time.time() - start_time
        print(f"\n" + "="*70)
        print(f"ğŸ“Š ç¬¬ä¸€éå®Œæˆï¼šä»…åŸå§‹å›¾ç‰¹å¾æ”¶é›†")
        print(f"="*70)
        print(f"â±ï¸  è€—æ—¶: {elapsed_time:.2f}ç§’ ({elapsed_time/60:.1f}åˆ†é’Ÿ)")
        print(f"ğŸ“Š åŸå§‹å›¾æ•°é‡: {len(file_paths)} ä¸ª")
        print(f"ğŸ“Š èŠ‚ç‚¹æ€»æ•°: {len(all_features)} ä¸ª")
        print(f"ğŸ“ˆ å¤„ç†é€Ÿåº¦: {len(all_features)/elapsed_time:.1f} ç‰¹å¾/ç§’")
        print(f"ğŸ¯ ç‰¹å¾ç»´åº¦: {all_features.shape[1]}")
        print(f"âœ… æ”»å‡»å›¾å°†ä½¿ç”¨æ­¤scaleræ ‡å‡†åŒ–ï¼ˆæ— éœ€é‡æ–°æ‰«æï¼‰")
        print(f"="*70)
        
        # æ¸…ç†å†…å­˜
        del all_features
        gc.collect()
        self.monitor_system_resources()
    
    def try_load_cached_features(self, file_paths):
        """å°è¯•åŠ è½½ç¼“å­˜çš„ç‰¹å¾æ•°æ®"""
        if not os.path.exists(self.features_cache_file):
            return None
        
        print("ğŸ” æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæ€§...")
        
        # æ”¹è¿›çš„ç¼“å­˜éªŒè¯ç­–ç•¥ï¼š
        # 1. æ£€æŸ¥ç¼“å­˜æ–‡ä»¶çš„ä¿®æ”¹æ—¶é—´
        # 2. é‡‡æ ·æ£€æŸ¥æ–‡ä»¶å“ˆå¸Œï¼ˆæ™ºèƒ½é‡‡æ ·ï¼‰
        # 3. æ£€æŸ¥æ–‡ä»¶æ€»æ•°æ˜¯å¦ä¸€è‡´
        
        cached_hashes = self.load_file_hashes()
        
        # æ£€æŸ¥æ–‡ä»¶æ€»æ•°æ˜¯å¦ä¸€è‡´
        if len(cached_hashes) != len(file_paths):
            print(f"ğŸ“ æ–‡ä»¶æ•°é‡å˜åŒ–: ç¼“å­˜ {len(cached_hashes)} vs å½“å‰ {len(file_paths)}")
            return None
        
        # æ™ºèƒ½é‡‡æ ·æ£€æŸ¥ï¼šæ£€æŸ¥æœ€æ–°ä¿®æ”¹çš„æ–‡ä»¶å’Œéšæœºé‡‡æ ·
        import random
        
        # æŒ‰æ–‡ä»¶ä¿®æ”¹æ—¶é—´æ’åºï¼Œæ£€æŸ¥æœ€æ–°çš„æ–‡ä»¶
        file_paths_with_mtime = []
        for file_type, file_path in file_paths:
            try:
                mtime = os.path.getmtime(file_path)
                file_paths_with_mtime.append((file_type, file_path, mtime))
            except:
                continue
        
        file_paths_with_mtime.sort(key=lambda x: x[2], reverse=True)  # æŒ‰ä¿®æ”¹æ—¶é—´é™åº
        
        # æ£€æŸ¥æœ€æ–°çš„20ä¸ªæ–‡ä»¶ + éšæœºé‡‡æ ·30ä¸ªæ–‡ä»¶
        recent_files = file_paths_with_mtime[:20]
        remaining_files = file_paths_with_mtime[20:]
        random_sample = random.sample(remaining_files, min(30, len(remaining_files)))
        
        files_to_check = recent_files + random_sample
        
        for file_type, file_path, mtime in files_to_check:
            current_hash = self.get_file_hash(file_path)
            if current_hash is None:
                continue
                
            if file_path not in cached_hashes or cached_hashes[file_path] != current_hash:
                print(f"ğŸ“ æ–‡ä»¶å·²å˜æ›´: {os.path.basename(file_path)}")
                return None
        
        try:
            print("âœ… ç¼“å­˜éªŒè¯é€šè¿‡ï¼ŒåŠ è½½ç‰¹å¾æ•°æ®...")
            with open(self.features_cache_file, 'rb') as f:
                cached_data = pickle.load(f)
            
            # æ£€æŸ¥ç¼“å­˜ç‰ˆæœ¬
            cache_version = cached_data.get('version', '0.0')
            current_version = '1.3'  # ç‰ˆæœ¬1.3ï¼šæ–¹æ¡ˆAï¼ˆä»…åŸå§‹å›¾ç”Ÿæˆscalerï¼‰+ clipä¼˜åŒ–
            if cache_version != current_version:
                print(f"ğŸ“ ç¼“å­˜ç‰ˆæœ¬ä¸åŒ¹é…: {cache_version} vs {current_version}")
                return None
                
            self.global_bounds = cached_data.get('global_bounds')
            self.global_centroid = cached_data.get('global_centroid')
            
            features = cached_data.get('features', [])
            print(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½ {len(features)} ä¸ªç‰¹å¾")
            print(f"ğŸ” é‡‡æ ·æ£€æŸ¥äº† {len(files_to_check)} ä¸ªæ–‡ä»¶çš„å®Œæ•´æ€§")
            return features
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
            return None
    
    def save_features_cache(self, features, file_paths):
        """ä¿å­˜ç‰¹å¾æ•°æ®åˆ°ç¼“å­˜"""
        try:
            print("ğŸ’¾ ä¿å­˜ç‰¹å¾ç¼“å­˜...")
            
            # è®¡ç®—å¹¶ä¿å­˜æ–‡ä»¶å“ˆå¸Œ
            file_hashes = {}
            for file_type, file_path in file_paths:
                file_hash = self.get_file_hash(file_path)
                if file_hash:
                    file_hashes[file_path] = file_hash
            
            # ä¿å­˜ç¼“å­˜æ•°æ®
            cache_data = {
                'features': features,
                'global_bounds': self.global_bounds,
                'global_centroid': self.global_centroid,
                'version': '1.3',  # ç‰ˆæœ¬1.3ï¼šæ–¹æ¡ˆAï¼ˆä»…åŸå§‹å›¾ç”Ÿæˆscalerï¼‰+ clipä¼˜åŒ–
                'strategy': 'original_only'  # â­ æ ‡è®°ä½¿ç”¨æ–¹æ¡ˆA
            }
            
            with open(self.features_cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            self.save_file_hashes(file_hashes)
            print("âœ… ç¼“å­˜ä¿å­˜å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥: {e}")
    
    def process_files_optimized(self, file_paths):
        """ä¼˜åŒ–çš„æ–‡ä»¶å¤„ç†æ–¹æ³•"""
        all_features = []
        all_geometries = []
        
        # åˆ†æ‰¹å¤„ç†æ–‡ä»¶
        batches = [file_paths[i:i + self.batch_size] for i in range(0, len(file_paths), self.batch_size)]
        print(f"ğŸ”„ åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {self.batch_size} ä¸ªæ–‡ä»¶")
        
        # ä¸²è¡Œå¤„ç†ï¼ˆé¿å…å¤šè¿›ç¨‹çš„å¤æ‚æ€§ï¼Œä½†ä¿ç•™æ‰¹æ¬¡å¤„ç†çš„å†…å­˜ä¼˜åŒ–ï¼‰
        for batch_idx, batch in enumerate(batches):
            if not self.monitor_system_resources():
                print("âš ï¸  å†…å­˜ä¸è¶³ï¼Œåœæ­¢å¤„ç†")
                break
                
            print(f"ğŸ“‚ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}")
            
            batch_features, batch_geometries = self.process_file_batch_serial(batch)
            all_features.extend(batch_features)
            all_geometries.extend(batch_geometries)
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if batch_idx % 10 == 0:
                gc.collect()
        
        # å…¨å±€ç»Ÿè®¡é‡å·²åœ¨ç¬¬ä¸€é˜¶æ®µè®¡ç®—ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤è®¡ç®—
        return all_features
    
    def process_file_batch_serial(self, file_batch):
        """ä¸²è¡Œå¤„ç†ä¸€æ‰¹æ–‡ä»¶"""
        batch_features = []
        batch_geometries = []
        
        for file_type, file_path in tqdm(file_batch, desc="å¤„ç†æ–‡ä»¶", leave=False):
            try:
                gdf = gpd.read_file(file_path)
                
                for idx, row in gdf.iterrows():
                    try:
                        features = self.extract_improved_features(row.geometry, row)
                        batch_features.append(features)
                        
                        # æ”¶é›†å‡ ä½•ä¿¡æ¯ç”¨äºå…¨å±€ç»Ÿè®¡
                        batch_geometries.append({
                            'bounds': row.geometry.bounds,
                            'centroid': (row.geometry.centroid.x, row.geometry.centroid.y)
                        })
                    except Exception as e:
                        print(f"âš ï¸  ç‰¹å¾æå–å¤±è´¥: {e}")
                        continue
                        
            except Exception as e:
                print(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ {os.path.basename(file_path)}: {e}")
                continue
        
        return batch_features, batch_geometries
    
    def precompute_k_neighbors(self, all_centroids, k=5):
        """
        ä½¿ç”¨ KD-tree é¢„è®¡ç®—æ‰€æœ‰èŠ‚ç‚¹çš„ Kè¿‘é‚»ï¼ˆO(n log n) è€Œä¸æ˜¯ O(nÂ²)ï¼‰
        
        Args:
            all_centroids: æ‰€æœ‰èŠ‚ç‚¹çš„è´¨å¿ƒåˆ—è¡¨ï¼ˆPointå¯¹è±¡ï¼‰
            k: è¿‘é‚»æ•°é‡
            
        Returns:
            dict: {èŠ‚ç‚¹ç´¢å¼•: {'indices': [...], 'distances': [...], 'centroids': [...]}}
        """
        from sklearn.neighbors import NearestNeighbors
        
        n = len(all_centroids)
        if n < 2:
            return {}
        
        # è½¬æ¢ä¸º numpy æ•°ç»„
        centroids_array = np.array([[c.x, c.y] for c in all_centroids])
        
        # æ„å»º KD-tree å¹¶æŸ¥è¯¢
        actual_k = min(k, n - 1)
        nbrs = NearestNeighbors(
            n_neighbors=actual_k + 1,  # +1 å› ä¸ºåŒ…æ‹¬è‡ªå·±
            algorithm='kd_tree'
        ).fit(centroids_array)
        
        distances, indices = nbrs.kneighbors(centroids_array)
        
        # æ„å»ºç»“æœå­—å…¸
        result = {}
        for i in range(n):
            # æ’é™¤è‡ªå·±ï¼ˆç¬¬ä¸€ä¸ªæ˜¯è‡ªå·±ï¼Œè·ç¦»ä¸º0ï¼‰
            neighbor_indices = indices[i][1:actual_k+1]
            neighbor_distances = distances[i][1:actual_k+1]
            neighbor_centroids = [all_centroids[j] for j in neighbor_indices]
            
            result[i] = {
                'indices': neighbor_indices,
                'distances': neighbor_distances,
                'centroids': neighbor_centroids
            }
        
        return result
    
    def process_files_with_local_bounds(self, file_paths):
        """ä½¿ç”¨ç‹¬ç«‹å…¨å±€å¤„ç†æ–‡ä»¶ï¼ˆæ¯ä¸ªåœ°å›¾è‡ªå·±è®¡ç®—è¾¹ç•Œæ¡†ï¼‰"""
        all_features = []
        
        # åˆ†æ‰¹å¤„ç†æ–‡ä»¶
        batches = [file_paths[i:i + self.batch_size] for i in range(0, len(file_paths), self.batch_size)]
        print(f"ğŸ”„ åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹ {self.batch_size} ä¸ªæ–‡ä»¶")
        
        for batch_idx, batch in enumerate(batches):
            if not self.monitor_system_resources():
                print("âš ï¸  å†…å­˜ä¸è¶³ï¼Œåœæ­¢å¤„ç†")
                break
                
            print(f"ğŸ“‚ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}")
            
            for file_type, file_path in tqdm(batch, desc="å¤„ç†æ–‡ä»¶", leave=False):
                try:
                    gdf = gpd.read_file(file_path)
                    geometries = gdf.geometry.tolist()
                    
                    # è®¡ç®—å½“å‰åœ°å›¾çš„è¾¹ç•Œæ¡†å’Œè´¨å¿ƒ
                    all_bounds = [geom.bounds for geom in geometries]
                    local_bounds = (
                        min(b[0] for b in all_bounds),
                        min(b[1] for b in all_bounds),
                        max(b[2] for b in all_bounds),
                        max(b[3] for b in all_bounds)
                    )
                    
                    all_centroids = [Point(geom.centroid.x, geom.centroid.y) for geom in geometries]
                    local_centroid = Point(
                        np.mean([c.x for c in all_centroids]),
                        np.mean([c.y for c in all_centroids])
                    )
                    
                    # ä½¿ç”¨ KD-tree é¢„è®¡ç®— Kè¿‘é‚»ï¼ˆå¤§å¹…åŠ é€Ÿï¼‰
                    k_neighbors_dict = self.precompute_k_neighbors(all_centroids)
                    
                    # æå–ç‰¹å¾
                    total_nodes = len(geometries)  # æ€»èŠ‚ç‚¹æ•°
                    for idx, row in gdf.iterrows():
                        try:
                            features = self.extract_improved_features(
                                row.geometry, 
                                row,
                                geometry_index=idx,
                                all_centroids=all_centroids,
                                local_bounds=local_bounds,
                                local_centroid=local_centroid,
                                k_neighbors_info=k_neighbors_dict.get(idx),
                                total_nodes=total_nodes  # ä¼ å…¥æ€»èŠ‚ç‚¹æ•°ï¼ˆç¬¬20ç»´ç‰¹å¾ï¼‰â­æ–°å¢
                            )
                            all_features.append(features)
                        except Exception as e:
                            print(f"âš ï¸  ç‰¹å¾æå–å¤±è´¥: {e}")
                            continue
                            
                except Exception as e:
                    print(f"âŒ æ–‡ä»¶è¯»å–å¤±è´¥ {os.path.basename(file_path)}: {e}")
                    continue
            
            # å®šæœŸæ¸…ç†å†…å­˜
            if batch_idx % 10 == 0:
                gc.collect()
        
        return all_features
    
    def calculate_global_statistics_from_geometries(self, geometries):
        """ä»å‡ ä½•ä¿¡æ¯è®¡ç®—å…¨å±€ç»Ÿè®¡é‡"""
        print("ğŸ“Š è®¡ç®—å…¨å±€ç»Ÿè®¡é‡...")
        
        if not geometries:
            self.global_bounds = (0, 0, 1, 1)
            self.global_centroid = (0.5, 0.5)
            return
        
        # è®¡ç®—å…¨å±€è¾¹ç•Œæ¡†
        min_x = min(geom['bounds'][0] for geom in geometries)
        min_y = min(geom['bounds'][1] for geom in geometries)
        max_x = max(geom['bounds'][2] for geom in geometries)
        max_y = max(geom['bounds'][3] for geom in geometries)
        
        self.global_bounds = (min_x, min_y, max_x, max_y)
        
        # è®¡ç®—å…¨å±€è´¨å¿ƒ
        centroids_x = [geom['centroid'][0] for geom in geometries]
        centroids_y = [geom['centroid'][1] for geom in geometries]
        
        self.global_centroid = (
            np.mean(centroids_x),
            np.mean(centroids_y)
        )
        
        print(f"ğŸŒ å…¨å±€è¾¹ç•Œæ¡†: {self.global_bounds}")
        print(f"ğŸ¯ å…¨å±€è´¨å¿ƒ: {self.global_centroid}")

    def calculate_global_stats_from_files(self, file_paths):
        """
        å¿«é€Ÿæ‰«ææ‰€æœ‰æ–‡ä»¶è®¡ç®—å…¨å±€ç»Ÿè®¡é‡
        åªè¯»å–å‡ ä½•ä¿¡æ¯ï¼Œä¸æå–å®Œæ•´ç‰¹å¾
        """
        print("ğŸ“Š å¿«é€Ÿæ‰«æè®¡ç®—å…¨å±€ç»Ÿè®¡é‡...")
        
        all_bounds = []
        all_centroids = []
        
        for file_type, file_path in tqdm(file_paths, desc="æ‰«ææ–‡ä»¶"):
            try:
                gdf = gpd.read_file(file_path)
                
                for idx, row in gdf.iterrows():
                    try:
                        geom = row.geometry
                        all_bounds.append(geom.bounds)
                        all_centroids.append((geom.centroid.x, geom.centroid.y))
                    except Exception:
                        continue
                        
            except Exception as e:
                print(f"âš ï¸  æ‰«ææ–‡ä»¶å¤±è´¥ {os.path.basename(file_path)}: {e}")
                continue
        
        if not all_bounds:
            print("âš ï¸  æœªæ‰¾åˆ°æœ‰æ•ˆå‡ ä½•æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å…¨å±€ç»Ÿè®¡é‡")
            self.global_bounds = (0, 0, 1, 1)
            self.global_centroid = Point(0.5, 0.5)
            return
        
        # è®¡ç®—å…¨å±€è¾¹ç•Œæ¡†
        min_x = min(b[0] for b in all_bounds)
        min_y = min(b[1] for b in all_bounds)
        max_x = max(b[2] for b in all_bounds)
        max_y = max(b[3] for b in all_bounds)
        self.global_bounds = (min_x, min_y, max_x, max_y)
        
        # è®¡ç®—å…¨å±€è´¨å¿ƒ
        avg_x = np.mean([c[0] for c in all_centroids])
        avg_y = np.mean([c[1] for c in all_centroids])
        self.global_centroid = Point(avg_x, avg_y)
        
        print(f"ğŸŒ å…¨å±€è¾¹ç•Œæ¡†: {self.global_bounds}")
        print(f"ğŸ¯ å…¨å±€è´¨å¿ƒ: ({avg_x:.2f}, {avg_y:.2f})")
        print(f"ğŸ“ˆ æ‰«æäº† {len(all_centroids)} ä¸ªå‡ ä½•è¦ç´ ")
    
    def second_pass_convert_and_save(self, incremental_mode=True):
        """
        ç¬¬äºŒéï¼šä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–å™¨è½¬æ¢å¹¶ä¿å­˜å›¾æ•°æ®ï¼ˆåŸå§‹å›¾+æ”»å‡»å›¾ï¼‰
        
        Args:
            incremental_mode: æ˜¯å¦ä½¿ç”¨å¢é‡æ›´æ–°æ¨¡å¼
                - True: åªæ›´æ–°å˜åŒ–çš„æ–‡ä»¶ï¼ˆåŸºäºæ–‡ä»¶å“ˆå¸Œï¼‰
                - False: æ¸…ç©ºå¹¶é‡æ–°ç”Ÿæˆæ‰€æœ‰æ–‡ä»¶
        """
        print("\n=== ç¬¬äºŒéï¼šè½¬æ¢å¹¶ä¿å­˜å›¾æ•°æ®ï¼ˆåŸå§‹å›¾ + æ”»å‡»å›¾ï¼‰ ===")
        print("ã€ç­–ç•¥ã€‘æ‰€æœ‰å›¾ä½¿ç”¨ç¬¬ä¸€éç”Ÿæˆçš„ global_scaler.pkl æ ‡å‡†åŒ–")
        print(f"ã€æ¨¡å¼ã€‘{'ğŸ”„ å¢é‡æ›´æ–°æ¨¡å¼ï¼ˆåªæ›´æ–°å˜åŒ–çš„æ–‡ä»¶ï¼‰' if incremental_mode else 'ğŸ”¥ å®Œå…¨é‡æ–°ç”Ÿæˆæ¨¡å¼'}\n")
        
        if not self.scaler_fitted:
            raise ValueError("å¿…é¡»å…ˆè°ƒç”¨ first_pass_collect_features()")
        
        # æ€»ä½“æ—¶é—´ç»Ÿè®¡
        total_start_time = time.time()
        
        # åŠ è½½æ—§çš„å“ˆå¸Œè®°å½•ï¼ˆç”¨äºåˆ¤æ–­æ–‡ä»¶æ˜¯å¦å˜åŒ–ï¼‰
        old_hashes = self.load_file_hashes() if incremental_mode else {}
        new_hashes = {}
        
        # æ£€æŸ¥åŸå§‹å›¾æ˜¯å¦æœ‰å˜åŒ–
        if incremental_mode and old_hashes:
            original_changed = self.check_original_files_changed(old_hashes)
            if original_changed:
                print("âš ï¸  æ£€æµ‹åˆ°åŸå§‹å›¾æ–‡ä»¶æœ‰å˜åŒ–ï¼")
                print("âš ï¸  åŸå§‹å›¾å˜åŒ–ä¼šå½±å“ global_scaler.pklï¼Œéœ€è¦é‡æ–°ç”Ÿæˆæ‰€æœ‰å›¾")
                print("âš ï¸  åˆ‡æ¢åˆ°å®Œå…¨é‡æ–°ç”Ÿæˆæ¨¡å¼...\n")
                incremental_mode = False
                old_hashes = {}
        
        # æ ¹æ®æ¨¡å¼å†³å®šæ˜¯å¦æ¸…ç†è¾“å‡ºç›®å½•
        if not incremental_mode:
            print("\nğŸ§¹ æ¸…ç†æ—§å›¾æ•°æ®...")
            self.clean_output_dirs()
            print("âœ… æ—§æ•°æ®å·²æ¸…ç†\n")
        else:
            print("\nğŸ” å¢é‡æ›´æ–°æ¨¡å¼ï¼šåªæ›´æ–°å˜åŒ–çš„æ–‡ä»¶ï¼Œä¿ç•™æœªå˜åŒ–çš„æ–‡ä»¶\n")
        
        # å¤„ç†åŸå§‹æ•°æ®
        print("\n" + "="*70)
        print("ğŸ“‚ å¤„ç†åŸå§‹æ•°æ®ï¼ˆTrainingSet/Originalï¼‰")
        print("="*70)
        original_start_time = time.time()
        skipped_count = 0
        processed_count = 0
        
        for filename in os.listdir(self.vector_dir):
            if filename.endswith('.geojson') and not filename.startswith('._'):
                graph_name = filename.replace('.geojson', '')
                file_path = os.path.join(self.vector_dir, filename)
                output_path = self.get_graph_output_path(filename, data_type='Original')
                
                try:
                    # å¢é‡æ›´æ–°æ¨¡å¼ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                    if incremental_mode:
                        should_update, reason = self.should_update_file(file_path, old_hashes, output_path)
                        
                        if not should_update:
                            print(f"â­ï¸  è·³è¿‡ {filename} ({reason})")
                            skipped_count += 1
                            # è®°å½•å“ˆå¸Œï¼ˆå³ä½¿è·³è¿‡ä¹Ÿè¦è®°å½•ï¼‰
                            new_hashes[file_path] = self.get_file_hash(file_path)
                            continue
                        else:
                            print(f"ğŸ”„ æ›´æ–° {filename} ({reason})")
                    
                    # è¯»å–å¹¶å¤„ç†æ–‡ä»¶
                    gdf = gpd.read_file(file_path)
                    
                    # æ„å»ºå›¾ï¼ˆä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–å™¨ï¼‰
                    data = self.build_graph_from_gdf(gdf, filename, use_global_scaler=True)
                    
                    self.save_graph_data(data, graph_name, data_type='Original')
                    processed_count += 1
                    
                    # è®°å½•æ–‡ä»¶å“ˆå¸Œ
                    new_hashes[file_path] = self.get_file_hash(file_path)
                    
                except Exception as e:
                    print(f"âŒ å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
                    continue
        
        original_elapsed = time.time() - original_start_time
        print(f"\nâœ… åŸå§‹æ•°æ®å¤„ç†å®Œæˆ")
        print(f"   - å¤„ç†æ•°é‡: {processed_count} ä¸ª")
        if incremental_mode:
            print(f"   - è·³è¿‡æ•°é‡: {skipped_count} ä¸ª")
        print(f"   - è€—æ—¶: {original_elapsed:.2f}ç§’ ({original_elapsed/60:.1f}åˆ†é’Ÿ)")
        print(f"   - å¹³å‡é€Ÿåº¦: {original_elapsed/processed_count if processed_count > 0 else 0:.2f}ç§’/å›¾")
        
        # å¤„ç†æ”»å‡»æ•°æ®
        print("\n" + "="*70)
        print("ğŸ“‚ å¤„ç†æ”»å‡»æ•°æ®ï¼ˆTrainingSet/Attackedï¼‰")
        print("="*70)
        attacked_start_time = time.time()
        
        # ç»Ÿè®¡ä¿¡æ¯
        attack_type_stats = {}  # {attack_type: {'count': N, 'time': T, 'skipped': S}}
        
        for attacked_subdir in os.listdir(self.attacked_dir):
            attack_dir_path = os.path.join(self.attacked_dir, attacked_subdir)
            if os.path.isdir(attack_dir_path):
                # ç»Ÿè®¡å¤„ç†çš„æ–‡ä»¶æ•°
                subdir_start_time = time.time()
                subdir_processed = 0
                subdir_skipped = 0
                total_files = len([f for f in os.listdir(attack_dir_path) if f.endswith('.geojson') and not f.startswith('._')])
                
                print(f"\nğŸ“‚ [{attacked_subdir}] å…± {total_files} ä¸ªæ–‡ä»¶")
                
                # åœ¨å¢é‡æ¨¡å¼ä¸‹ï¼Œæ˜¾ç¤ºæ›´è¯¦ç»†çš„ä¿¡æ¯
                use_tqdm = not incremental_mode  # å¢é‡æ¨¡å¼ä¸‹ä¸ç”¨è¿›åº¦æ¡ï¼Œæ”¹ç”¨é€è¡Œè¾“å‡º
                
                file_list = os.listdir(attack_dir_path)
                iterator = tqdm(file_list, desc=f"  å¤„ç†ä¸­", leave=False) if use_tqdm else file_list
                
                for filename in iterator:
                    if filename.endswith('.geojson') and not filename.startswith('._'):
                        graph_name = filename.replace('.geojson', '')
                        file_path = os.path.join(attack_dir_path, filename)
                        output_path = self.get_graph_output_path(filename, attacked_subdir, 'Attacked')
                        
                        try:
                            # å¢é‡æ›´æ–°æ¨¡å¼ï¼šæ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
                            if incremental_mode:
                                should_update, reason = self.should_update_file(file_path, old_hashes, output_path)
                                
                                if not should_update:
                                    if subdir_skipped < 3:  # åªæ˜¾ç¤ºå‰å‡ ä¸ªè·³è¿‡çš„æ–‡ä»¶
                                        print(f"  â­ï¸  è·³è¿‡ {attacked_subdir}/{filename} ({reason})")
                                    subdir_skipped += 1
                                    # è®°å½•å“ˆå¸Œï¼ˆå³ä½¿è·³è¿‡ä¹Ÿè¦è®°å½•ï¼‰
                                    new_hashes[file_path] = self.get_file_hash(file_path)
                                    continue
                                else:
                                    print(f"  ğŸ”„ æ›´æ–° {attacked_subdir}/{filename} ({reason})")
                            
                            # è¯»å–å¹¶å¤„ç†æ–‡ä»¶
                            gdf = gpd.read_file(file_path)
                            
                            data = self.build_graph_from_gdf(gdf, filename, use_global_scaler=True)
                            self.save_graph_data(data, graph_name, attacked_subdir, 'Attacked')
                            subdir_processed += 1
                            
                            # è®°å½•æ–‡ä»¶å“ˆå¸Œ
                            new_hashes[file_path] = self.get_file_hash(file_path)
                            
                        except Exception as e:
                            print(f"    âŒ å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
                            continue
                
                subdir_elapsed = time.time() - subdir_start_time
                attack_type_stats[attacked_subdir] = {
                    'count': subdir_processed,
                    'time': subdir_elapsed,
                    'skipped': subdir_skipped
                }
                
                if incremental_mode and subdir_skipped > 3:
                    print(f"  ... è¿˜æœ‰ {subdir_skipped - 3} ä¸ªæ–‡ä»¶è¢«è·³è¿‡")
                
                print(f"  âœ… {attacked_subdir}: {subdir_processed} ä¸ªå¤„ç†" + 
                      (f", {subdir_skipped} ä¸ªè·³è¿‡" if incremental_mode else "") +
                      f"ï¼Œè€—æ—¶ {subdir_elapsed:.2f}ç§’ " +
                      f"(å¹³å‡ {subdir_elapsed/subdir_processed if subdir_processed > 0 else 0:.2f}ç§’/å›¾)")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        attacked_elapsed = time.time() - attacked_start_time
        total_attacked_count = sum(stats['count'] for stats in attack_type_stats.values())
        total_attacked_skipped = sum(stats.get('skipped', 0) for stats in attack_type_stats.values())
        total_elapsed = time.time() - total_start_time
        
        # ä¿å­˜å“ˆå¸Œè®°å½•ï¼ˆå¢é‡æ¨¡å¼ï¼‰
        if incremental_mode:
            print("\nğŸ’¾ ä¿å­˜æ–‡ä»¶å“ˆå¸Œè®°å½•...")
            self.save_file_hashes(new_hashes)
            print(f"âœ… å·²ä¿å­˜ {len(new_hashes)} ä¸ªæ–‡ä»¶çš„å“ˆå¸Œè®°å½•")
        
        print("\n" + "="*70)
        print("âœ… è®­ç»ƒé›†è½¬æ¢å®Œæˆï¼")
        print("="*70)
        
        # æ‰“å°è¯¦ç»†ç»Ÿè®¡
        print(f"\nğŸ“Š è½¬æ¢ç»Ÿè®¡æ±‡æ€»:")
        print(f"   åŸå§‹å›¾: {processed_count} ä¸ªå¤„ç†" + 
              (f", {skipped_count} ä¸ªè·³è¿‡" if incremental_mode else "") +
              f"ï¼Œè€—æ—¶ {original_elapsed:.2f}ç§’")
        print(f"   æ”»å‡»å›¾: {total_attacked_count} ä¸ªå¤„ç†" + 
              (f", {total_attacked_skipped} ä¸ªè·³è¿‡" if incremental_mode else "") +
              f"ï¼Œè€—æ—¶ {attacked_elapsed:.2f}ç§’")
        
        total_processed = processed_count + total_attacked_count
        total_skipped = skipped_count + total_attacked_skipped if incremental_mode else 0
        
        print(f"   æ€»è®¡: {total_processed} ä¸ªå›¾å¤„ç†" + 
              (f", {total_skipped} ä¸ªè·³è¿‡" if incremental_mode else "") +
              f"ï¼Œè€—æ—¶ {total_elapsed:.2f}ç§’ ({total_elapsed/60:.1f}åˆ†é’Ÿ)")
        print(f"   å¹³å‡é€Ÿåº¦: {total_elapsed/total_processed if total_processed > 0 else 0:.2f}ç§’/å›¾")
        
        if incremental_mode:
            print(f"\nâš¡ å¢é‡æ›´æ–°æ•ˆæœ:")
            print(f"   - è·³è¿‡ç‡: {total_skipped / (total_processed + total_skipped) * 100 if (total_processed + total_skipped) > 0 else 0:.1f}%")
            print(f"   - èŠ‚çœæ—¶é—´: çº¦ {total_skipped * (total_elapsed/total_processed if total_processed > 0 else 0) / 60:.1f}åˆ†é’Ÿ")
        
        print(f"\nğŸ“‚ æ”»å‡»ç±»å‹ç»Ÿè®¡ (Top 10 æœ€æ…¢):")
        # æŒ‰æ—¶é—´æ’åº
        sorted_attacks = sorted(attack_type_stats.items(), key=lambda x: x[1]['time'], reverse=True)
        for i, (attack_type, stats) in enumerate(sorted_attacks[:10], 1):
            avg_time = stats['time'] / stats['count'] if stats['count'] > 0 else 0
            skip_info = f" (è·³è¿‡{stats.get('skipped', 0)})" if incremental_mode and stats.get('skipped', 0) > 0 else ""
            print(f"   {i:2d}. {attack_type:40s} {stats['count']:4d}å›¾{skip_info} {stats['time']:7.2f}ç§’ (å¹³å‡{avg_time:.2f}ç§’/å›¾)")
        
        print("="*70)
    
    def convert_train_set_to_graph(self, incremental_mode=True):
        """
        å®Œæ•´çš„ä¸¤éè½¬æ¢æµç¨‹ï¼ˆKNN + Delaunay ç»Ÿä¸€å›¾æ„å»º + æ–¹æ¡ˆAä¼˜åŒ– + å¢é‡æ›´æ–°ï¼‰ï¼š
        1. ç¬¬ä¸€éï¼šâ­ä»…æ”¶é›†åŸå§‹å›¾ç‰¹å¾ï¼Œæ‹Ÿåˆå…¨å±€æ ‡å‡†åŒ–å™¨ï¼ˆæ–¹æ¡ˆAï¼‰
        2. ç¬¬äºŒéï¼šåŸå§‹å›¾+æ”»å‡»å›¾éƒ½ä½¿ç”¨æ­¤æ ‡å‡†åŒ–å™¨è½¬æ¢å¹¶ä¿å­˜
        
        Args:
            incremental_mode: æ˜¯å¦ä½¿ç”¨å¢é‡æ›´æ–°æ¨¡å¼ï¼ˆé»˜è®¤Trueï¼‰
                - True: åªæ›´æ–°å˜åŒ–çš„GeoJSONæ–‡ä»¶ï¼ˆåŸºäºMD5å“ˆå¸Œï¼‰
                - False: æ¸…ç©ºå¹¶é‡æ–°ç”Ÿæˆæ‰€æœ‰å›¾æ–‡ä»¶
        
        ã€æ–¹æ¡ˆAæ ¸å¿ƒä¼˜åŠ¿ã€‘ï¼š
        - âš¡ é€Ÿåº¦æå¿«ï¼šç¬¬ä¸€éåªéœ€5åˆ†é’Ÿï¼ˆvs æ–¹æ¡ˆBçš„6å°æ—¶ï¼‰
        - ğŸ¯ ç¬¦åˆé›¶æ°´å°é€»è¾‘ï¼šåŸå§‹å›¾æ˜¯åŸºå‡†ï¼Œæ”»å‡»å›¾å‘åŸå§‹å›¾å¯¹é½
        - ğŸ›¡ï¸ ç‰¹å¾ç©ºé—´çº¯å‡€ï¼šä¸è¢«æç«¯æ”»å‡»"æ±¡æŸ“"scaler
        - ğŸ“Š ç†è®ºæ­£ç¡®ï¼šæ”»å‡»å›¾æ˜¯åŸå§‹å›¾çš„å‡ ä½•å˜æ¢
        - âœ… æ·»åŠ clipé˜²æŠ¤ï¼šç‰¹å¾å€¼é™åˆ¶åœ¨[-10, 10]èŒƒå›´å†…
        
        ã€å¢é‡æ›´æ–°ä¼˜åŠ¿ã€‘â­æ–°å¢ï¼š
        - ğŸ”„ æ™ºèƒ½æ£€æµ‹ï¼šåŸºäºæ–‡ä»¶å“ˆå¸Œåˆ¤æ–­GeoJSONæ˜¯å¦å˜åŒ–
        - âš¡ å¤§å¹…æé€Ÿï¼šåªå¤„ç†å˜åŒ–çš„æ–‡ä»¶ï¼Œè·³è¿‡æœªå˜åŒ–çš„æ–‡ä»¶
        - ğŸ›¡ï¸ å®‰å…¨æœºåˆ¶ï¼šåŸå§‹å›¾å˜åŒ–æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°å®Œå…¨é‡æ–°ç”Ÿæˆæ¨¡å¼
        - ğŸ“Š è¯¦ç»†ç»Ÿè®¡ï¼šæ˜¾ç¤ºè·³è¿‡ç‡å’ŒèŠ‚çœæ—¶é—´
        
        ã€å…¶ä»–ä¼˜åŒ–ã€‘ï¼š
        - æ¯ä¸ªåœ°å›¾ä½¿ç”¨è‡ªå·±çš„è¾¹ç•Œæ¡†å½’ä¸€åŒ–ä½ç½®ç‰¹å¾
        - è‡ªé€‚åº”Kå€¼ï¼šKæœ€å¤§ä¸º8ï¼ˆèŠ‚ç‚¹æ•°<50æ—¶K=5ï¼‰
        - èŠ‚ç‚¹æ•°ç¼–ç ï¼šè¡¥å¿è‡ªé€‚åº”å½’ä¸€åŒ–åçš„å›¾è§„æ¨¡ä¿¡æ¯
        """
        print("\n" + "="*70)
        print("KNN + Delaunay ç»Ÿä¸€å›¾è®­ç»ƒé›†è½¬æ¢ï¼ˆæ–¹æ¡ˆAï¼šä»…åŸå§‹å›¾ç”Ÿæˆscalerï¼‰")
        print("="*70)
        print("ã€æ ¸å¿ƒç‰¹æ€§ã€‘")
        print("  1. ç‰¹å¾ï¼š20ç»´å‡ ä½•ä¸å˜ç‰¹å¾ + clipé˜²æŠ¤ï¼ˆ-10~10ï¼‰")
        print("  2. â­ æ ‡å‡†åŒ–ç­–ç•¥ï¼šæ–¹æ¡ˆAï¼ˆä»…ç”¨åŸå§‹å›¾ç”Ÿæˆscalerï¼‰")
        print("     - ç¬¬ä¸€éï¼šä»…åŸå§‹å›¾ â†’ ç”Ÿæˆ global_scaler.pkl")
        print("     - ç¬¬äºŒéï¼šæ‰€æœ‰å›¾ä½¿ç”¨æ­¤scaleræ ‡å‡†åŒ–")
        print("     - ä¼˜åŠ¿ï¼šé€Ÿåº¦å¿«ã€ç¬¦åˆé›¶æ°´å°é€»è¾‘ã€ç‰¹å¾ç©ºé—´çº¯å‡€")
        print("  3. â­ å¢é‡æ›´æ–°ï¼š" + ("å¯ç”¨ ğŸ”„" if incremental_mode else "ç¦ç”¨ ğŸ”¥"))
        print("     - æ™ºèƒ½æ£€æµ‹ï¼šåŸºäºæ–‡ä»¶å“ˆå¸Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°")
        print("     - åŸå§‹å›¾å˜åŒ–ï¼šè‡ªåŠ¨å®Œå…¨é‡æ–°ç”Ÿæˆ")
        print("     - æ”»å‡»å›¾å˜åŒ–ï¼šä»…æ›´æ–°å˜åŒ–çš„æ–‡ä»¶")
        print("  4. ä½ç½®å½’ä¸€åŒ–ï¼šç‹¬ç«‹å…¨å±€ï¼ˆæ¯ä¸ªåœ°å›¾ç‹¬ç«‹å½’ä¸€åŒ–ï¼‰")
        print("  5. å›¾æ„å»ºï¼šKNN + Delaunay ç»Ÿä¸€å›¾")
        print("     - è‡ªé€‚åº”Kå€¼ï¼šKæœ€å¤§ä¸º8ï¼ˆèŠ‚ç‚¹æ•°<50æ—¶K=5ï¼‰")
        print("     - KNNä¿è¯å±€éƒ¨å¯†é›†ï¼šæ¯ä¸ªèŠ‚ç‚¹è‡³å¤š8ä¸ªé‚»å±…")
        print("     - Delaunayä¿è¯å…¨å±€è¿é€šï¼šè¦†ç›–æ‰€æœ‰èŠ‚ç‚¹")
        print("     - é€‚ç”¨æ‰€æœ‰æ•°æ®ç±»å‹ï¼šç‚¹/çº¿/é¢ï¼Œæ— å­¤å²›èŠ‚ç‚¹")
        print("  6. é²æ£’æ€§ï¼šå¯¹è£å‰ªã€åˆ é™¤å¯¹è±¡æ”»å‡»æå¼ºé²æ£’æ€§")
        print("="*70 + "\n")
        
        # ç¬¬ä¸€éï¼šæ”¶é›†ç‰¹å¾å’Œç»Ÿè®¡é‡
        self.first_pass_collect_features()
        
        # âœ… éªŒè¯scaleræ˜¯å¦å·²æ‹Ÿåˆ
        if not self.scaler_fitted:
            raise RuntimeError("âŒ ç¬¬ä¸€éç‰¹å¾æ”¶é›†å¤±è´¥ï¼šå…¨å±€æ ‡å‡†åŒ–å™¨æœªæ‹Ÿåˆï¼Œæ— æ³•ç»§ç»­ç¬¬äºŒéå¤„ç†")
        
        # ç¬¬äºŒéï¼šè½¬æ¢å¹¶ä¿å­˜ï¼ˆä¼ é€’å¢é‡æ¨¡å¼å‚æ•°ï¼‰
        self.second_pass_convert_and_save(incremental_mode=incremental_mode)
        
        print("\n" + "="*70)
        print("è®­ç»ƒé›†å›¾ç»“æ„è½¬æ¢å®Œæˆï¼")
        print("="*70)
        print(f"è¾“å‡ºç›®å½•: {self.graph_dir}")
        print("\nç”Ÿæˆæ–‡ä»¶:")
        print("  - Original/: åŸå§‹å›¾æ•°æ®ï¼ˆKNN + Delaunayç»Ÿä¸€å›¾ï¼‰")
        print("  - Attacked/: æ”»å‡»å›¾æ•°æ®ï¼ˆKNN + Delaunayç»Ÿä¸€å›¾ï¼‰")
        print("  - global_scaler.pkl: å…¨å±€æ ‡å‡†åŒ–å™¨ï¼ˆä¾›æµ‹è¯•é›†ä½¿ç”¨ï¼‰")
        print("\nå›¾ç»“æ„ç‰¹ç‚¹:")
        print("  - æ— å­¤å²›èŠ‚ç‚¹ï¼ˆDelaunayä¿è¯å…¨å±€è¿é€šï¼‰")
        print("  - å±€éƒ¨å¯†é›†ï¼ˆKNNä¿è¯å±€éƒ¨è¿æ¥ï¼ŒKâ‰¤8ï¼‰")
        print("  - è‡ªé€‚åº”Kå€¼ï¼ˆKæœ€å¤§ä¸º8ï¼ŒèŠ‚ç‚¹æ•°<50æ—¶K=5ï¼‰")
        print("  - èšç±»ä¿¡æ¯ä¿ç•™ï¼ˆä½œä¸ºèŠ‚ç‚¹å±æ€§ï¼‰")
        print("  - é€‚ç”¨æ‰€æœ‰æ•°æ®ç±»å‹ï¼ˆç‚¹/çº¿/é¢ç»Ÿä¸€æ–¹æ¡ˆï¼‰")
        print("  - å¯¹å‡ ä½•å˜æ¢å’Œå†…å®¹ç ´åæ”»å‡»é«˜åº¦é²æ£’")
        print("="*70 + "\n")

def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "="*70)
    print("    KNN + Delaunay ç»Ÿä¸€å›¾ - è®­ç»ƒé›†å›¾ç»“æ„è½¬æ¢ï¼ˆè‡ªé€‚åº”ç‰ˆï¼‰")
    print("="*70 + "\n")
    
    # æ£€æŸ¥å¿…éœ€ä¾èµ–
    missing_deps = []
    try:
        from scipy.spatial import Delaunay
        print("âœ… å·²æ£€æµ‹åˆ° scipy.spatial.Delaunay")
    except ImportError:
        missing_deps.append("scipy")
    
    try:
        from sklearn.neighbors import NearestNeighbors
        print("âœ… å·²æ£€æµ‹åˆ° sklearn.neighbors.NearestNeighbors")
    except ImportError:
        missing_deps.append("scikit-learn")
    
    if missing_deps:
        print("\n" + "="*70)
        print("é”™è¯¯ï¼šç¼ºå°‘å¿…éœ€ä¾èµ–")
        print("="*70)
        print(f"\nç¼ºå°‘çš„åº“: {', '.join(missing_deps)}\n")
        print("è¯·å®‰è£…ç¼ºå°‘çš„ä¾èµ–ï¼š")
        print("  pip install scipy scikit-learn\n")
        print("æˆ–è€…ä½¿ç”¨condaï¼š")
        print("  conda install scipy scikit-learn\n")
        print("å®‰è£…åé‡æ–°è¿è¡Œæ­¤è„šæœ¬ã€‚")
        print("="*70 + "\n")
        return
    
    print()
    
    # åˆ›å»ºä¼˜åŒ–ç‰ˆè½¬æ¢å™¨
    converter = ImprovedTrainSetVectorToGraphConverter(
        batch_size=25,      # â­é™ä½ä¸º25ï¼ˆåŸ50ï¼‰ï¼Œå‡å°‘å†…å­˜å ç”¨
        max_workers=4,      # å¯æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´ (é»˜è®¤8)
        use_cache=True      # å¯ç”¨ç¼“å­˜åŠ é€Ÿ
    )
    
    # â­ å¢é‡æ›´æ–°æ¨¡å¼ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    # å¦‚éœ€å®Œå…¨é‡æ–°ç”Ÿæˆï¼Œè®¾ç½® incremental_mode=False
    incremental_mode = True  # True: å¢é‡æ›´æ–° | False: å®Œå…¨é‡æ–°ç”Ÿæˆ
    
    print(f"ğŸ”§ é…ç½®å‚æ•°:")
    print(f"   - æ‰¹æ¬¡å¤§å°: {converter.batch_size}")
    print(f"   - æœ€å¤§å·¥ä½œè¿›ç¨‹: {converter.max_workers}")
    print(f"   - ç¼“å­˜å¯ç”¨: {converter.use_cache}")
    print(f"   - å¢é‡æ›´æ–°: {'å¯ç”¨ ğŸ”„' if incremental_mode else 'ç¦ç”¨ ğŸ”¥'}")
    print()
    
    # è½¬æ¢è®­ç»ƒé›†æ•°æ®
    try:
        converter.convert_train_set_to_graph(incremental_mode=incremental_mode)
    except RuntimeError as e:
        print(f"\n{'='*70}")
        print("âŒ è½¬æ¢å¤±è´¥")
        print(f"{'='*70}")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        print(f"\nå¯èƒ½åŸå› :")
        print("  1. å†…å­˜ä¸è¶³ï¼ˆå½“å‰å†…å­˜ä½¿ç”¨ç‡>90%ï¼‰")
        print("  2. GeoJSONæ–‡ä»¶æŸå")
        print("  3. ç£ç›˜ç©ºé—´ä¸è¶³")
        print(f"\nå»ºè®®:")
        print("  1. å…³é—­å…¶ä»–ç¨‹åºé‡Šæ”¾å†…å­˜")
        print("  2. å‡å° batch_sizeï¼ˆå½“å‰50 â†’ å»ºè®®25ï¼‰")
        print("  3. æ£€æŸ¥ GeoJSON æ–‡ä»¶å®Œæ•´æ€§")
        print(f"{'='*70}\n")
        return
    
    print("="*70)
    print("ã€é‡è¦æç¤ºã€‘")
    print("="*70)
    print("1. âœ… å·²ç”Ÿæˆå…¨å±€æ ‡å‡†åŒ–å™¨ global_scaler.pklï¼ˆæ–¹æ¡ˆAï¼šä»…åŸºäºåŸå§‹å›¾ï¼‰")
    print("2. âœ… å›¾æ•°æ®åŒ…å«èšç±»ä¿¡æ¯ï¼ˆdata.node_cluster_idsï¼‰")
    print("3. âš ï¸  æ¨¡å‹è®­ç»ƒæ—¶éœ€è¦ä¿®æ”¹ input_dim=20ï¼ˆåŸæ¥æ˜¯19ï¼‰")
    print("4. ğŸ“Š å›¾æ„å»ºæ–¹å¼ï¼šKNN + Delaunay ç»Ÿä¸€å›¾")
    print("5. ğŸ”§ å¦‚éœ€ä½¿ç”¨èšç±»ä¿¡æ¯ï¼Œå¯è®¿é—® data.node_cluster_ids")
    print("6. ğŸ¯ è‡ªé€‚åº”Kå€¼ï¼šKæœ€å¤§ä¸º8ï¼ˆèŠ‚ç‚¹æ•°<50æ—¶K=5ï¼‰")
    print("7. â­ æ ‡å‡†åŒ–ç­–ç•¥ï¼šæ–¹æ¡ˆAï¼ˆé€Ÿåº¦å¿«ã€ç¬¦åˆé›¶æ°´å°é€»è¾‘ï¼‰")
    
    print("\nã€æ–¹æ¡ˆAä¼˜åŠ¿ã€‘â­â­â­")
    print("="*70)
    print("âœ“ é€Ÿåº¦æå¿«: ç¬¬ä¸€éåªéœ€5åˆ†é’Ÿï¼ˆvs æ–¹æ¡ˆBçš„6å°æ—¶ï¼‰")
    print("âœ“ ç¬¦åˆé›¶æ°´å°é€»è¾‘: åŸå§‹å›¾æ˜¯anchorï¼Œæ”»å‡»å›¾æ˜¯positive")
    print("âœ“ ç‰¹å¾ç©ºé—´çº¯å‡€: ä¸è¢«æç«¯æ”»å‡»æ±¡æŸ“scaler")
    print("âœ“ æ•°å€¼ç¨³å®š: æ·»åŠ clipé˜²æŠ¤ï¼Œé™åˆ¶ç‰¹å¾èŒƒå›´")
    print("âœ“ ç†è®ºæ­£ç¡®: æ”»å‡»å›¾åº”è¯¥å‘åŸå§‹å›¾å¯¹é½")
    
    print("\nã€æ€§èƒ½ä¼˜åŒ–ã€‘â­æ–°å¢å¢é‡æ›´æ–°")
    print("="*70)
    print("âœ“ KD-treeåŠ é€ŸKNN: O(n log n) å¤æ‚åº¦")
    print("âœ“ Delaunayä¸‰è§’å‰–åˆ†: O(n log n) å¤æ‚åº¦")
    print("âœ“ æ™ºèƒ½ç¼“å­˜æœºåˆ¶: é¿å…é‡å¤è®¡ç®—ï¼ŒäºŒæ¬¡è¿è¡Œæé€Ÿ")
    print("âœ“ åˆ†æ‰¹å¤„ç†: å†…å­˜å ç”¨ä¼˜åŒ–ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®")
    print("âœ“ å¢é‡æ›´æ–°: åŸºäºæ–‡ä»¶å“ˆå¸Œï¼Œåªæ›´æ–°å˜åŒ–çš„æ–‡ä»¶")
    print("  - åŸå§‹å›¾å˜åŒ– â†’ è‡ªåŠ¨å®Œå…¨é‡æ–°ç”Ÿæˆ")
    print("  - æ”»å‡»å›¾å˜åŒ– â†’ ä»…æ›´æ–°å˜åŒ–çš„æ–‡ä»¶")
    print("  - å¤§å¹…èŠ‚çœæ—¶é—´ï¼Œç‰¹åˆ«æ˜¯ä¿®æ”¹å°‘é‡æ”»å‡»æ–‡ä»¶æ—¶")
    print("âœ“ å®æ—¶ç›‘æ§: ç³»ç»Ÿèµ„æºç›‘æ§ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º")
    print("âœ“ è‡ªé€‚åº”Kå€¼: Kæœ€å¤§ä¸º8ï¼Œä¿æŒå›¾ç¨€ç–æ€§")
    
    print("\nã€KNN + Delaunay ç»Ÿä¸€å›¾çš„ä¼˜åŠ¿ã€‘")
    print("="*70)
    print("  âœ“ é€‚ç”¨æ‰€æœ‰æ•°æ®ç±»å‹ï¼ˆç‚¹/çº¿/é¢ç»Ÿä¸€æ–¹æ¡ˆï¼‰")
    print("  âœ“ æ— å­¤å²›èŠ‚ç‚¹ï¼ˆDelaunayä¿è¯100%è¿é€šï¼‰")
    print("  âœ“ å±€éƒ¨å¯†é›†ï¼ˆKNNä¿è¯é‚»åŸŸå……åˆ†ï¼ŒKâ‰¤8ï¼‰")
    print("  âœ“ è‡ªé€‚åº”Kå€¼ï¼ˆKæœ€å¤§ä¸º8ï¼ŒèŠ‚ç‚¹æ•°<50æ—¶K=5ï¼‰")
    print("  âœ“ å›¾ç¨€ç–æ€§å¥½ï¼ˆé¿å…è¿‡åº¦è¿æ¥ï¼Œè®­ç»ƒæ›´å¿«ï¼‰")
    print("  âœ“ 20ç»´ç‰¹å¾ï¼ˆæ–°å¢èŠ‚ç‚¹æ•°ç¼–ç ï¼‰")
    print("  âœ“ æ¨¡å‹æ³›åŒ–èƒ½åŠ›æå‡ï¼ˆç»“æ„å½’ä¸€åŒ–ï¼‰")
    print("  âœ“ è®¡ç®—é«˜æ•ˆï¼ˆO(n log n)ï¼Œæ— éœ€R-treeï¼‰")
    print("="*70 + "\n")

if __name__ == "__main__":
    main()

