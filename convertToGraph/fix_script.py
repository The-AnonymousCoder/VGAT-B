#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤TestSetè„šæœ¬çš„å·¥å…·
"""

import shutil
import os

def main():
    print("å¼€å§‹ä¿®å¤TestSetè„šæœ¬...")

    # å¤‡ä»½æ—§æ–‡ä»¶
    if os.path.exists('convertToGraph-TestSet-IMPROVED.py'):
        shutil.copy2('convertToGraph-TestSet-IMPROVED.py', 'convertToGraph-TestSet-IMPROVED.py.backup')
        print("å·²å¤‡ä»½æ—§æ–‡ä»¶")

    # è¯»å–TrainingSetæ–‡ä»¶ä½œä¸ºæ¨¡æ¿
    with open('convertToGraph-TrainingSet-IMPROVED.py', 'r', encoding='utf-8') as f:
        training_content = f.read()

    # åŸºæœ¬æ›¿æ¢
    test_content = training_content.replace(
        'class ImprovedTrainSetVectorToGraphConverter:',
        'class ImprovedTestSetVectorToGraphConverter:'
    ).replace(
        'è®­ç»ƒé›†',
        'æµ‹è¯•é›†'
    ).replace(
        'TrainSet',
        'TestSet'
    ).replace(
        'TrainingSet',
        'TestSet'
    ).replace(
        'ImprovedTrainSetVectorToGraphConverter(',
        'ImprovedTestSetVectorToGraphConverter('
    ).replace(
        'convert_train_set_to_graph',
        'convert_test_set_to_graph'
    )

    # æ›¿æ¢æ„é€ å‡½æ•°å‚æ•°
    old_init = '''    def __init__(self, vector_dir="../convertToGeoJson/GeoJson/TrainingSet",
                 attacked_dir="../convertToGeoJson-Attacked/GeoJson-Attacked/TrainingSet",
                 graph_dir="Graph/TrainingSet",
                 batch_size=100,
                 max_workers=None,
                 use_cache=True):
        self.vector_dir = vector_dir
        self.attacked_dir = attacked_dir
        self.graph_dir = graph_dir
        self.batch_size = batch_size
        self.max_workers = max_workers or min(8, mp.cpu_count())
        self.use_cache = use_cache'''

    new_init = '''    def __init__(self, original_dir="../convertToGeoJson/GeoJson/TestSet",
                 attacked_dir="../convertToGeoJson-Attacked/GeoJson-Attacked/TestSet",
                 graph_dir="Graph/TestSet"):
        self.original_dir = original_dir
        self.attacked_dir = attacked_dir
        self.graph_dir = graph_dir'''

    test_content = test_content.replace(old_init, new_init)

    # ç§»é™¤TrainingSetç‰¹æœ‰çš„åˆå§‹åŒ–ä»£ç 
    lines = test_content.split('\n')
    new_lines = []
    skip_line = False

    for line in lines:
        # è·³è¿‡TrainingSetç‰¹æœ‰çš„åˆå§‹åŒ–
        if 'self.batch_size = batch_size' in line:
            continue
        elif 'self.max_workers = max_workers or min(8, mp.cpu_count())' in line:
            continue
        elif 'self.use_cache = use_cache' in line:
            continue
        elif 'self.cache_dir =' in line:
            continue
        elif 'self.features_cache_file =' in line:
            continue
        elif 'self.file_hashes_file =' in line:
            continue
        elif 'os.path.join(self.cache_dir,' in line:
            continue

        # è·³è¿‡batch processingç›¸å…³çš„æ–¹æ³•
        if 'def partition_by_hilbert' in line:
            skip_line = True
            continue
        elif skip_line and line.strip().startswith('def '):
            skip_line = False

        if not skip_line:
            new_lines.append(line)

    test_content = '\n'.join(new_lines)

    # ç§»é™¤ä¸éœ€è¦çš„å¯¼å…¥
    imports_to_remove = [
        'import multiprocessing as mp',
        'from concurrent.futures import ProcessPoolExecutor, as_completed',
        'import psutil',
        'import gc',
        'import time',
        'import hashlib',
        'import json'
    ]

    for imp in imports_to_remove:
        test_content = test_content.replace(imp + '\n', '')

    # ä¿®æ”¹æ„é€ å‡½æ•°ä½“
    test_content = test_content.replace(
        '''        self.ensure_graph_dir()

        # ä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–å™¨
        self.global_scaler = StandardScaler()
        self.scaler_fitted = False

        # å­˜å‚¨å…¨å±€ç»Ÿè®¡é‡ï¼ˆç”¨äºè®¡ç®—ç›¸å¯¹ä½ç½®ï¼‰
        self.global_bounds = None
        self.global_centroid = None

        # ç¼“å­˜ç›¸å…³
        self.cache_dir = os.path.join(self.graph_dir, "cache")
        if not os.path.exists(self.cache_dir):
            os.makedirs(self.cache_dir)
        # â­ æ–¹æ¡ˆAï¼šä»…ç”¨åŸå§‹å›¾ç”Ÿæˆscalerï¼Œç¼“å­˜æ–‡ä»¶ååŒºåˆ†
        self.features_cache_file = os.path.join(self.cache_dir, "features_cache_original_only.pkl")
        self.file_hashes_file = os.path.join(self.cache_dir, "file_hashes_original_only.json")''',
        '''        self.ensure_graph_dir()

        # â­ å…¨å±€æ ‡å‡†åŒ–å™¨ï¼ˆä¸¤éå¤„ç†ï¼‰
        self.global_scaler = StandardScaler()
        self.scaler_fitted = False'''
    )

    # ç§»é™¤TrainingSetç‰¹æœ‰çš„mainå‡½æ•°å‚æ•°
    test_content = test_content.replace(
        '    converter = ImprovedTrainSetVectorToGraphConverter(\n        batch_size=25,      # â­é™ä½ä¸º25ï¼ˆåŸ50ï¼‰ï¼Œå‡å°‘å†…å­˜å ç”¨\n        max_workers=4,      # å¯æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´ (é»˜è®¤8)\n        use_cache=True      # å¯ç”¨ç¼“å­˜åŠ é€Ÿ\n    )',
        '    converter = ImprovedTestSetVectorToGraphConverter()'
    )

    test_content = test_content.replace(
        '''    # â­ å¢é‡æ›´æ–°æ¨¡å¼ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
    # å¦‚éœ€å®Œå…¨é‡æ–°ç”Ÿæˆï¼Œè®¾ç½® incremental_mode=False
    incremental_mode = True  # True: å¢é‡æ›´æ–° | False: å®Œå…¨é‡æ–°ç”Ÿæˆ

    print(f"ğŸ”§ é…ç½®å‚æ•°:")
    print(f"   - æ‰¹æ¬¡å¤§å°: {converter.batch_size}")
    print(f"   - æœ€å¤§å·¥ä½œè¿›ç¨‹: {converter.max_workers}")
    print(f"   - ç¼“å­˜å¯ç”¨: {converter.use_cache}")
    print(f"   - å¢é‡æ›´æ–°: {'å¯ç”¨ ğŸ”„' if incremental_mode else 'ç¦ç”¨ ğŸ”¥'}")
    print()

    # è½¬æ¢è®­ç»ƒé›†æ•°æ®
    try:
        converter.convert_train_set_to_graph(incremental_mode=incremental_mode)''',
        '''    # è½¬æ¢æµ‹è¯•é›†æ•°æ®ï¼ˆä¸¤éå¤„ç†ï¼‰
    converter.convert_test_set_to_graph()'''
    )

    # å†™å›æ–‡ä»¶
    with open('convertToGraph-TestSet-IMPROVED.py', 'w', encoding='utf-8') as f:
        f.write(test_content)

    print("TestSetè„šæœ¬å·²é‡æ–°åˆ›å»ºå®Œæˆï¼")

if __name__ == '__main__':
    main()






















