#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
çŸ¢é‡åœ°å›¾æ•°æ®è½¬æ¢ä¸ºGeoJSONæ ¼å¼çš„è„šæœ¬
åŠŸèƒ½ï¼šå°†SourceDataç›®å½•ä¸‹çš„GDBå’ŒSHPæ–‡ä»¶è½¬æ¢ä¸ºGeoJSONæ ¼å¼
è¾“å‡ºï¼šè½¬æ¢åçš„æ–‡ä»¶ä¿å­˜åœ¨GeoJsonç›®å½•ä¸‹ï¼Œå‘½åæ ¼å¼ä¸º"æ–‡ä»¶å¤¹å-å›¾å±‚å"
"""

import os
import sys
from pathlib import Path
import logging
from typing import List, Tuple, Optional
import json

try:
    import geopandas as gpd
    import fiona
    from fiona.drvsupport import supported_drivers
except ImportError as e:
    print("é”™è¯¯ï¼šç¼ºå°‘å¿…è¦çš„åœ°ç†æ•°æ®å¤„ç†åº“")
    print("è¯·å®‰è£…ä»¥ä¸‹åº“ï¼špip install geopandas fiona")
    print(f"è¯¦ç»†é”™è¯¯ï¼š{e}")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('conversion.log', encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

class VectorToGeoJsonConverter:
    """çŸ¢é‡æ•°æ®åˆ°GeoJSONè½¬æ¢å™¨"""
    
    def __init__(self, source_dir: str = "SourceData", output_dir: str = "GeoJson"):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            source_dir: æºæ•°æ®ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
        """
        self.script_dir = Path(__file__).parent
        self.source_dir = self.script_dir / source_dir
        self.output_dir = self.script_dir / output_dir
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºTrainingSetå’ŒTestSetè¾“å‡ºç›®å½•
        self.training_output_dir = self.output_dir / "TrainingSet"
        self.test_output_dir = self.output_dir / "TestSet"
        self.training_output_dir.mkdir(exist_ok=True)
        self.test_output_dir.mkdir(exist_ok=True)
        
        logger.info(f"æºæ•°æ®ç›®å½•: {self.source_dir}")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info(f"è®­ç»ƒé›†è¾“å‡ºç›®å½•: {self.training_output_dir}")
        logger.info(f"æµ‹è¯•é›†è¾“å‡ºç›®å½•: {self.test_output_dir}")
    
    def discover_gdb_files(self, dataset: str) -> List[Tuple[str, str]]:
        """
        å‘ç°æŒ‡å®šæ•°æ®é›†ä¸­çš„æ‰€æœ‰GDBæ–‡ä»¶
        
        Args:
            dataset: æ•°æ®é›†åç§° ("TrainingSet" æˆ– "TestSet")
            
        Returns:
            List[Tuple[str, str]]: (gdbæ–‡ä»¶è·¯å¾„, æ–‡ä»¶å¤¹åç§°) çš„åˆ—è¡¨
        """
        gdb_files = []
        gdb_dir = self.source_dir / dataset / "GDB"
        
        if not gdb_dir.exists():
            logger.warning(f"GDBç›®å½•ä¸å­˜åœ¨: {gdb_dir}")
            return gdb_files
        
        for item in gdb_dir.iterdir():
            if item.is_dir() and item.suffix.lower() == '.gdb':
                folder_name = item.stem  # å»æ‰.gdbåç¼€
                gdb_files.append((str(item), folder_name))
                logger.info(f"å‘ç°{dataset}ä¸­çš„GDBæ–‡ä»¶: {item}")
        
        return gdb_files
    
    def discover_shp_files(self, dataset: str) -> List[Tuple[str, str, str]]:
        """
        å‘ç°æŒ‡å®šæ•°æ®é›†ä¸­çš„æ‰€æœ‰SHPæ–‡ä»¶
        
        Args:
            dataset: æ•°æ®é›†åç§° ("TrainingSet" æˆ– "TestSet")
            
        Returns:
            List[Tuple[str, str, str]]: (shpæ–‡ä»¶è·¯å¾„, æ–‡ä»¶å¤¹åç§°, æ–‡ä»¶å) çš„åˆ—è¡¨
        """
        shp_files = []
        shp_dir = self.source_dir / dataset / "SHP"
        
        if not shp_dir.exists():
            logger.warning(f"SHPç›®å½•ä¸å­˜åœ¨: {shp_dir}")
            return shp_files
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰.shpæ–‡ä»¶ï¼Œä½†æ’é™¤ä»¥.shpç»“å°¾çš„æ–‡ä»¶å¤¹
        for shp_file in shp_dir.rglob("*.shp"):
            # æ£€æŸ¥æ˜¯å¦ä¸ºçœŸæ­£çš„æ–‡ä»¶ï¼ˆä¸æ˜¯æ–‡ä»¶å¤¹ï¼‰
            if not shp_file.is_file():
                logger.warning(f"è·³è¿‡ä»¥.shpç»“å°¾çš„æ–‡ä»¶å¤¹: {shp_file}")
                continue
                
            # è·å–ç›¸å¯¹äºSHPç›®å½•çš„çˆ¶æ–‡ä»¶å¤¹åç§°
            relative_path = shp_file.relative_to(shp_dir)
            folder_name = relative_path.parts[0] if len(relative_path.parts) > 1 else shp_dir.name
            file_name = shp_file.stem  # å»æ‰.shpåç¼€
            
            shp_files.append((str(shp_file), folder_name, file_name))
            logger.info(f"å‘ç°{dataset}ä¸­çš„SHPæ–‡ä»¶: {shp_file} (æ–‡ä»¶å¤¹: {folder_name})")
        
        return shp_files
    
    def get_gdb_layers(self, gdb_path: str) -> List[str]:
        """
        è·å–GDBæ–‡ä»¶ä¸­çš„æ‰€æœ‰å›¾å±‚
        
        Args:
            gdb_path: GDBæ–‡ä»¶è·¯å¾„
            
        Returns:
            List[str]: å›¾å±‚åç§°åˆ—è¡¨
        """
        try:
            layers = fiona.listlayers(gdb_path)
            logger.info(f"GDB {gdb_path} åŒ…å« {len(layers)} ä¸ªå›¾å±‚: {layers}")
            return layers
        except Exception as e:
            logger.error(f"æ— æ³•è¯»å–GDBæ–‡ä»¶ {gdb_path} çš„å›¾å±‚: {e}")
            return []
    
    def convert_gdb_to_geojson(self, gdb_path: str, folder_name: str, output_dir: Path) -> int:
        """
        å°†GDBæ–‡ä»¶è½¬æ¢ä¸ºGeoJSON
        
        Args:
            gdb_path: GDBæ–‡ä»¶è·¯å¾„
            folder_name: æ–‡ä»¶å¤¹åç§°ï¼ˆç”¨äºå‘½åï¼‰
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            
        Returns:
            int: æˆåŠŸè½¬æ¢çš„å›¾å±‚æ•°é‡
        """
        success_count = 0
        layers = self.get_gdb_layers(gdb_path)
        
        for layer in layers:
            try:
                # è¯»å–å›¾å±‚æ•°æ®
                gdf = gpd.read_file(gdb_path, layer=layer)
                
                if gdf.empty:
                    logger.warning(f"å›¾å±‚ {layer} ä¸ºç©ºï¼Œè·³è¿‡")
                    continue
                
                # è½¬æ¢ä¸ºWGS84åæ ‡ç³»ï¼ˆGeoJSONæ ‡å‡†ï¼‰
                if gdf.crs and gdf.crs != 'EPSG:4326':
                    gdf = gdf.to_crs('EPSG:4326')
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼šæ–‡ä»¶å¤¹å-å›¾å±‚å
                output_filename = f"{folder_name}-{layer}.geojson"
                output_path = output_dir / output_filename
                
                # ä¿å­˜ä¸ºGeoJSON
                gdf.to_file(output_path, driver='GeoJSON', encoding='utf-8')
                
                logger.info(f"æˆåŠŸè½¬æ¢: {layer} -> {output_filename} ({len(gdf)} ä¸ªè¦ç´ )")
                success_count += 1
                
            except Exception as e:
                logger.error(f"è½¬æ¢GDBå›¾å±‚ {layer} å¤±è´¥: {e}")
        
        return success_count
    
    def convert_shp_to_geojson(self, shp_path: str, folder_name: str, file_name: str, output_dir: Path) -> bool:
        """
        å°†SHPæ–‡ä»¶è½¬æ¢ä¸ºGeoJSON
        
        Args:
            shp_path: SHPæ–‡ä»¶è·¯å¾„
            folder_name: æ–‡ä»¶å¤¹åç§°
            file_name: æ–‡ä»¶åï¼ˆä¸åŒ…å«æ‰©å±•åï¼‰
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            
        Returns:
            bool: è½¬æ¢æ˜¯å¦æˆåŠŸ
        """
        # å°è¯•å¤šç§ç¼–ç æ ¼å¼
        encodings = ['utf-8', 'gbk', 'gb2312', 'cp936', 'latin1', 'iso-8859-1']
        
        for encoding in encodings:
            try:
                # è¯»å–SHPæ–‡ä»¶
                gdf = gpd.read_file(shp_path, encoding=encoding)
                
                if gdf.empty:
                    logger.warning(f"SHPæ–‡ä»¶ {shp_path} ä¸ºç©ºï¼Œè·³è¿‡")
                    return False
                
                # è½¬æ¢ä¸ºWGS84åæ ‡ç³»ï¼ˆGeoJSONæ ‡å‡†ï¼‰
                if gdf.crs and gdf.crs != 'EPSG:4326':
                    gdf = gdf.to_crs('EPSG:4326')
                
                # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼šæ–‡ä»¶å¤¹å-æ–‡ä»¶å
                output_filename = f"{folder_name}-{file_name}.geojson"
                output_path = output_dir / output_filename
                
                # ä¿å­˜ä¸ºGeoJSONï¼ˆå§‹ç»ˆä½¿ç”¨UTF-8ç¼–ç ï¼‰
                gdf.to_file(output_path, driver='GeoJSON', encoding='utf-8')
                
                logger.info(f"æˆåŠŸè½¬æ¢: {shp_path} -> {output_filename} ({len(gdf)} ä¸ªè¦ç´ ) [ç¼–ç : {encoding}]")
                return True
                
            except UnicodeDecodeError as e:
                # ç¼–ç é”™è¯¯ï¼Œå°è¯•ä¸‹ä¸€ç§ç¼–ç 
                logger.debug(f"ç¼–ç  {encoding} è¯»å– {shp_path} å¤±è´¥: {e}")
                continue
            except Exception as e:
                # å…¶ä»–é”™è¯¯ï¼Œä¹Ÿå°è¯•ä¸‹ä¸€ç§ç¼–ç 
                logger.debug(f"ç¼–ç  {encoding} å¤„ç† {shp_path} æ—¶å‡ºé”™: {e}")
                continue
        
        # æ‰€æœ‰ç¼–ç éƒ½å¤±è´¥äº†
        logger.error(f"è½¬æ¢SHPæ–‡ä»¶ {shp_path} å¤±è´¥: å°è¯•äº†æ‰€æœ‰ç¼–ç æ ¼å¼ {encodings} éƒ½æ— æ³•è¯»å–æ–‡ä»¶")
        return False
    
    def clean_output_directory(self, output_dir: Path, dataset: str):
        """
        æ¸…ç†è¾“å‡ºç›®å½•ä¸­çš„æ—§æ–‡ä»¶
        
        Args:
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            dataset: æ•°æ®é›†åç§°
        """
        if output_dir.exists():
            old_files = list(output_dir.glob("*.geojson"))
            if old_files:
                logger.info(f"æ¸…ç†{dataset}ä¸­çš„ {len(old_files)} ä¸ªæ—§GeoJSONæ–‡ä»¶")
                for old_file in old_files:
                    try:
                        old_file.unlink()
                        logger.debug(f"åˆ é™¤æ—§æ–‡ä»¶: {old_file.name}")
                    except Exception as e:
                        logger.warning(f"åˆ é™¤æ—§æ–‡ä»¶ {old_file.name} å¤±è´¥: {e}")
            else:
                logger.info(f"{dataset}è¾“å‡ºç›®å½•ä¸ºç©ºï¼Œæ— éœ€æ¸…ç†")
        else:
            logger.info(f"{dataset}è¾“å‡ºç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ç›®å½•")
            output_dir.mkdir(parents=True, exist_ok=True)

    def convert_dataset(self, dataset: str, output_dir: Path):
        """
        è½¬æ¢æŒ‡å®šæ•°æ®é›†çš„æ‰€æœ‰çŸ¢é‡æ•°æ®æ–‡ä»¶
        
        Args:
            dataset: æ•°æ®é›†åç§° ("TrainingSet" æˆ– "TestSet")
            output_dir: è¾“å‡ºç›®å½•
        """
        logger.info(f"=== å¼€å§‹è½¬æ¢{dataset}æ•°æ®é›† ===")
        
        # é¦–å…ˆæ¸…ç†æ—§æ–‡ä»¶
        self.clean_output_directory(output_dir, dataset)
        
        converted_count = 0
        failed_count = 0
        
        # è½¬æ¢GDBæ–‡ä»¶
        logger.info(f"--- è½¬æ¢{dataset}ä¸­çš„GDBæ–‡ä»¶ ---")
        gdb_files = self.discover_gdb_files(dataset)
        
        for gdb_path, folder_name in gdb_files:
            logger.info(f"å¤„ç†{dataset}ä¸­çš„GDBæ–‡ä»¶: {gdb_path}")
            converted_layers = self.convert_gdb_to_geojson(gdb_path, folder_name, output_dir)
            converted_count += converted_layers
        
        # è½¬æ¢SHPæ–‡ä»¶
        logger.info(f"--- è½¬æ¢{dataset}ä¸­çš„SHPæ–‡ä»¶ ---")
        shp_files = self.discover_shp_files(dataset)
        
        for shp_path, folder_name, file_name in shp_files:
            logger.info(f"å¤„ç†{dataset}ä¸­çš„SHPæ–‡ä»¶: {shp_path}")
            if self.convert_shp_to_geojson(shp_path, folder_name, file_name, output_dir):
                converted_count += 1
            else:
                failed_count += 1
        
        logger.info(f"{dataset}è½¬æ¢å®Œæˆ: æˆåŠŸè½¬æ¢ {converted_count} ä¸ªæ–‡ä»¶, å¤±è´¥ {failed_count} ä¸ªæ–‡ä»¶")
        return converted_count, failed_count
    
    def convert_all(self):
        """
        è½¬æ¢æ‰€æœ‰æ”¯æŒçš„çŸ¢é‡æ•°æ®æ–‡ä»¶
        """
        logger.info("å¼€å§‹çŸ¢é‡æ•°æ®è½¬æ¢...")
        logger.info("æœ¬æ¬¡è¿è¡Œå°†å®Œå…¨è¦†ç›–ä¹‹å‰ç”Ÿæˆçš„æ‰€æœ‰GeoJSONæ–‡ä»¶")
        
        # æ¸…ç†æ—§çš„æ—¥å¿—æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
        log_file = Path("conversion.log")
        if log_file.exists():
            try:
                import time
                # å¤‡ä»½ä¹‹å‰çš„æ—¥å¿—ï¼Œä½¿ç”¨å½“å‰æ—¶é—´æˆ³
                timestamp = int(time.time())
                backup_log = Path(f"conversion_backup_{timestamp}.log")
                if not backup_log.exists():
                    import shutil
                    shutil.copy2(log_file, backup_log)
                    logger.info(f"æ—§æ—¥å¿—å·²å¤‡ä»½ä¸º: {backup_log.name}")
            except Exception as e:
                logger.warning(f"å¤‡ä»½æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        
        total_converted = 0
        total_failed = 0
        
        # è½¬æ¢TrainingSet
        training_converted, training_failed = self.convert_dataset("TrainingSet", self.training_output_dir)
        total_converted += training_converted
        total_failed += training_failed
        
        # è½¬æ¢TestSet
        test_converted, test_failed = self.convert_dataset("TestSet", self.test_output_dir)
        total_converted += test_converted
        total_failed += test_failed
        
        # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
        logger.info("=== è½¬æ¢å®Œæˆ ===")
        logger.info(f"æ€»è®¡æˆåŠŸè½¬æ¢: {total_converted} ä¸ªæ–‡ä»¶")
        logger.info(f"æ€»è®¡è½¬æ¢å¤±è´¥: {total_failed} ä¸ªæ–‡ä»¶")
        logger.info(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        logger.info("âœ… æ‰€æœ‰ç”Ÿæˆçš„GeoJSONæ–‡ä»¶å·²å®Œå…¨æ›¿æ¢ä¹‹å‰çš„å¯¹åº”æ–‡ä»¶")
        logger.info("ğŸ“ æ¯æ¬¡è¿è¡Œéƒ½ä¼šè‡ªåŠ¨æ¸…ç†å¹¶é‡æ–°ç”Ÿæˆæ‰€æœ‰è¾“å‡ºæ–‡ä»¶")
        
        # åˆ—å‡ºç”Ÿæˆçš„æ–‡ä»¶
        self._list_generated_files("TrainingSet", self.training_output_dir)
        self._list_generated_files("TestSet", self.test_output_dir)
    
    def _list_generated_files(self, dataset: str, output_dir: Path):
        """
        åˆ—å‡ºæŒ‡å®šæ•°æ®é›†ç”Ÿæˆçš„æ–‡ä»¶
        
        Args:
            dataset: æ•°æ®é›†åç§°
            output_dir: è¾“å‡ºç›®å½•
        """
        if output_dir.exists():
            geojson_files = list(output_dir.glob("*.geojson"))
            logger.info(f"{dataset}ç”Ÿæˆçš„GeoJSONæ–‡ä»¶æ•°é‡: {len(geojson_files)}")
            for file in sorted(geojson_files):
                logger.info(f"  - {file.name}")
        else:
            logger.warning(f"{dataset}è¾“å‡ºç›®å½•ä¸å­˜åœ¨: {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    try:
        # æ£€æŸ¥é©±åŠ¨æ”¯æŒ
        supported_drivers['OpenFileGDB'] = 'r'  # ç¡®ä¿æ”¯æŒGDBè¯»å–
        
        # åˆ›å»ºè½¬æ¢å™¨å¹¶æ‰§è¡Œè½¬æ¢
        converter = VectorToGeoJsonConverter()
        converter.convert_all()
        
    except KeyboardInterrupt:
        logger.info("ç”¨æˆ·ä¸­æ–­æ“ä½œ")
    except Exception as e:
        logger.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        raise


if __name__ == "__main__":
    main()
