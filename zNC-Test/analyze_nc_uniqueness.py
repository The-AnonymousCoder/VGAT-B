#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†ææµ‹è¯•é›†é›¶æ°´å°çš„NCå”¯ä¸€æ€§
åªåˆ†æNCå€¼ï¼ˆå½’ä¸€åŒ–ç›¸å…³ç³»æ•°ï¼‰
"""

import numpy as np
import cv2
from pathlib import Path
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import json
import re
import pandas as pd

def generate_label_from_filename(filename):
    """ä»æ–‡ä»¶åç”Ÿæˆç®€æ´çš„æ ‡ç­¾"""
    # ç§»é™¤æ‰©å±•åå’Œ_watermarkåç¼€
    base = filename.replace('_watermark.png', '').replace('_watermark.npy', '')
    
    # æå–å…³é”®éƒ¨åˆ†
    # å¤„ç† H50-XXX æ ¼å¼
    if base.startswith('H50-'):
        return base.replace('H50-', '')
    
    # å¤„ç† shanghai-latest-free.shp-gis_osm_xxx æ ¼å¼
    if 'gis_osm_' in base:
        match = re.search(r'gis_osm_(\w+)_', base)
        if match:
            category = match.group(1)
            # è½¬æ¢ä¸ºæ›´å‹å¥½çš„åç§°
            category_map = {
                'landuse': 'Landuse',
                'natural': 'Natural',
                'railways': 'Railways',
                'waterways': 'Waterways',
                'places': 'Places',
                'transport': 'Transport'
            }
            return category_map.get(category, category.capitalize())
    
    # å¦‚æœæ–‡ä»¶åå¤ªé•¿ï¼Œæˆªå–å…³é”®éƒ¨åˆ†
    if len(base) > 15:
        # å°è¯•æå–æœ€åä¸€ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†
        parts = base.split('-')
        if len(parts) > 1:
            return parts[-1]
        return base[:15]
    
    return base

def analyze_nc_uniqueness():
    """åˆ†æé›¶æ°´å°NCçŸ©é˜µ"""
    
    script_dir = Path(__file__).resolve().parent
    watermark_dir = script_dir / 'vector-data-zerowatermark'
    
    # è‡ªé€‚åº”æ‰«ææ‰€æœ‰æ°´å°æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨.pngï¼‰
    watermark_files = {}
    for ext in ['.png', '.npy']:
        for file_path in watermark_dir.glob(f'*_watermark{ext}'):
            base_name = file_path.stem.replace('_watermark', '')
            if base_name not in watermark_files:
                watermark_files[base_name] = file_path
    
    if not watermark_files:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æ°´å°æ–‡ä»¶")
        return
    
    # åŠ è½½é›¶æ°´å°å‘é‡
    vectors = []
    found_labels = []
    found_files = []
    
    print(f"\nğŸ“Š æ‰¾åˆ° {len(watermark_files)} ä¸ªæ°´å°æ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½...\n")
    
    for base_name, file_path in sorted(watermark_files.items()):
        label = generate_label_from_filename(file_path.name)
        
        try:
            if file_path.suffix == '.png':
                img = cv2.imread(str(file_path), 0)
                if img is None:
                    print(f"âš ï¸  è­¦å‘Š: æ— æ³•è¯»å–å›¾ç‰‡ {file_path.name}")
                    continue
                vec = (img.flatten() / 255).astype(np.uint8)
            else:  # .npy
                vec = np.load(file_path).astype(np.uint8)
                # å¦‚æœæ˜¯2Dæ•°ç»„ï¼Œå±•å¹³
                if vec.ndim > 1:
                    vec = vec.flatten()
            
            vectors.append(vec)
            found_labels.append(label)
            found_files.append(base_name)
            print(f"âœ“ åŠ è½½ {label:20s}: {vec.shape} -> {len(vec)} bits")
        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: åŠ è½½ {file_path.name} å¤±è´¥: {e}")
            continue
    
    if not vectors:
        print("âŒ é”™è¯¯: æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ°´å°æ–‡ä»¶")
        return
    
    n = len(vectors)
    print(f"\nğŸ“ è®¡ç®— {n}x{n} NCçŸ©é˜µ...\n")
    
    # è®¡ç®—NCçŸ©é˜µ
    nc_matrix = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            v1 = vectors[i].astype(float)
            v2 = vectors[j].astype(float)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)
            if norm1 > 0 and norm2 > 0:
                nc = np.dot(v1, v2) / (norm1 * norm2)
            else:
                nc = 0.0
            nc_matrix[i, j] = nc
    
    # æå–éå¯¹è§’çº¿å…ƒç´ 
    mask = ~np.eye(n, dtype=bool)
    off_diag = nc_matrix[mask]
    
    # ç»Ÿè®¡ä¿¡æ¯
    max_off_diag_nc = float(np.max(off_diag))
    min_off_diag_nc = float(np.min(off_diag))
    mean_off_diag_nc = float(np.mean(off_diag))
    std_off_diag_nc = float(np.std(off_diag))
    median_off_diag_nc = float(np.median(off_diag))
    
    # ç»Ÿè®¡å„åŒºé—´çš„é…å¯¹æ•°
    ranges = [
        (0.0, 0.5, "æä½ç›¸ä¼¼"),
        (0.5, 0.75, "ä½ç›¸ä¼¼"),
        (0.75, 0.85, "ä¸­ç­‰ç›¸ä¼¼"),
        (0.85, 0.9, "é«˜ç›¸ä¼¼"),
        (0.9, 1.0, "æé«˜ç›¸ä¼¼")
    ]
    
    total_pairs = len(off_diag)
    
    # ç»Ÿè®¡ä¸åŒé˜ˆå€¼çš„é…å¯¹æ•°
    threshold_uniqueness = 0.82
    pairs_ge_080 = int(np.sum(off_diag >= 0.80))
    pairs_ge_082 = int(np.sum(off_diag >= 0.82))
    pairs_ge_085 = int(np.sum(off_diag >= 0.85))
    pairs_ge_090 = int(np.sum(off_diag >= 0.90))
    
    # æ‰¾å‡ºæœ€é«˜çš„NCå€¼é…å¯¹
    indices = np.triu_indices(n, k=1)
    nc_pairs = [(found_labels[i], found_labels[j], nc_matrix[i, j]) 
                for i, j in zip(indices[0], indices[1])]
    nc_pairs.sort(key=lambda x: x[2], reverse=True)
    
    # è¯¦ç»†åˆ†æè¾“å‡º
    print("\n" + "=" * 80)
    print("ğŸ“Š NCå”¯ä¸€æ€§åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    print(f"\nğŸ“ˆ åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»é…å¯¹æ•°: {total_pairs}")
    print(f"   æœ€å¤§éå¯¹è§’çº¿NCå€¼: {max_off_diag_nc:.6f}")
    print(f"   æœ€å°éå¯¹è§’çº¿NCå€¼: {min_off_diag_nc:.6f}")
    print(f"   å¹³å‡éå¯¹è§’çº¿NCå€¼: {mean_off_diag_nc:.6f}")
    print(f"   æ ‡å‡†å·®: {std_off_diag_nc:.6f}")
    print(f"   ä¸­ä½æ•°: {median_off_diag_nc:.6f}")
    
    print(f"\nğŸ¯ é˜ˆå€¼ç»Ÿè®¡:")
    print(f"   NC â‰¥ 0.80 çš„é…å¯¹æ•°: {pairs_ge_080} ({pairs_ge_080/total_pairs*100:.2f}%)")
    print(f"   NC â‰¥ 0.82 çš„é…å¯¹æ•°: {pairs_ge_082} ({pairs_ge_082/total_pairs*100:.2f}%)")
    print(f"   NC â‰¥ 0.85 çš„é…å¯¹æ•°: {pairs_ge_085} ({pairs_ge_085/total_pairs*100:.2f}%)")
    print(f"   NC â‰¥ 0.90 çš„é…å¯¹æ•°: {pairs_ge_090} ({pairs_ge_090/total_pairs*100:.2f}%)")
    
    print(f"\nğŸ“Š ç›¸ä¼¼åº¦åˆ†å¸ƒ:")
    for low, high, desc in ranges:
        count = int(np.sum((off_diag >= low) & (off_diag < high)))
        if high == 1.0:
            count = int(np.sum(off_diag >= low))
        pct = count / total_pairs * 100 if total_pairs > 0 else 0
        print(f"   {desc:8s} [{low:.2f}-{high:.2f}): {count:3d} å¯¹ ({pct:5.2f}%)")
    
    # å”¯ä¸€æ€§è¯„ä¼°
    uniqueness_ok = max_off_diag_nc < threshold_uniqueness
    print(f"\nâœ… å”¯ä¸€æ€§è¯„ä¼° (é˜ˆå€¼={threshold_uniqueness}):")
    if uniqueness_ok:
        print(f"   âœ“ é€šè¿‡: æœ€å¤§éå¯¹è§’çº¿NCå€¼ {max_off_diag_nc:.6f} < {threshold_uniqueness}")
    else:
        print(f"   âœ— æœªé€šè¿‡: æœ€å¤§éå¯¹è§’çº¿NCå€¼ {max_off_diag_nc:.6f} â‰¥ {threshold_uniqueness}")
    
    # æ˜¾ç¤ºé«˜ç›¸ä¼¼åº¦é…å¯¹
    high_sim_pairs = [p for p in nc_pairs if p[2] >= 0.75]
    if high_sim_pairs:
        print(f"\nâš ï¸  é«˜ç›¸ä¼¼åº¦é…å¯¹ (NC â‰¥ 0.75):")
        for label1, label2, nc_val in high_sim_pairs[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"   {label1:20s} <-> {label2:20s}: {nc_val:.6f}")
        if len(high_sim_pairs) > 10:
            print(f"   ... è¿˜æœ‰ {len(high_sim_pairs) - 10} å¯¹æœªæ˜¾ç¤º")
    
    print("\n" + "=" * 80 + "\n")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°JSON
    base_dir = Path(__file__).resolve().parents[1]
    out_dir = base_dir / 'zzManuscript' / 'Figure'
    out_dir.mkdir(parents=True, exist_ok=True)
    
    stats = {
        "max_off_diag_nc": max_off_diag_nc,
        "min_off_diag_nc": min_off_diag_nc,
        "mean_off_diag_nc": mean_off_diag_nc,
        "std_off_diag_nc": std_off_diag_nc,
        "median_off_diag_nc": median_off_diag_nc,
        "pairs_ge_0.80": pairs_ge_080,
        "pairs_ge_0.82": pairs_ge_082,
        "pairs_ge_0.85": pairs_ge_085,
        "pairs_ge_0.90": pairs_ge_090,
        "total_pairs": total_pairs,
        "threshold_ok": uniqueness_ok,
        "threshold_uniqueness": threshold_uniqueness,
        "high_similarity_pairs": [
            {"label1": label1, "label2": label2, "nc": float(nc_val)}
            for label1, label2, nc_val in high_sim_pairs[:20]
        ]
    }
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°ä¸¤ä¸ªä½ç½®
    stats_path = out_dir / 'nc_uniqueness_stats.json'
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    print(f"âœ“ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_path}")
    
    # åŒæ—¶ä¿å­˜åˆ° zNC-Test æ–‡ä»¶å¤¹
    script_dir = Path(__file__).resolve().parent
    local_stats_path = script_dir / 'nc_uniqueness_stats.json'
    with open(local_stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    print(f"âœ“ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {local_stats_path}")
    
    # ä¿å­˜NCçŸ©é˜µä¸ºCSVæ–‡ä»¶ï¼ˆçŸ­æ ‡ç­¾ï¼‰ä¸å…¨ç§°CSV
    nc_df = pd.DataFrame(nc_matrix, index=found_labels, columns=found_labels)
    csv_path = script_dir / 'NC_Matrix.csv'
    nc_df.to_csv(csv_path, float_format='%.6f')
    print(f"âœ“ NCçŸ©é˜µCSVå·²ä¿å­˜: {csv_path}")

    # å…¨ç§°ç‰ˆæœ¬ï¼ˆä½¿ç”¨åŸå§‹æ–‡ä»¶åŸºåï¼Œä¾¿äºå®Œæ•´æ˜¾ç¤ºï¼‰
    nc_df_full = pd.DataFrame(nc_matrix, index=found_files, columns=found_files)
    csv_full_path = script_dir / 'NC_Matrix_full.csv'
    nc_df_full.to_csv(csv_full_path, float_format='%.6f')
    print(f"âœ“ NCçŸ©é˜µå…¨ç§°CSVå·²ä¿å­˜: {csv_full_path}")

    # ä¿å­˜æ ‡ç­¾æ˜ å°„ï¼ˆçŸ­æ ‡ç­¾ -> å…¨ç§°ï¼‰
    mapping_path = script_dir / 'label_mapping.json'
    mapping = [{"short": s, "full": f} for s, f in zip(found_labels, found_files)]
    with open(mapping_path, 'w', encoding='utf-8') as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)
    print(f"âœ“ æ ‡ç­¾æ˜ å°„å·²ä¿å­˜: {mapping_path}")
    
    # ç”Ÿæˆçƒ­åŠ›å›¾ï¼ˆç»Ÿä¸€é£æ ¼ï¼šæ— æ ‡é¢˜ï¼Œæ ‡æ³¨æ–‡å­—ä¸ºé»‘è‰²ï¼‰
    # ä½¿ç”¨â€œå…¨ç§°â€æ ‡ç­¾ï¼ˆä¸å†ç¼©å†™ï¼‰ï¼Œç›´æ¥å–åŸå§‹æ–‡ä»¶åŸºåä»¥ä¾¿åœ¨çƒ­åŠ›å›¾ä¸Šå®Œæ•´å±•ç¤º
    display_labels = found_files
    fig, ax = plt.subplots(figsize=(10, 8))
    im = ax.imshow(nc_matrix, cmap='viridis', vmin=0, vmax=1, aspect='auto')
    
    # æ·»åŠ colorbar
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('NC Value', rotation=270, labelpad=20)
    
    # è®¾ç½®åˆ»åº¦ï¼ˆä½¿ç”¨å…¨ç§°æ ‡ç­¾ä»¥ä¾¿åœ¨çƒ­åŠ›å›¾ä¸Šå®Œæ•´å±•ç¤ºçŸ¢é‡å›¾åï¼‰
    ax.set_xticks(np.arange(len(display_labels)))
    ax.set_yticks(np.arange(len(display_labels)))
    ax.set_xticklabels(display_labels)
    ax.set_yticklabels(display_labels)
    
    # æ—‹è½¬xè½´æ ‡ç­¾
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨ï¼ˆç»Ÿä¸€ä¸ºé»‘è‰²å­—ä½“ï¼‰
    for i in range(len(found_labels)):
        for j in range(len(found_labels)):
            text = ax.text(j, i, f'{nc_matrix[i, j]:.3f}',
                          ha="center", va="center", 
                          color="black",
                          fontsize=11)
    
    plt.tight_layout()
    
    # ä¿å­˜çƒ­åŠ›å›¾åˆ°ä¸¤ä¸ªä½ç½®
    output_path = out_dir / 'NC_Matrix_Heatmap.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ çƒ­åŠ›å›¾å·²ä¿å­˜: {output_path}")
    
    local_heatmap_path = script_dir / 'NC_Matrix_Heatmap.png'
    plt.savefig(local_heatmap_path, dpi=300, bbox_inches='tight')
    print(f"âœ“ çƒ­åŠ›å›¾å·²ä¿å­˜: {local_heatmap_path}")
    plt.close()

if __name__ == '__main__':
    analyze_nc_uniqueness()
