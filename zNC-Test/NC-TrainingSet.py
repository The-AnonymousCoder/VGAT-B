#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é²æ£’æ€§éªŒè¯è„šæœ¬
ä½¿ç”¨è¢«æ”»å‡»çš„çŸ¢é‡åœ°å›¾éªŒè¯é›¶æ°´å°çš„é²æ£’æ€§
åªæµ‹è¯•è®­ç»ƒé›†ï¼Œæ¯ä¸ªå›¾éƒ½ä½¿ç”¨å¯¹åº”çš„é›¶æ°´å°
"""

import os
import pickle
import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime
import shutil

# è®¾ç½®ä¸­æ–‡å­—ä½“ - é€‚é…Windowsç¯å¢ƒ
try:
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'PingFang SC', 'Heiti SC', 'Songti SC']
    plt.rcParams['axes.unicode_minus'] = False
except:
    # å¦‚æœä¸­æ–‡å­—ä½“ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤å­—ä½“
    plt.rcParams['font.family'] = 'DejaVu Sans'

class RobustnessVerifier:
    """é²æ£’æ€§éªŒè¯å™¨"""
    
    def __init__(self, model_path=None, use_trained_model=True):
        # æ¨¡å‹è·¯å¾„æ”¹ä¸ºVGAT/models/gat_model_IMPROVED_best.pthï¼ˆä½¿ç”¨æ”¹è¿›ç‰ˆæ¨¡å‹ï¼‰
        if model_path is None:
            model_path = os.path.normpath(os.path.join(os.path.dirname(__file__), '..', 'VGAT', 'models', 'gat_model_IMPROVED_best.pth'))
        self.model_path = model_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        self.model = None
        self.use_trained_model = use_trained_model
        if use_trained_model:
            self.load_model()
        
        # åˆå§‹åŒ–æ”»å‡»ç±»å‹æ˜ å°„å­—å…¸ï¼ˆåŸºäºattack200.pyï¼‰
        self.attack_type_mapping = self._create_attack_type_mapping()
    
    def _create_attack_type_mapping(self):
        """åˆ›å»ºæ”»å‡»ç±»å‹æ˜ å°„å­—å…¸ï¼ˆåŸºäºattack200.pyï¼‰"""
        mapping = {}
        
        # åŸºç¡€æ”»å‡»ç±»å‹ï¼ˆå‰54ä¸ªï¼ŒåŸºäºattack200.pyï¼‰
        basic_attacks = {
            "delete_10pct_vertices": "åˆ é™¤10%é¡¶ç‚¹",
            "delete_20pct_vertices": "åˆ é™¤20%é¡¶ç‚¹", 
            "delete_30pct_vertices": "åˆ é™¤30%é¡¶ç‚¹",
            "delete_40pct_vertices": "åˆ é™¤40%é¡¶ç‚¹",
            "delete_50pct_vertices": "åˆ é™¤50%é¡¶ç‚¹",
            "delete_10pct_objects": "åˆ é™¤10%å›¾å½¢å¯¹è±¡",
            "delete_20pct_objects": "åˆ é™¤20%å›¾å½¢å¯¹è±¡",
            "delete_30pct_objects": "åˆ é™¤30%å›¾å½¢å¯¹è±¡",
            "delete_40pct_objects": "åˆ é™¤40%å›¾å½¢å¯¹è±¡",
            "delete_50pct_objects": "åˆ é™¤50%å›¾å½¢å¯¹è±¡",
            "add_10pct_vertices": "æ·»åŠ 10%é¡¶ç‚¹",
            "add_20pct_vertices": "æ·»åŠ 20%é¡¶ç‚¹",
            "add_30pct_vertices": "æ·»åŠ 30%é¡¶ç‚¹",
            "add_40pct_vertices": "æ·»åŠ 40%é¡¶ç‚¹",
            "add_50pct_vertices": "æ·»åŠ 50%é¡¶ç‚¹",
            "noise_10pct_strength_0.2": "å™ªå£°æ‰°åŠ¨10%é¡¶ç‚¹ï¼Œå¼ºåº¦0.2",
            "noise_10pct_strength_0.4": "å™ªå£°æ‰°åŠ¨10%é¡¶ç‚¹ï¼Œå¼ºåº¦0.4",
            "noise_10pct_strength_0.6": "å™ªå£°æ‰°åŠ¨10%é¡¶ç‚¹ï¼Œå¼ºåº¦0.6",
            "noise_20pct_strength_0.3": "å™ªå£°æ‰°åŠ¨20%é¡¶ç‚¹ï¼Œå¼ºåº¦0.3",
            "noise_30pct_strength_0.8": "å™ªå£°æ‰°åŠ¨30%é¡¶ç‚¹ï¼Œå¼ºåº¦0.8",
            "crop_x_center_50pct": "æ²¿Xè½´ä¸­å¿ƒè£å‰ª50%",
            "crop_y_center_50pct": "æ²¿Yè½´ä¸­å¿ƒè£å‰ª50%",
            "crop_top_left": "è£å‰ªå·¦ä¸Šè§’åŒºåŸŸ",
            "crop_bottom_right": "è£å‰ªå³ä¸‹è§’åŒºåŸŸ",
            "crop_random_40pct": "éšæœºè£å‰ª40%",
            "translate_10_10": "Xè½´å³ç§»10å•ä½ï¼ŒYè½´ä¸Šç§»10å•ä½",
            "translate_20_20": "Xè½´å³ç§»20å•ä½ï¼ŒYè½´ä¸Šç§»20å•ä½",
            "translate_x_30": "ä»…Xè½´å³ç§»30å•ä½",
            "translate_y_15": "ä»…Yè½´ä¸Šç§»15å•ä½",
            "translate_neg10": "Xè½´å·¦ç§»10å•ä½ï¼ŒYè½´ä¸‹ç§»10å•ä½",
            "scale_0.5x": "ç¼©æ”¾0.5å€",
            "scale_2x": "ç¼©æ”¾2å€",
            "scale_x0.5_y2": "Xè½´ç¼©å°0.5å€ï¼ŒYè½´æ”¾å¤§2å€",
            "scale_x2_y0.5": "Xè½´æ”¾å¤§2å€ï¼ŒYè½´ç¼©å°0.5å€",
            "scale_random": "éšæœºç¼©æ”¾",
            "rotate_45": "æ—‹è½¬45åº¦",
            "rotate_90": "æ—‹è½¬90åº¦",
            "rotate_135": "æ—‹è½¬135åº¦",
            "rotate_180": "æ—‹è½¬180åº¦",
            "rotate_random": "éšæœºæ—‹è½¬",
            "flip_x": "Xè½´é•œåƒç¿»è½¬",
            "flip_y": "Yè½´é•œåƒç¿»è½¬",
            "flip_xy": "åŒæ—¶Xã€Yè½´é•œåƒç¿»è½¬",
            "reverse_vertex_order": "åè½¬é¡¶ç‚¹é¡ºåº",
            "reverse_object_order": "åè½¬å¯¹è±¡é¡ºåº",
            "shuffle_objects": "æ‰“ä¹±å¯¹è±¡é¡ºåº",
            "shuffle_vertices": "æ‰“ä¹±é¡¶ç‚¹é¡ºåº",
            "jitter_vertices": "é¡¶ç‚¹é¡ºåºéšæœºåç§»",
            "merge_objects": "åˆå¹¶å¯¹è±¡",
            "split_objects": "æ‹†åˆ†å¯¹è±¡",
        }
        
        # æ·»åŠ åŸºç¡€æ”»å‡»ç±»å‹
        for attack_key, description in basic_attacks.items():
            mapping[attack_key] = description
        
        # æ·»åŠ æ‰©å±•æ”»å‡»ç±»å‹ï¼ˆattack_055åˆ°attack_100ï¼‰
        for i in range(55, 101):
            mapping[f"attack_{i:03d}"] = f"æ‰©å±•æ”»å‡»ç­–ç•¥{i}"
        
        # æ·»åŠ ç»„åˆæ”»å‡»ç±»å‹ï¼ˆcombo_attack_001åˆ°combo_attack_100ï¼‰
        for i in range(1, 101):
            mapping[f"combo_attack_{i:03d}"] = f"ç»„åˆæ”»å‡»ç­–ç•¥{i}"
        
        return mapping
    
    def get_attack_type_description(self, filename):
        """æ ¹æ®æ–‡ä»¶åè·å–æ”»å‡»ç±»å‹æè¿°"""
        # ç§»é™¤æ–‡ä»¶æ‰©å±•åå’Œå›¾åå‰ç¼€
        base_name = filename.replace('_graph.pkl', '').replace('.geojson', '')
        
        # ç§»é™¤å›¾åå‰ç¼€ï¼ˆå¦‚Boundary_ã€Building_ç­‰ï¼‰
        for prefix in ['Boundary_', 'Building_', 'Lake_', 'Landuse_', 'Railways_', 'Road_']:
            if base_name.startswith(prefix):
                base_name = base_name[len(prefix):]
                break
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ”»å‡»ç±»å‹
        for attack_key, description in self.attack_type_mapping.items():
            if attack_key in base_name:
                return description
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ï¼Œè¿”å›æ–‡ä»¶å
        return base_name
    
    def load_model(self):
        """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆå¼ºåˆ¶ï¼Œå¤±è´¥å³æŠ¥é”™ï¼‰"""
        try:
            print("åŠ è½½è®­ç»ƒå¥½çš„æ”¹è¿›ç‰ˆGATæ¨¡å‹...")
            if os.path.exists(self.model_path):
                # å¯¼å…¥æ¨¡å‹ç±»ï¼ˆä½¿ç”¨importlibå› ä¸ºæ–‡ä»¶ååŒ…å«è¿å­—ç¬¦ï¼‰
                import sys
                import importlib.util
                
                base_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), '..'))
                vgat_improved_path = os.path.join(base_dir, 'VGAT', 'VGAT-IMPROVED.py')
                
                # åŠ¨æ€åŠ è½½æ¨¡å—
                spec = importlib.util.spec_from_file_location("vgat_improved", vgat_improved_path)
                vgat_module = importlib.util.module_from_spec(spec)
                sys.modules['vgat_improved'] = vgat_module
                spec.loader.exec_module(vgat_module)
                
                ImprovedGATModel = vgat_module.ImprovedGATModel
                
                # ä½¿ç”¨IMPROVEDç‰ˆæœ¬çš„æ¨¡å‹å‚æ•°ï¼šinput_dim=20, hidden_dim=256, num_heads=8
                self.model = ImprovedGATModel(input_dim=20, hidden_dim=256, output_dim=1024, num_heads=8, dropout=0.3)
                
                # åŠ è½½æƒé‡
                checkpoint = torch.load(self.model_path, map_location=self.device)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.model.to(self.device)
                self.model.eval()
                
                print("æ”¹è¿›ç‰ˆGATæ¨¡å‹åŠ è½½å®Œæˆ")
                print(f"  è¾“å…¥ç»´åº¦: 20 (20ç»´å‡ ä½•ä¸å˜ç‰¹å¾)")
                print(f"  éšè—ç»´åº¦: 256")
                print(f"  è¾“å‡ºç»´åº¦: 1024")
                print(f"  æ³¨æ„åŠ›å¤´æ•°: 8")
            else:
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def extract_robust_features(self, graph_data):
        """æå–é²æ£’ç‰¹å¾ï¼ˆå¼ºåˆ¶ä½¿ç”¨è®­ç»ƒæ¨¡å‹ï¼‰"""
        if not hasattr(graph_data, 'x') or not hasattr(graph_data, 'edge_index'):
            raise ValueError("è¾“å…¥å›¾æ•°æ®æ— æ•ˆï¼Œç¼ºå°‘å¿…è¦å±æ€§ 'x' æˆ– 'edge_index'")
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œæ— æ³•æå–é²æ£’ç‰¹å¾")

        with torch.no_grad():
            features = self.model(graph_data.x.to(self.device), graph_data.edge_index.to(self.device))
            features = features.cpu().numpy()

        # æ–°æ¨¡å‹ç›´æ¥è¾“å‡º1024ç»´ç‰¹å¾ï¼Œä¸éœ€è¦æ‰©å±•
        # ç¡®ä¿ç‰¹å¾ç»´åº¦æ­£ç¡®
        if len(features) != 1024:
            print(f"è­¦å‘Šï¼šæ¨¡å‹è¾“å‡ºç‰¹å¾ç»´åº¦ä¸º{len(features)}ï¼ŒæœŸæœ›1024ç»´")
            # å¦‚æœç»´åº¦ä¸å¯¹ï¼Œè¿›è¡Œé€‚å½“è°ƒæ•´
            if len(features) < 1024:
                features = np.tile(features, (1024 // len(features) + 1,))
            features = features[:1024]
        
        return features
    
    def load_copyright_image(self, image_path=None):
        """åŠ è½½ç‰ˆæƒå›¾åƒ"""
        try:
            if image_path is None:
                # ä½¿ç”¨ ZeroWatermark/Cat32.pngï¼ˆè„šæœ¬ç›¸å¯¹è·¯å¾„ï¼‰
                image_path = os.path.normpath(os.path.join(os.path.dirname(__file__), '..', 'ZeroWatermark', 'Cat32.png'))
            image = Image.open(image_path)
            image = image.convert('L')
            image = image.resize((32, 32))  # è°ƒæ•´å¤§å°ä¸º32x32
            threshold = 128
            image = image.point(lambda x: 0 if x < threshold else 255, '1')
            return np.array(image)
        except Exception as e:
            print(f"åŠ è½½ç‰ˆæƒå›¾åƒå¤±è´¥: {e}")
            return np.random.randint(0, 2, (32, 32))
    
    def features_to_matrix(self, features, target_shape):
        """å°†ç‰¹å¾å‘é‡è½¬æ¢ä¸ºçŸ©é˜µï¼ˆä½¿ç”¨ä¸­ä½æ•°é˜ˆå€¼äºŒå€¼åŒ–ï¼‰"""
        total_elements = target_shape[0] * target_shape[1]
        
        # å¦‚æœç‰¹å¾æ•°é‡ä¸è¶³ï¼Œé‡å¤å¡«å……
        if len(features) < total_elements:
            features = np.tile(features, (total_elements // len(features) + 1,))
        
        # å–å‰total_elementsä¸ªå…ƒç´ 
        features = features[:total_elements]
        
        # é‡å¡‘ä¸ºç›®æ ‡å½¢çŠ¶
        matrix = features.reshape(target_shape)
        
        # äºŒå€¼åŒ–ï¼ˆä¸­ä½æ•°é˜ˆå€¼ä¸ç¬¬4æ­¥ä¿æŒä¸€è‡´ï¼‰
        threshold = np.median(matrix)
        matrix = (matrix > threshold).astype(np.uint8)
        
        return matrix
    
    def verify_copyright(self, graph_data, zero_watermark, original_copyright):
        """éªŒè¯ç‰ˆæƒ"""
        # æå–é²æ£’ç‰¹å¾
        robust_features = self.extract_robust_features(graph_data)
        
        # å°†ç‰¹å¾è½¬æ¢ä¸ºçŸ©é˜µ
        feature_matrix = self.features_to_matrix(robust_features, original_copyright.shape)
        
        # ä»é›¶æ°´å°ä¸­æå–ç‰ˆæƒå›¾åƒ
        extracted_copyright = np.logical_xor(zero_watermark, feature_matrix).astype(np.uint8)
        
        # è®¡ç®—NCå€¼ï¼ˆå½’ä¸€åŒ–ç›¸å…³ç³»æ•°ï¼‰
        nc_value = self.calculate_nc(original_copyright, extracted_copyright)
        
        return extracted_copyright, nc_value
    
    def calculate_nc(self, original, extracted):
        """è®¡ç®—å½’ä¸€åŒ–ç›¸å…³ç³»æ•°ï¼ˆNCå€¼ï¼‰"""
        # å°†å›¾åƒè½¬æ¢ä¸ºå‘é‡
        original_vec = original.flatten().astype(float)
        extracted_vec = extracted.flatten().astype(float)
        
        # è®¡ç®—å½’ä¸€åŒ–ç›¸å…³ç³»æ•°
        # NC = (AÂ·B) / (||A||Â·||B||)
        # å…¶ä¸­ AÂ·B æ˜¯ç‚¹ç§¯ï¼Œ||A|| å’Œ ||B|| æ˜¯å‘é‡çš„æ¨¡é•¿
        dot_product = np.sum(original_vec * extracted_vec)
        norm_original = np.sqrt(np.sum(original_vec ** 2))
        norm_extracted = np.sqrt(np.sum(extracted_vec ** 2))
        
        if norm_original == 0 or norm_extracted == 0:
            return 0.0
        
        nc = dot_product / (norm_original * norm_extracted)
        
        return nc
    
    def load_watermark(self, filename, watermark_dir=None):
        """åŠ è½½å¯¹åº”çš„é›¶æ°´å°"""
        if watermark_dir is None:
            # ZeroWatermark/ZeroWatermark/TrainingSet ä¸‹
            watermark_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), '..', 'ZeroWatermark', 'ZeroWatermark', 'TrainingSet'))
        watermark_path = os.path.join(watermark_dir, f"{filename}_watermark.npy")
        if os.path.exists(watermark_path):
            return np.load(watermark_path)
        else:
            print(f"é›¶æ°´å°æ–‡ä»¶ä¸å­˜åœ¨: {watermark_path}")
            return None
    
    def verify_robustness(self, original_graph, attacked_graphs, zero_watermark, copyright_image, filename, attacked_filenames=None):
        """éªŒè¯é²æ£’æ€§"""
        print(f"éªŒè¯é²æ£’æ€§: {filename}")
        
        results = []
        
        # å¯¹æ¯ä¸ªè¢«æ”»å‡»çš„å›¾è¿›è¡ŒéªŒè¯
        for i, attacked_graph in enumerate(attacked_graphs):
            # éªŒè¯ç‰ˆæƒ
            extracted_copyright, nc_value = self.verify_copyright(
                attacked_graph, zero_watermark, copyright_image
            )
            
            # è·å–æ”»å‡»ç±»å‹æè¿°
            attack_filename = attacked_filenames[i] if attacked_filenames and i < len(attacked_filenames) else f"attack_{i+1}"
            attack_description = self.get_attack_type_description(attack_filename)
            
            results.append({
                'attack_index': i,
                'attack_filename': attack_filename,
                'attack_description': attack_description,
                'nc_value': nc_value,
                'extracted_copyright': extracted_copyright
            })
            
            print(f"  æ”»å‡» {i+1}: NCå€¼ = {nc_value:.4f}")
        
        return results
    
    def save_robustness_results(self, results, zero_watermark, copyright_image, filename, results_dir=None):
        """ä¿å­˜é²æ£’æ€§éªŒè¯ç»“æœ"""
        if results_dir is None:
            # è®­ç»ƒé›†ç»“æœç›®å½•ï¼šzNC-Test/NC-Results/TrainingSet
            results_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), 'NC-Results', 'TrainingSet'))
        if not os.path.exists(results_dir):
            os.makedirs(results_dir)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        nc_values = [r['nc_value'] for r in results]
        avg_nc = np.mean(nc_values)
        max_nc = np.max(nc_values)
        min_nc = np.min(nc_values)
        
        # ä¿å­˜ç»“æœå›¾åƒ
        plt.figure(figsize=(15, 10))
        
        # ç¬¬ä¸€è¡Œï¼šåŸå§‹å›¾åƒ
        plt.subplot(3, 4, 1)
        plt.imshow(zero_watermark, cmap='gray')
        plt.title('Zero Watermark')
        plt.axis('off')
        
        plt.subplot(3, 4, 2)
        plt.imshow(copyright_image, cmap='gray')
        plt.title('Original Copyright')
        plt.axis('off')
        
        # ç¬¬äºŒè¡Œï¼šå‰6ä¸ªæ”»å‡»ç»“æœ
        for i in range(min(6, len(results))):
            plt.subplot(3, 4, i + 7)
            plt.imshow(results[i]['extracted_copyright'], cmap='gray')
            plt.title(f'Attack {i+1}\nNC: {results[i]["nc_value"]:.4f}')
            plt.axis('off')
        
        # ç¬¬ä¸‰è¡Œï¼šç»Ÿè®¡ä¿¡æ¯
        plt.subplot(3, 4, 11)
        plt.text(0.5, 0.5, f'Average NC: {avg_nc:.4f}\nMax NC: {max_nc:.4f}\nMin NC: {min_nc:.4f}', 
                horizontalalignment='center', verticalalignment='center',
                transform=plt.gca().transAxes, fontsize=10)
        plt.title('Statistics')
        plt.axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, f'{filename}_robustness_results.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶
        stats_path = os.path.join(results_dir, f'{filename}_robustness_stats.txt')
        with open(stats_path, 'w') as f:
            f.write(f"é²æ£’æ€§éªŒè¯ç»“æœ: {filename}\n")
            f.write(f"å¹³å‡NCå€¼: {avg_nc:.4f}\n")
            f.write(f"æœ€é«˜NCå€¼: {max_nc:.4f}\n")
            f.write(f"æœ€ä½NCå€¼: {min_nc:.4f}\n")
            f.write(f"éªŒè¯æˆåŠŸæ•°é‡ (NC > 0.7): {sum(1 for nc in nc_values if nc > 0.7)}/{len(nc_values)}\n")
            f.write("\nè¯¦ç»†ç»“æœ:\n")
            for i, result in enumerate(results):
                f.write(f"æ”»å‡» {i+1}: NCå€¼ = {result['nc_value']:.4f}\n")
        
        print(f"é²æ£’æ€§éªŒè¯ç»“æœå·²ä¿å­˜: {results_dir}")
        return avg_nc, max_nc, min_nc
    
    def save_excel_results(self, all_results, results_dir=None):
        """åªä¿å­˜summary.csvï¼ˆç»Ÿè®¡æ±‡æ€»ï¼‰"""
        if results_dir is None:
            results_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), 'NC-Results', 'TrainingSet'))
        if not os.path.exists(results_dir):
            os.makedirs(results_dir)
        
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        
        # ğŸ†• ä¿å­˜ç»Ÿè®¡æ±‡æ€»CSVï¼ˆæ¯ä¸ªå›¾çš„å¹³å‡å€¼ï¼‰
        summary_data = []
        for graph_name, results in all_results.items():
            nc_values = [r['nc_value'] for r in results]
            summary_data.append({
                'å›¾åç§°': graph_name,
                'æ”»å‡»æ•°é‡': len(nc_values),
                'å¹³å‡NCå€¼': np.mean(nc_values),
                'æœ€å¤§NCå€¼': np.max(nc_values),
                'æœ€å°NCå€¼': np.min(nc_values),
                'æ ‡å‡†å·®': np.std(nc_values)
            })
        
        # â­ æ·»åŠ æ€»ä½“ç»Ÿè®¡è¡Œ
        if summary_data:
            all_avg_nc = [d['å¹³å‡NCå€¼'] for d in summary_data]
            all_max_nc = [d['æœ€å¤§NCå€¼'] for d in summary_data]
            all_min_nc = [d['æœ€å°NCå€¼'] for d in summary_data]
            all_std = [d['æ ‡å‡†å·®'] for d in summary_data]
            total_attacks = sum(d['æ”»å‡»æ•°é‡'] for d in summary_data)
            
            summary_data.append({
                'å›¾åç§°': 'Overall Average',
                'æ”»å‡»æ•°é‡': total_attacks,
                'å¹³å‡NCå€¼': np.mean(all_avg_nc),
                'æœ€å¤§NCå€¼': np.mean(all_max_nc),
                'æœ€å°NCå€¼': np.mean(all_min_nc),
                'æ ‡å‡†å·®': np.mean(all_std)
            })
        
        df_summary = pd.DataFrame(summary_data)
        csv_summary_path = os.path.join(results_dir, f'train_set_nc_summary_{timestamp}.csv')
        df_summary.to_csv(csv_summary_path, index=False, encoding='utf-8-sig')
        print(f"CSVç»Ÿè®¡æ±‡æ€»å·²ä¿å­˜: {csv_summary_path}")
        
        return csv_summary_path

class GraphDataLoader:
    """å›¾æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, graph_dir=None):
        # è®­ç»ƒé›†å›¾æ•°æ®æ ¹ç›®å½•ï¼ˆä½¿ç”¨ç»å¯¹è·¯å¾„ï¼‰
        if graph_dir is None:
            base_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), '..'))
            graph_dir = os.path.join(base_dir, 'convertToGraph', 'Graph', 'TrainingSet')
        self.graph_dir = os.path.normpath(graph_dir)
        print(f"å›¾æ•°æ®åŠ è½½è·¯å¾„: {self.graph_dir}")
    
    def load_original_graphs_train_only(self):
        """åªåŠ è½½è®­ç»ƒé›†çš„åŸå§‹å›¾æ•°æ®"""
        original_dir = os.path.join(self.graph_dir, 'Original')
        
        if not os.path.exists(original_dir):
            print(f"ç›®å½•ä¸å­˜åœ¨: {original_dir}")
            return [], []
        
        graphs = []
        filenames = []
        
        # åŠ è½½æ‰€æœ‰åŸå§‹å›¾ï¼ˆè®­ç»ƒé›†ï¼‰
        for filename in os.listdir(original_dir):
            if filename.endswith('_graph.pkl'):
                try:
                    with open(os.path.join(original_dir, filename), 'rb') as f:
                        graph_data = pickle.load(f)
                        graphs.append(graph_data)
                        base_name = filename.replace('_graph.pkl', '')
                        filenames.append(base_name)
                        print(f"æˆåŠŸåŠ è½½è®­ç»ƒå›¾æ•°æ®: {filename}")
                except Exception as e:
                    print(f"åŠ è½½å›¾æ•°æ®å¤±è´¥ {filename}: {e}")
                    continue
        
        print(f"æ€»å…±åŠ è½½äº† {len(graphs)} ä¸ªè®­ç»ƒå›¾æ•°æ®")
        return graphs, filenames
    
    def load_attacked_graphs_for_original(self, original_filename):
        """åŠ è½½æŒ‡å®šåŸå§‹å›¾å¯¹åº”çš„æ‰€æœ‰è¢«æ”»å‡»å›¾"""
        attacked_dir = os.path.join(self.graph_dir, 'Attacked')
        
        if not os.path.exists(attacked_dir):
            print(f"è¢«æ”»å‡»å›¾ç›®å½•ä¸å­˜åœ¨: {attacked_dir}")
            return [], []
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„å­ç›®å½•
        subdir_path = os.path.join(attacked_dir, original_filename)
        if os.path.exists(subdir_path):
            # å¦‚æœå­˜åœ¨å­ç›®å½•ï¼Œä»å­ç›®å½•ä¸­åŠ è½½æ‰€æœ‰è¢«æ”»å‡»å›¾
            attacked_graphs = []
            attacked_filenames = []
            for filename in sorted(os.listdir(subdir_path)):  # æ’åºä»¥ä¿æŒä¸€è‡´æ€§
                if filename.endswith('_graph.pkl'):
                    try:
                        with open(os.path.join(subdir_path, filename), 'rb') as f:
                            graph_data = pickle.load(f)
                            attacked_graphs.append(graph_data)
                            attacked_filenames.append(filename)
                            print(f"æˆåŠŸåŠ è½½è¢«æ”»å‡»å›¾: {filename}")
                    except Exception as e:
                        print(f"åŠ è½½è¢«æ”»å‡»å›¾å¤±è´¥ {filename}: {e}")
                        continue
            
            print(f"ä¸º {original_filename} åŠ è½½äº† {len(attacked_graphs)} ä¸ªè¢«æ”»å‡»å›¾")
            return attacked_graphs, attacked_filenames
        else:
            print(f"æ²¡æœ‰æ‰¾åˆ° {original_filename} å¯¹åº”çš„è¢«æ”»å‡»å›¾ç›®å½•")
            return [], []

def main():
    """ä¸»å‡½æ•°"""
    print("="*70)
    print("ç¬¬äº”æ­¥ï¼šé²æ£’æ€§éªŒè¯ï¼ˆæ”¹è¿›ç‰ˆï¼‰")
    print("="*70)
    print("ä½¿ç”¨æ”¹è¿›çš„GATæ¨¡å‹ï¼ˆ20ç»´ç‰¹å¾ + ImprovedGATModelï¼‰")
    print("æ¨¡å‹é…ç½®ï¼šinput_dim=20, hidden_dim=256, num_heads=8")
    print("="*70)
    print()
    
    # æ¸…ç†å¹¶å‡†å¤‡ç»“æœè¾“å‡ºç›®å½•ï¼ˆç¡®ä¿æ¯æ¬¡è¿è¡Œå¯å®Œç¾æ›¿æ¢ï¼‰
    results_root = os.path.normpath(os.path.join(os.path.dirname(__file__), 'NC-Results', 'TrainingSet'))
    if os.path.exists(results_root):
        print(f"æ¸…ç†æ—§çš„ç»“æœç›®å½•: {results_root}")
        try:
            shutil.rmtree(results_root)
            print("[OK] æ—§ç»“æœå·²æ¸…ç†")
        except Exception as e:
            print(f"[WARNING] æ¸…ç†ç›®å½•æ—¶å‡ºé”™: {e}")
            print("å°è¯•ç»§ç»­...")
    os.makedirs(results_root, exist_ok=True)
    print()
    
    # åŠ è½½è®­ç»ƒé›†çš„åŸå§‹å›¾æ•°æ®
    data_loader = GraphDataLoader()
    original_graphs, filenames = data_loader.load_original_graphs_train_only()
    
    if not original_graphs:
        print("æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå›¾æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œç¬¬äºŒæ­¥")
        return
    
    # åˆ›å»ºé²æ£’æ€§éªŒè¯å™¨
    verifier = RobustnessVerifier()
    
    # åŠ è½½ç‰ˆæƒå›¾åƒ
    copyright_image = verifier.load_copyright_image()
    print(f"ç‰ˆæƒå›¾åƒå¤§å°: {copyright_image.shape}")
    
    # ä¸ºæ¯ä¸ªè®­ç»ƒå›¾éªŒè¯é²æ£’æ€§
    all_avg_nc = []
    all_max_nc = []
    all_min_nc = []
    all_excel_results = {}  # æ”¶é›†Excelæ•°æ®
    
    for i, (original_graph, filename) in enumerate(zip(original_graphs, filenames)):
        print(f"\nå¤„ç†ç¬¬ {i+1}/{len(original_graphs)} ä¸ªå›¾: {filename}")
        
        # åŠ è½½å¯¹åº”çš„é›¶æ°´å°
        zero_watermark = verifier.load_watermark(filename)
        if zero_watermark is None:
            print(f"è·³è¿‡ {filename}ï¼Œé›¶æ°´å°ä¸å­˜åœ¨")
            continue
        
        # åŠ è½½å¯¹åº”çš„è¢«æ”»å‡»å›¾
        attacked_graphs, attacked_filenames = data_loader.load_attacked_graphs_for_original(filename)
        
        if not attacked_graphs:
            print(f"è·³è¿‡ {filename}ï¼Œæ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„è¢«æ”»å‡»å›¾")
            continue
        
        # éªŒè¯é²æ£’æ€§
        results = verifier.verify_robustness(
            original_graph, attacked_graphs, zero_watermark, copyright_image, filename, attacked_filenames
        )
        
        # æ”¶é›†Excelæ•°æ®
        all_excel_results[filename] = results
        
        # ä¿å­˜ç»“æœ
        avg_nc, max_nc, min_nc = verifier.save_robustness_results(
            results, zero_watermark, copyright_image, filename
        )
        
        all_avg_nc.append(avg_nc)
        all_max_nc.append(max_nc)
        all_min_nc.append(min_nc)
    
    # è¾“å‡ºæ€»ä½“ç»“æœ
    if all_avg_nc:

        
        # æŒ‰æ”»å‡»ç±»å‹ç»Ÿè®¡ï¼ˆä»Excelç»“æœä¸­æå–ï¼‰
        if all_excel_results:
            print(f"\næŒ‰æ”»å‡»ç±»å‹ç»Ÿè®¡:")
            print("-" * 50)
            
            # æ”¶é›†æ‰€æœ‰æ”»å‡»ç±»å‹çš„NCå€¼
            attack_nc_values = {}
            for filename, results in all_excel_results.items():
                for result in results:
                    attack_desc = result.get('attack_description', 'æœªçŸ¥æ”»å‡»')
                    nc_value = result.get('nc_value', 0)
                    if attack_desc not in attack_nc_values:
                        attack_nc_values[attack_desc] = []
                    attack_nc_values[attack_desc].append(nc_value)
            
            # è®¡ç®—æ¯ç§æ”»å‡»ç±»å‹çš„ç»Ÿè®¡
            for attack_type, nc_values in attack_nc_values.items():
                avg_nc = np.mean(nc_values)
                max_nc = np.max(nc_values)
                min_nc = np.min(nc_values)
                success_count = sum(1 for nc in nc_values if nc > 0.7)
                success_rate = (success_count / len(nc_values)) * 100
                
                print(f"{attack_type}:")
                print(f"  æµ‹è¯•æ•°é‡: {len(nc_values)}")
                print(f"  æˆåŠŸéªŒè¯æ•°: {success_count}")
                print(f"  æˆåŠŸç‡: {success_rate:.2f}%")
                print(f"  å¹³å‡NCå€¼: {avg_nc:.4f}")
                print(f"  æœ€å¤§NCå€¼: {max_nc:.4f}")
                print(f"  æœ€å°NCå€¼: {min_nc:.4f}")
                print()
        
        print(f"\n{'='*70}")
        print(f"è®­ç»ƒé›†NCå€¼éªŒè¯æ€»ä½“ç»“æœ")
        print(f"{'='*70}")
        print(f"å¤„ç†çš„å›¾æ•°é‡: {len(all_avg_nc)}")
        
        # æŒ‰åŸå§‹åœ°å›¾ç»Ÿè®¡
        print(f"\næŒ‰åŸå§‹åœ°å›¾ç»Ÿè®¡:")
        print("-" * 50)
        for i, filename in enumerate(filenames):
            if i < len(all_avg_nc):
                avg_nc = all_avg_nc[i]
                max_nc = all_max_nc[i]
                min_nc = all_min_nc[i]
                success_status = "æˆåŠŸ" if avg_nc > 0.7 else "å¤±è´¥"
                
                print(f"{filename}:")
                print(f"  å¹³å‡NCå€¼: {avg_nc:.4f}")
                print(f"  æœ€å¤§NCå€¼: {max_nc:.4f}")
                print(f"  æœ€å°NCå€¼: {min_nc:.4f}")
                print(f"  éªŒè¯çŠ¶æ€: {success_status}")
        
        # æ€»ä½“ç»Ÿè®¡
        print(f"\næ€»ä½“ç»Ÿè®¡:")
        print("-" * 50)
        overall_avg_nc = np.mean(all_avg_nc)
        overall_max_nc = np.mean(all_max_nc)
        overall_min_nc = np.mean(all_min_nc)
        overall_std_nc = np.std(all_avg_nc)
        
        print(f"æ€»ä½“å¹³å‡NCå€¼: {overall_avg_nc:.4f}")
        print(f"æ€»ä½“æœ€å¤§NCå€¼: {overall_max_nc:.4f}")
        print(f"æ€»ä½“æœ€å°NCå€¼: {overall_min_nc:.4f}")
        print(f"æ€»ä½“NCå€¼æ ‡å‡†å·®: {overall_std_nc:.4f}")
        
        # æˆåŠŸç‡ç»Ÿè®¡
        success_count = sum(1 for avg_nc in all_avg_nc if avg_nc > 0.7)
        success_rate = (success_count / len(all_avg_nc)) * 100
        
        print(f"\næˆåŠŸç‡ç»Ÿè®¡:")
        print("-" * 50)
        print(f"æ€»éªŒè¯æ•°é‡: {len(all_avg_nc)}")
        print(f"æˆåŠŸéªŒè¯æ•°é‡: {success_count}")
        print(f"æ€»ä½“æˆåŠŸç‡: {success_rate:.2f}%")
        
        # éªŒè¯æˆåŠŸæ•°é‡ç»Ÿè®¡
        print(f"\néªŒè¯æˆåŠŸç»Ÿè®¡:")
        print("-" * 50)
        print(f"åœ°å›¾éªŒè¯æˆåŠŸæ•°é‡: {success_count}/{len(all_avg_nc)}")
        
        # ç”ŸæˆExcelè¡¨æ ¼
        if all_excel_results:
            verifier.save_excel_results(all_excel_results)
        
        print(f"\n{'='*70}")
        print(f"[OK] é²æ£’æ€§éªŒè¯å®Œæˆï¼")
        print(f"ç»“æœä¿å­˜ç›®å½•: {results_root}")
        print(f"{'='*70}")
    else:
        print("\n[WARNING] æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å›¾æ•°æ®")
        print(f"{'='*70}")

if __name__ == "__main__":
    main() 