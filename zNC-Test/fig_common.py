#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Fig1-Fig12 å…±äº«é€»è¾‘æ¨¡å—
æå–å…¬å…±å‡½æ•°ï¼Œé¿å…ä»£ç å†—ä½™

åŒ…å«å†…å®¹ï¼š
1. è·¯å¾„é…ç½®
2. 20ç»´ç‰¹å¾æå–å‡½æ•°
3. å›¾æ„å»ºå‡½æ•°ï¼ˆKNN + Delaunayï¼‰
4. æ¨¡å‹åŠ è½½å‡½æ•°
5. é›¶æ°´å°å·¥å…·å‡½æ•°ï¼ˆload_cat32, features_to_matrix, calc_ncï¼‰
6. ç»“æœä¿å­˜å‡½æ•°
"""

from pathlib import Path
import sys
from typing import List, Tuple, Optional
import pickle

import numpy as np
from shapely.geometry import Point  # type: ignore

try:
    import geopandas as gpd  # type: ignore
except Exception:
    gpd = None

try:
    from sklearn.preprocessing import StandardScaler  # type: ignore
    from sklearn.neighbors import NearestNeighbors  # type: ignore
except Exception:
    StandardScaler = None
    NearestNeighbors = None

try:
    from scipy.spatial import Delaunay  # type: ignore
except Exception:
    Delaunay = None

try:
    import torch  # type: ignore
    from torch_geometric.data import Data  # type: ignore
except Exception:
    Data = None
    torch = None

# ====================
# è·¯å¾„é…ç½®
# ====================

PROJECT_ROOT = Path(__file__).resolve().parents[1]
SCRIPT_DIR = Path(__file__).resolve().parent

# æ¨¡å‹å’Œèµ„æºè·¯å¾„
MODEL_PATH = PROJECT_ROOT / 'VGAT' / 'models' / 'gat_model_IMPROVED_best.pth'
CAT32_PATH = PROJECT_ROOT / 'ZeroWatermark' / 'Cat32.png'
GLOBAL_SCALER_PATH = PROJECT_ROOT / 'convertToGraph' / 'Graph' / 'TrainingSet' / 'global_scaler.pkl'

# é…ç½®å‚æ•°
K_FOR_KNN = 8  # KNNé‚»å±…æ•°ï¼ˆå·²å¼ƒç”¨ï¼Œæ”¹ç”¨è‡ªé€‚åº”Kå€¼ï¼‰

# å…¨å±€æ ‡å‡†åŒ–å™¨ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰
_global_scaler = None
_global_scaler_loaded = False


def adaptive_k_for_graph(n_nodes):
    """
    â­ æ ¹æ®èŠ‚ç‚¹æ•°è‡ªé€‚åº”ç¡®å®šKå€¼ï¼ˆä¸è®­ç»ƒé›†å®Œå…¨ä¸€è‡´ï¼‰
    
    å…¬å¼ï¼šK = min(round(2 * log10(n)) + 2, n-1)
    - KéšèŠ‚ç‚¹æ•°å¯¹æ•°å¢é•¿
    - é™åˆ¶èŒƒå›´ï¼š[1, min(12, n-1)]
    
    ç¤ºä¾‹ï¼š
    - n=1      â†’ K=1   (n-1)
    - n=2      â†’ K=1   (n-1)
    - n=3-7    â†’ K=min(4, n-1)
    - n=8-999  â†’ K=min(è®¡ç®—å€¼, n-1)
    - n=1,000  â†’ K=8   (2*3 + 2 = 8)
    - n=10,000 â†’ K=10  (2*4 + 2 = 10)
    - nâ‰¥100,000â†’ K=12  (è¾¾åˆ°ä¸Šé™)
    
    Args:
        n_nodes: å›¾çš„èŠ‚ç‚¹æ€»æ•°
        
    Returns:
        int: æ¨èçš„Kå€¼ï¼ˆèŒƒå›´1-12ï¼‰
    """
    if n_nodes < 2:
        return 1  # å•èŠ‚ç‚¹å›¾ï¼ŒK=1
    
    if n_nodes == 2:
        return 1  # 2ä¸ªèŠ‚ç‚¹ï¼ŒK=1
    
    # K = 2 * log10(n) + 2ï¼Œä½†ä¸è¶…è¿‡ n-1
    k = int(round(2 * np.log10(n_nodes) + 2))
    
    # é™åˆ¶èŒƒå›´ [1, min(12, n-1)]
    k = max(1, min(min(12, n_nodes - 1), k))
    
    return k


def load_global_scaler():
    """
    åŠ è½½è®­ç»ƒé›†çš„å…¨å±€æ ‡å‡†åŒ–å™¨ï¼ˆå¼ºåˆ¶è¦æ±‚ï¼‰
    
    Returns:
        StandardScaler: å…¨å±€æ ‡å‡†åŒ–å™¨
        
    Raises:
        FileNotFoundError: å¦‚æœæœªæ‰¾åˆ°å…¨å±€æ ‡å‡†åŒ–å™¨æ–‡ä»¶
        RuntimeError: å¦‚æœåŠ è½½å¤±è´¥æˆ–æ ‡å‡†åŒ–å™¨æ— æ•ˆ
    """
    global _global_scaler, _global_scaler_loaded
    
    # å¦‚æœå·²ç»å°è¯•è¿‡åŠ è½½ï¼Œç›´æ¥è¿”å›ç»“æœï¼ˆæˆ–æŠ›å‡ºä¹‹å‰çš„é”™è¯¯ï¼‰
    if _global_scaler_loaded:
        if _global_scaler is None:
            raise RuntimeError("å…¨å±€æ ‡å‡†åŒ–å™¨åŠ è½½å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
        return _global_scaler
    
    _global_scaler_loaded = True  # æ ‡è®°å·²å°è¯•åŠ è½½
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not GLOBAL_SCALER_PATH.exists():
        error_msg = (
            f"âŒ æœªæ‰¾åˆ°å…¨å±€æ ‡å‡†åŒ–å™¨æ–‡ä»¶ï¼\n"
            f"   è·¯å¾„: {GLOBAL_SCALER_PATH}\n"
            f"   \n"
            f"   è¯·å…ˆè¿è¡Œä»¥ä¸‹æ­¥éª¤ç”Ÿæˆå…¨å±€æ ‡å‡†åŒ–å™¨ï¼š\n"
            f"   1. cd convertToGraph\n"
            f"   2. python convertToGraph-TrainingSet-IMPROVED.py\n"
            f"   \n"
            f"   å…¨å±€æ ‡å‡†åŒ–å™¨æ˜¯ä¿è¯è®­ç»ƒé›†å’Œæµ‹è¯•é›†ç‰¹å¾ä¸€è‡´æ€§çš„å…³é”®ï¼"
        )
        print(error_msg)
        raise FileNotFoundError(error_msg)
    
    try:
        with open(GLOBAL_SCALER_PATH, 'rb') as f:
            scaler_data = pickle.load(f)
        
        if isinstance(scaler_data, dict):
            _global_scaler = scaler_data.get('scaler')
        else:
            _global_scaler = scaler_data
        
        # éªŒè¯æ ‡å‡†åŒ–å™¨æ˜¯å¦æœ‰æ•ˆ
        if _global_scaler is None:
            raise RuntimeError("å…¨å±€æ ‡å‡†åŒ–å™¨æ–‡ä»¶å·²æŸåï¼ˆscalerä¸ºNoneï¼‰")
        
        print(f"âœ“ å·²åŠ è½½è®­ç»ƒé›†çš„å…¨å±€æ ‡å‡†åŒ–å™¨: {GLOBAL_SCALER_PATH}")
        return _global_scaler
        
    except Exception as e:
        error_msg = (
            f"âŒ åŠ è½½å…¨å±€æ ‡å‡†åŒ–å™¨å¤±è´¥ï¼\n"
            f"   é”™è¯¯: {e}\n"
            f"   è·¯å¾„: {GLOBAL_SCALER_PATH}\n"
            f"   \n"
            f"   è¯·é‡æ–°ç”Ÿæˆå…¨å±€æ ‡å‡†åŒ–å™¨ï¼š\n"
            f"   1. cd convertToGraph\n"
            f"   2. python convertToGraph-TrainingSet-IMPROVED.py\n"
        )
        print(error_msg)
        raise RuntimeError(error_msg) from e


# ====================
# ç‰¹å¾æå–å‡½æ•°ï¼ˆ20ç»´ï¼‰
# ====================

def extract_features_20d(geometry, all_geometries=None, idx=None, bounds_stats=None) -> np.ndarray:
    """
    æå–20ç»´å‡ ä½•ä¸å˜ç‰¹å¾ï¼ˆé€‚é…IMPROVEDæ¨¡å‹ï¼‰
    
    ç‰¹å¾åˆ—è¡¨ï¼š
    0-2:   å‡ ä½•ç±»å‹ç¼–ç ï¼ˆone-hotï¼‰
    3:     Huä¸å˜çŸ©Ï†1
    4:     è¾¹ç•Œå¤æ‚åº¦
    5-7:   å½“å‰åœ°å›¾ç›¸å¯¹ä½ç½®ï¼ˆå®è§‚ï¼‰
    8-10:  å±€éƒ¨ç›¸å¯¹ä½ç½®ï¼ˆå¾®è§‚ï¼ŒKè¿‘é‚»ï¼‰
    11-12: é•¿å®½æ¯” + çŸ©å½¢åº¦
    13:    Solidity
    14:    å¯¹æ•°é¡¶ç‚¹æ•°
    15-17: æ‹“æ‰‘é‚»åŸŸç‰¹å¾ï¼ˆé»˜è®¤å€¼ï¼‰
    18:    å­”æ´æ•°é‡
    19:    èŠ‚ç‚¹æ•°ç¼–ç 
    """
    feats: List[float] = []
    
    # ç»´åº¦0-2: å‡ ä½•ç±»å‹ç¼–ç ï¼ˆone-hotï¼‰
    geom_type = getattr(geometry, 'geom_type', 'Unknown')
    if geom_type == 'Point':
        feats.extend([1, 0, 0])
    elif geom_type in ['LineString', 'MultiLineString']:
        feats.extend([0, 1, 0])
    elif geom_type in ['Polygon', 'MultiPolygon']:
        feats.extend([0, 0, 1])
    else:
        feats.extend([0, 0, 0])
    
    # åŸºæœ¬å±æ€§
    area = getattr(geometry, 'area', 0.0) or 0.0
    perimeter = getattr(geometry, 'length', 0.0) or 0.0
    
    # ç»´åº¦3: Huä¸å˜çŸ©Ï†1ï¼ˆç®€åŒ–ç‰ˆï¼‰
    hu1 = 0.0
    if area > 1e-6 and geom_type in ['Polygon', 'MultiPolygon']:
        try:
            coords = np.array(geometry.exterior.coords[:-1]) if geom_type == 'Polygon' else np.array(max(geometry.geoms, key=lambda p: p.area).exterior.coords[:-1])
            if len(coords) >= 3:
                cx, cy = np.mean(coords[:, 0]), np.mean(coords[:, 1])
                mu20 = np.sum((coords[:, 0] - cx)**2) / len(coords)
                mu02 = np.sum((coords[:, 1] - cy)**2) / len(coords)
                nu20, nu02 = mu20 / area, mu02 / area
                hu1 = np.log1p(abs(nu20 + nu02)) / 10.0
        except:
            pass
    feats.append(hu1)
    
    # ç»´åº¦4: è¾¹ç•Œå¤æ‚åº¦
    boundary_complexity = np.log1p(perimeter / np.sqrt(area)) / 5.0 if area > 1e-6 else 0.0
    feats.append(boundary_complexity)
    
    # ç»´åº¦5-7: å½“å‰åœ°å›¾ç›¸å¯¹ä½ç½®ï¼ˆå®è§‚ç©ºé—´ï¼‰
    centroid = geometry.centroid
    if bounds_stats:
        minx, miny, maxx, maxy = bounds_stats['bounds']
        local_width, local_height = maxx - minx, maxy - miny
        local_cx, local_cy = bounds_stats['centroid']
        local_diagonal = np.sqrt(local_width**2 + local_height**2)
        rel_x = (centroid.x - minx) / local_width if local_width > 1e-6 else 0.5
        rel_y = (centroid.y - miny) / local_height if local_height > 1e-6 else 0.5
        dist_to_center = centroid.distance(Point(local_cx, local_cy)) / local_diagonal if local_diagonal > 1e-6 else 0.0
        feats.extend([rel_x, rel_y, dist_to_center])
    else:
        feats.extend([0.5, 0.5, 0.0])
    
    # ç»´åº¦8-10: å±€éƒ¨ç›¸å¯¹ä½ç½®ï¼ˆå¾®è§‚ç©ºé—´ï¼ŒåŸºäºKè¿‘é‚»ï¼‰
    if all_geometries and idx is not None and len(all_geometries) > 1:
        try:
            # âš¡ æ€§èƒ½ä¼˜åŒ–ï¼šä½¿ç”¨é¢„è®¡ç®—çš„è´¨å¿ƒæ•°ç»„
            if bounds_stats and 'precomputed_centroids' in bounds_stats:
                centroids = bounds_stats['precomputed_centroids']
            else:
                # é™çº§æ–¹æ¡ˆï¼šé‡æ–°è®¡ç®—ï¼ˆä¸åº”è¯¥æ‰§è¡Œåˆ°è¿™é‡Œï¼‰
                centroids = np.array([[g.centroid.x, g.centroid.y] for g in all_geometries])
            
            k = min(K_FOR_KNN, len(all_geometries) - 1)
            dists = np.linalg.norm(centroids - centroids[idx], axis=1)
            neighbor_idxs = np.argsort(dists)[1:k+1]
            neighbor_centroids = centroids[neighbor_idxs]
            local_cx, local_cy = np.mean(neighbor_centroids, axis=0)
            local_radius = np.mean(dists[neighbor_idxs])
            local_rel_x = np.clip((centroid.x - local_cx) / (local_radius * 2), -1, 1) if local_radius > 1e-6 else 0.0
            local_rel_y = np.clip((centroid.y - local_cy) / (local_radius * 2), -1, 1) if local_radius > 1e-6 else 0.0
            local_dist = np.sqrt((centroid.x - local_cx)**2 + (centroid.y - local_cy)**2) / local_radius if local_radius > 1e-6 else 0.0
            feats.extend([local_rel_x, local_rel_y, local_dist])
        except:
            feats.extend([0.0, 0.0, 0.0])
    else:
        feats.extend([0.0, 0.0, 0.0])
    
    # ç»´åº¦11-12: é•¿å®½æ¯” + çŸ©å½¢åº¦
    if geom_type in ['Polygon', 'MultiPolygon'] and area > 0:
        try:
            min_rect = geometry.minimum_rotated_rectangle
            rect_area = min_rect.area if min_rect.area > 0 else area
            coords = list(min_rect.exterior.coords)
            d1 = Point(coords[0]).distance(Point(coords[1]))
            d2 = Point(coords[1]).distance(Point(coords[2]))
            aspect_ratio = min(d1, d2) / max(d1, d2) if max(d1, d2) > 1e-6 else 1.0
            rectangularity = area / rect_area if rect_area > 0 else 1.0
            feats.extend([aspect_ratio, rectangularity])
        except:
            feats.extend([0.5, 0.8])
    else:
        feats.extend([0.5, 0.8])
    
    # ç»´åº¦13: Solidity
    solidity = 0.8
    if geom_type in ['Polygon', 'MultiPolygon'] and area > 0:
        try:
            convex_hull_area = geometry.convex_hull.area
            solidity = area / convex_hull_area if convex_hull_area > 0 else 0.8
        except:
            pass
    feats.append(solidity)
    
    # ç»´åº¦14: å¯¹æ•°é¡¶ç‚¹æ•°
    n_vertices = 0
    if geom_type == 'Polygon':
        n_vertices = len(geometry.exterior.coords) - 1
    elif geom_type == 'MultiPolygon':
        n_vertices = sum(len(p.exterior.coords) - 1 for p in geometry.geoms)
    elif geom_type == 'LineString':
        n_vertices = len(geometry.coords)
    elif geom_type == 'MultiLineString':
        n_vertices = sum(len(line.coords) for line in geometry.geoms)
    log_vertices = np.log1p(n_vertices) / 10.0
    feats.append(log_vertices)
    
    # ç»´åº¦15-17: æ‹“æ‰‘é‚»åŸŸç‰¹å¾ï¼ˆç®€åŒ–ï¼šä½¿ç”¨é»˜è®¤å€¼ï¼Œåœ¨å›¾æ„å»ºåä¼šæ›´æ–°ï¼‰
    feats.extend([0.5, 0.5, 0.5])
    
    # ç»´åº¦18: å­”æ´æ•°é‡
    n_holes = 0
    if geom_type == 'Polygon':
        n_holes = len(geometry.interiors)
    elif geom_type == 'MultiPolygon':
        n_holes = sum(len(p.interiors) for p in geometry.geoms)
    holes_normalized = np.log1p(n_holes) / 5.0
    feats.append(holes_normalized)
    
    # ç»´åº¦19: èŠ‚ç‚¹æ•°ç¼–ç 
    total_nodes = len(all_geometries) if all_geometries else 100
    node_count_encoding = np.log1p(total_nodes) / 10.0
    feats.append(node_count_encoding)
    
    return np.array(feats, dtype=np.float32)


# ====================
# å›¾æ„å»ºå‡½æ•°
# ====================

def _hilbert_curve_sort(centroids):
    """
    ä½¿ç”¨Hilbertæ›²çº¿å¯¹åæ ‡ç‚¹æ’åºï¼ˆä¿æŒç©ºé—´å±€éƒ¨æ€§ï¼‰
    
    Args:
        centroids: nx2çš„åæ ‡æ•°ç»„
        
    Returns:
        æ’åºåçš„ç´¢å¼•æ•°ç»„
    """
    try:
        from hilbertcurve.hilbertcurve import HilbertCurve
    except ImportError:
        # é™çº§åˆ°ç®€å•çš„x+yæ’åº
        print(f"      âš ï¸ hilbertcurveæœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–æ’åº")
        print(f"      æç¤ºï¼špip install hilbertcurve å¯è·å¾—æ›´å¥½æ€§èƒ½")
        return np.argsort(centroids[:, 0] + centroids[:, 1])
    
    n = len(centroids)
    
    # æ ‡å‡†åŒ–åæ ‡åˆ°[0, 2^p-1]èŒƒå›´
    x_min, x_max = centroids[:, 0].min(), centroids[:, 0].max()
    y_min, y_max = centroids[:, 1].min(), centroids[:, 1].max()
    
    # è®¡ç®—åˆé€‚çš„Hilbertæ›²çº¿é˜¶æ•°ï¼ˆpå€¼ï¼‰
    # 2^påº”è¯¥è¶³å¤Ÿå¤§ä»¥ä¿è¯ç²¾åº¦ï¼Œä½†ä¸èƒ½å¤ªå¤§å¯¼è‡´æº¢å‡º
    p = min(15, max(8, int(np.log2(np.sqrt(n))) + 3))
    max_coord = (1 << p) - 1  # 2^p - 1
    
    # æ ‡å‡†åŒ–åæ ‡
    if x_max > x_min:
        x_norm = ((centroids[:, 0] - x_min) / (x_max - x_min) * max_coord).astype(int)
    else:
        x_norm = np.zeros(n, dtype=int)
    
    if y_max > y_min:
        y_norm = ((centroids[:, 1] - y_min) / (y_max - y_min) * max_coord).astype(int)
    else:
        y_norm = np.zeros(n, dtype=int)
    
    # è®¡ç®—Hilbertè·ç¦»
    hc = HilbertCurve(p, 2)  # pé˜¶ï¼Œ2ç»´
    hilbert_distances = np.array([
        hc.distance_from_point([int(x), int(y)]) 
        for x, y in zip(x_norm, y_norm)
    ])
    
    # æŒ‰Hilbertè·ç¦»æ’åº
    return np.argsort(hilbert_distances)


def build_knn_delaunay_edges(geometries, k: int = None):
    """
    æ„å»º KNN + Delaunay ç»Ÿä¸€å›¾ï¼ˆä¸è®­ç»ƒé›†é€»è¾‘å®Œå…¨ä¸€è‡´ï¼‰
    
    æ ¸å¿ƒç­–ç•¥ï¼š
    1. ä½¿ç”¨åŸå§‹å‡ ä½•è´¨å¿ƒåæ ‡ï¼ˆä¸æ˜¯æ ‡å‡†åŒ–åçš„ç‰¹å¾ï¼‰
    2. KNNæ„å»ºï¼šæ¯ä¸ªèŠ‚ç‚¹è¿æ¥Kä¸ªæœ€è¿‘é‚»ï¼ˆâ­è‡ªé€‚åº”Kå€¼ï¼‰
    3. Delaunayä¸‰è§’å‰–åˆ†ï¼šä¿è¯å…¨å±€è¿é€š
    4. åˆå¹¶å»é‡ï¼šå–å¹¶é›†
    
    Args:
        geometries: å‡ ä½•è¦ç´ åˆ—è¡¨
        k: KNNé‚»å±…æ•°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨è‡ªé€‚åº”Kå€¼ï¼Œæ¨èï¼‰
        
    Returns:
        edges: è¾¹åˆ—è¡¨ [[src, dst], ...]
    """
    from sklearn.neighbors import NearestNeighbors
    try:
        from scipy.spatial import Delaunay as DelaunayTri
    except ImportError:
        DelaunayTri = None
    
    n = len(geometries)
    if n < 2:
        return []
    
    # âœ… ä½¿ç”¨åŸå§‹å‡ ä½•è´¨å¿ƒåæ ‡
    centroids = np.array([[geom.centroid.x, geom.centroid.y] for geom in geometries])
    
    # â­ è‡ªé€‚åº”Kå€¼ï¼ˆä¸è®­ç»ƒé›†å®Œå…¨ä¸€è‡´ï¼‰
    if k is None:
        k = adaptive_k_for_graph(n)
    
    # === KNN æ„å»ºï¼ˆæ‰€æœ‰è§„æ¨¡éƒ½æ‰§è¡Œï¼‰ ===
    actual_k = min(k, n - 1)
    knn_edges = []
    
    if actual_k >= 1:
        print(f"    [1/2] æ„å»ºKNNå›¾ï¼ˆK={actual_k}ï¼Œå…±{n}ä¸ªèŠ‚ç‚¹ï¼‰...")
        nbrs = NearestNeighbors(
            n_neighbors=actual_k + 1,  # +1å› ä¸ºåŒ…æ‹¬è‡ªå·±
            algorithm='kd_tree'
        ).fit(centroids)
        
        distances, indices = nbrs.kneighbors(centroids)
        
        for i in range(n):
            # æ’é™¤è‡ªå·±ï¼ˆç¬¬ä¸€ä¸ªï¼‰ï¼Œå–Kä¸ªæœ€è¿‘é‚»
            for j in indices[i][1:actual_k+1]:
                knn_edges.append([i, int(j)])
        
        print(f"    âœ“ KNNå®Œæˆï¼Œè¾¹æ•°: {len(knn_edges)}")
    
    # === Delaunay ä¸‰è§’å‰–åˆ†ï¼ˆæ‰€æœ‰èŠ‚ç‚¹ç»Ÿä¸€å¤„ç†ï¼‰ ===
    delaunay_edges = []
    
    if DelaunayTri is None:
        print(f"    âš ï¸ scipy.spatial.Delaunayä¸å¯ç”¨ï¼Œè·³è¿‡Delaunay")
    elif n < 3:
        if n == 2:
            delaunay_edges = [[0, 1]]
            print(f"    [2/2] Delaunay: 2èŠ‚ç‚¹ç›´æ¥è¿æ¥")
    else:
        # æ‰€æœ‰è§„æ¨¡æ•°æ®éƒ½ç›´æ¥åšDelaunayï¼ˆä¸è®­ç»ƒé›†å®Œå…¨ä¸€è‡´ï¼‰
        print(f"    [2/2] æ„å»ºDelaunayä¸‰è§’å‰–åˆ†ï¼ˆ{n}ä¸ªèŠ‚ç‚¹ï¼‰...")
        try:
            tri = DelaunayTri(centroids)
            
            edge_set = set()
            for simplex in tri.simplices:
                edge_set.add(tuple(sorted([simplex[0], simplex[1]])))
                edge_set.add(tuple(sorted([simplex[1], simplex[2]])))
                edge_set.add(tuple(sorted([simplex[2], simplex[0]])))
            
            for edge in edge_set:
                delaunay_edges.append([edge[0], edge[1]])
            
            print(f"    âœ“ Delaunayå®Œæˆï¼Œè¾¹æ•°: {len(delaunay_edges)}")
        except Exception as e:
            print(f"    âš ï¸ Delaunayå¤±è´¥: {e}")
    
    # === åˆå¹¶å»é‡ï¼ˆä¸è®­ç»ƒé›†å®Œå…¨ä¸€è‡´ï¼šå…ˆå­˜å‚¨æ— å‘è¾¹ï¼Œåç»­ç”¨to_undirectedè½¬æ¢ï¼‰ ===
    all_edges = knn_edges + delaunay_edges
    
    edge_set = set()
    
    for edge in all_edges:
        edge_tuple = tuple(sorted(edge))  # æ— å‘è¾¹ï¼šç»Ÿä¸€ä¸º (min, max)
        edge_set.add(edge_tuple)
    
    # âœ… è½¬æ¢ä¸ºè¾¹åˆ—è¡¨ï¼ˆå•å‘è¡¨ç¤ºï¼Œä¸è®­ç»ƒé›†ä¸€è‡´ï¼‰
    unique_edges = [[e[0], e[1]] for e in edge_set]
    
    print(f"    âœ“ åˆå¹¶å»é‡å®Œæˆï¼Œæ— å‘è¾¹æ•°: {len(unique_edges)}å¯¹")
    
    return unique_edges


def gdf_to_graph(gdf, max_nodes=None) -> Optional[Data]:
    """
    ä»GeoDataFrameæ„å»ºå›¾ç»“æ„ï¼ˆ20ç»´ç‰¹å¾ + KNN+Delaunayå›¾ï¼‰
    
    Args:
        gdf: GeoDataFrameå¯¹è±¡
        max_nodes: æœ€å¤§èŠ‚ç‚¹æ•°é˜ˆå€¼ï¼Œè¶…è¿‡åˆ™è¿”å›Noneï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
    
    Returns:
        Dataå¯¹è±¡æˆ–Noneï¼ˆå¦‚æœèŠ‚ç‚¹æ•°è¶…è¿‡é˜ˆå€¼ï¼‰
    """
    if Data is None or StandardScaler is None:
        print("ç¼ºå°‘ä¾èµ–ï¼štorch-geometric æˆ– scikit-learn")
        return None
    
    # â­æ£€æŸ¥èŠ‚ç‚¹æ•°ï¼Œè¶…è¿‡é˜ˆå€¼åˆ™è·³è¿‡ï¼ˆä»…å½“max_nodesä¸ä¸ºNoneæ—¶ï¼‰
    if max_nodes is not None:
        num_nodes = len(gdf)
        if num_nodes > max_nodes:
            return None
    
    geometries = gdf.geometry.tolist()
    
    # âš¡ æ€§èƒ½ä¼˜åŒ–ï¼šé¢„è®¡ç®—æ‰€æœ‰è´¨å¿ƒï¼ˆé¿å…åœ¨extract_features_20dä¸­é‡å¤è®¡ç®—ï¼‰
    print(f"    é¢„è®¡ç®—è´¨å¿ƒæ•°ç»„...")
    all_centroids = np.array([[g.centroid.x, g.centroid.y] for g in geometries])
    
    # è®¡ç®—è¾¹ç•Œç»Ÿè®¡ä¿¡æ¯
    all_bounds = [g.bounds for g in geometries]
    bounds_stats = {
        'bounds': (
            min(b[0] for b in all_bounds),
            min(b[1] for b in all_bounds),
            max(b[2] for b in all_bounds),
            max(b[3] for b in all_bounds)
        ),
        'centroid': (
            np.mean(all_centroids[:, 0]),
            np.mean(all_centroids[:, 1])
        ),
        'precomputed_centroids': all_centroids  # âš¡ ä¼ é€’é¢„è®¡ç®—çš„è´¨å¿ƒ
    }
    
    # æå–ç‰¹å¾
    print(f"    æå–ç‰¹å¾...")
    feats = [extract_features_20d(geometries[i], geometries, i, bounds_stats) 
             for i in range(len(geometries))]
    feats = np.array(feats, dtype=np.float32)
    
    # ç‰¹å¾å½’ä¸€åŒ–ï¼ˆå¿…é¡»ä½¿ç”¨å…¨å±€æ ‡å‡†åŒ–å™¨ï¼‰
    if len(feats) > 0:
        # åŠ è½½è®­ç»ƒé›†çš„å…¨å±€æ ‡å‡†åŒ–å™¨ï¼ˆå¦‚æœä¸å­˜åœ¨ä¼šæŠ›å‡ºå¼‚å¸¸ï¼‰
        global_scaler = load_global_scaler()
        # âœ… ä½¿ç”¨è®­ç»ƒé›†çš„å…¨å±€æ ‡å‡†åŒ–å™¨ï¼ˆç¬¦åˆæœºå™¨å­¦ä¹ æ ‡å‡†å®è·µï¼‰
        feats = global_scaler.transform(feats)
    
    # âœ… æ„å»ºè¾¹ï¼šä½¿ç”¨KNN+Delaunayï¼ˆåŸºäºåŸå§‹å‡ ä½•è´¨å¿ƒï¼Œè‡ªé€‚åº”Kå€¼ï¼‰
    edges = build_knn_delaunay_edges(geometries, k=None)  # Noneè¡¨ç¤ºä½¿ç”¨è‡ªé€‚åº”Kå€¼
    
    # è½¬æ¢ä¸ºedge_indexæ ¼å¼
    if len(edges) > 0:
        edge_index = torch.tensor(edges, dtype=torch.long).T
        # âœ… è½¬æ¢ä¸ºæ— å‘å›¾ï¼ˆè‡ªåŠ¨æ·»åŠ åå‘è¾¹ï¼Œä¸è®­ç»ƒé›†å®Œå…¨ä¸€è‡´ï¼‰
        from torch_geometric.utils import to_undirected
        edge_index = to_undirected(edge_index)
    else:
        edge_index = torch.empty((2, 0), dtype=torch.long)
    
    # åˆ›å»ºDataå¯¹è±¡
    data = Data(
        x=torch.tensor(feats, dtype=torch.float32),
        edge_index=edge_index
    )
    
    return data


# ====================
# æ¨¡å‹åŠ è½½å‡½æ•°
# ====================

def load_improved_gat_model(device='cpu', model_path=None):
    """
    åŠ è½½ImprovedGATModelæ¨¡å‹
    
    Returns:
        model: åŠ è½½å¥½çš„æ¨¡å‹ï¼ˆå·²è®¾ç½®ä¸ºevalæ¨¡å¼ï¼‰
        device: ä½¿ç”¨çš„è®¾å¤‡
    """
    if model_path is None:
        model_path = MODEL_PATH
    
    # å…è®¸è‡ªåŠ¨å›é€€ï¼šé»˜è®¤bestæŸå/æˆªæ–­æ—¶ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹æˆ–checkpoint
    model_path = Path(model_path)
    models_dir = PROJECT_ROOT / 'VGAT' / 'models'
    ckpt_dir = PROJECT_ROOT / 'VGAT' / 'checkpoints'
    candidates = [
        model_path,
        models_dir / 'gat_model_IMPROVED_best_V2.pth',
        ckpt_dir / 'gat_checkpoint_latest.pth',
        ckpt_dir / 'gat_checkpoint_emergency_epoch39.pth',
        ckpt_dir / 'gat_checkpoint_emergency_epoch21.pth',
    ]
    
    # æ·»åŠ VGATè·¯å¾„åˆ°sys.path
    vgat_path = str(PROJECT_ROOT / 'VGAT')
    if vgat_path not in sys.path:
        sys.path.insert(0, vgat_path)
    
    try:
        from VGAT import ImprovedGATModel  # type: ignore
    except Exception as exc:
        print(f'å¯¼å…¥ImprovedGATModelå¤±è´¥: {exc}')
        print('å°è¯•ç›´æ¥åŠ è½½æ¨¡å—...')
        try:
            import importlib.util
            spec = importlib.util.spec_from_file_location(
                "vgat_improved", 
                str(PROJECT_ROOT / 'VGAT' / 'VGAT-IMPROVED.py')
            )
            vgat_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(vgat_module)
            ImprovedGATModel = vgat_module.ImprovedGATModel
        except Exception as e2:
            raise ImportError(f"æ— æ³•åŠ è½½æ¨¡å‹ç±»: {e2}")
    
    # åˆ›å»ºæ¨¡å‹ï¼ˆ20ç»´è¾“å…¥ï¼Œ256éšè—å±‚ï¼Œ1024è¾“å‡ºï¼Œ8ä¸ªæ³¨æ„åŠ›å¤´ï¼‰
    model = ImprovedGATModel(
        input_dim=20,
        hidden_dim=256,
        output_dim=1024,
        num_heads=8,
        dropout=0.3
    )
    
    # åŠ è½½æƒé‡ï¼ˆå®¹é”™ï¼šæ”¯æŒplain state_dictæˆ–å¸¦'model_state_dict'çš„dictï¼›å°è¯•å¤šä¸ªå€™é€‰è·¯å¾„ï¼‰
    last_error = None
    for cand in candidates:
        try:
            if not cand.exists():
                continue
            # ä¼˜å…ˆè¿‡æ»¤æ˜æ˜¾å¼‚å¸¸çš„å°æ–‡ä»¶ï¼ˆ<1MBï¼‰
            try:
                if cand.stat().st_size < (1 << 20):  # 1MB
                    continue
            except Exception:
                pass
            try:
                ckpt = torch.load(str(cand), map_location=device, weights_only=False)
            except TypeError:
                ckpt = torch.load(str(cand), map_location=device)
            state = None
            if isinstance(ckpt, dict) and 'model_state_dict' in ckpt:
                state = ckpt['model_state_dict']
            elif isinstance(ckpt, dict):
                # å¯èƒ½ç›´æ¥æ˜¯state_dict
                state = ckpt
            if isinstance(state, dict):
                model.load_state_dict(state, strict=False)
                model.to(device)
                model.eval()
                print(f'âœ“ æˆåŠŸåŠ è½½æ¨¡å‹: {cand.name}')
                print(f'  ä½¿ç”¨å€™é€‰è·¯å¾„: {cand}')
                break
        except Exception as e:
            last_error = e
            continue
    else:
        raise RuntimeError(f"æ— æ³•åŠ è½½æ¨¡å‹ã€‚å·²å°è¯•: {[str(c) for c in candidates if c.exists()]}\næœ€åé”™è¯¯: {last_error}")
    
    print(f'  è®¾å¤‡: {device}')
    print(f'  è¾“å…¥ç»´åº¦: 20')
    print(f'  è¾“å‡ºç»´åº¦: 1024')
    
    return model, device


# ====================
# é›¶æ°´å°å·¥å…·å‡½æ•°
# ====================

def load_cat32(img_path=None):
    """åŠ è½½å¹¶é¢„å¤„ç†ç‰ˆæƒå›¾åƒä¸º32x32äºŒå€¼å›¾"""
    if img_path is None:
        img_path = CAT32_PATH
    
    try:
        from PIL import Image  # type: ignore
    except Exception:
        raise ImportError("éœ€è¦å®‰è£… pillow: pip install pillow")
    
    try:
        img = Image.open(img_path)
        img = img.convert('L').resize((32, 32))
        img = img.point(lambda x: 0 if x < 128 else 255, '1')
        return np.array(img, dtype=np.uint8)
    except Exception as exc:
        print(f'åŠ è½½ç‰ˆæƒå›¾åƒå¤±è´¥: {exc}')
        # è¿”å›éšæœºå›¾åƒä½œä¸ºå¤‡é€‰
        return (np.random.rand(32, 32) > 0.5).astype(np.uint8)


def features_to_matrix(features: np.ndarray, shape=(32, 32)) -> np.ndarray:
    """å°†ç‰¹å¾å‘é‡è½¬æ¢ä¸ºäºŒå€¼çŸ©é˜µï¼ˆåŸºäºä¸­ä½æ•°é˜ˆå€¼ï¼‰"""
    total = shape[0] * shape[1]
    
    # å±•å¹³ç‰¹å¾
    if features.ndim > 1:
        features_1d = features.flatten()
    else:
        features_1d = features
    
    # å¦‚æœç‰¹å¾ä¸è¶³ï¼Œé‡å¤å¡«å……
    if len(features_1d) < total:
        rep = (total + len(features_1d) - 1) // len(features_1d)
        features_1d = np.tile(features_1d, rep)
    
    # æˆªå–åˆ°ç›®æ ‡å¤§å°å¹¶reshape
    mat = features_1d[:total].reshape(shape)
    
    # ä¸­ä½æ•°é˜ˆå€¼äºŒå€¼åŒ–
    thr = np.median(mat)
    return (mat > thr).astype(np.uint8)


def calc_nc(a: np.ndarray, b: np.ndarray) -> float:
    """è®¡ç®—å½’ä¸€åŒ–ç›¸å…³ç³»æ•°ï¼ˆNCï¼‰"""
    va = a.flatten().astype(float)
    vb = b.flatten().astype(float)
    
    dot = float(np.sum(va * vb))
    na = float(np.sqrt(np.sum(va ** 2)))
    nb = float(np.sqrt(np.sum(vb ** 2)))
    
    if na == 0 or nb == 0:
        return 0.0
    
    return dot / (na * nb)


def extract_features_from_graph(graph_data, model, device, copyright_shape=(32, 32)):
    """
    ä»å›¾æ•°æ®ä¸­æå–1024ç»´ç‰¹å¾å¹¶è½¬ä¸ºäºŒå€¼çŸ©é˜µ
    
    Args:
        graph_data: PyTorch Geometric Dataå¯¹è±¡
        model: VGATæ¨¡å‹
        device: è®¾å¤‡
        copyright_shape: ç‰ˆæƒå›¾åƒå½¢çŠ¶
    
    Returns:
        feat_matrix: äºŒå€¼ç‰¹å¾çŸ©é˜µ
    """
    with torch.no_grad():
        feat = model(
            graph_data.x.to(device),
            graph_data.edge_index.to(device)
        ).detach().cpu().numpy()
    
    # ç¡®ä¿æ˜¯1024ç»´
    if feat.ndim > 1:
        feat = feat.flatten()
    
    if len(feat) != 1024:
        if len(feat) < 1024:
            rep = (1024 + len(feat) - 1) // len(feat)
            feat = np.tile(feat, rep)
        feat = feat[:1024]
    
    # è½¬ä¸ºäºŒå€¼çŸ©é˜µ
    feat_matrix = features_to_matrix(feat, copyright_shape)
    
    return feat_matrix


# ====================
# GeoJSONè½¬æ¢å‡½æ•°
# ====================

def convert_to_geojson(input_paths: List[Path], output_dir: Path) -> List[Path]:
    """
    æ‰¹é‡è½¬æ¢çŸ¢é‡æ•°æ®ä¸ºGeoJSONï¼ˆä¸ convertToGeoJson.py å¯¹é½ï¼‰

    è¡Œä¸ºï¼š
      - å¯¹ .shp æŒ‰å¤šç¼–ç é¡ºåºå°è¯•è¯»å–ï¼ˆé¿å…ç¼–ç é—®é¢˜ï¼‰
      - è‹¥è¯»å–å¤±è´¥ä¸”ç¼ºå°‘ .shxï¼Œå¯é€šè¿‡å¤–éƒ¨è®¾ç½®ç¯å¢ƒå˜é‡ SHAPE_RESTORE_SHX=YES
      - å°†åæ ‡ç³»ç»Ÿä¸€è½¬æ¢ä¸º EPSG:4326
      - ä»¥ UTF-8 ç¼–ç è¾“å‡º GeoJSON
      - Append æ¨¡å¼ï¼šä¸ä¼šåˆ é™¤å·²æœ‰æ–‡ä»¶
    """
    if gpd is None:
        raise ImportError("éœ€è¦å®‰è£… geopandas")

    output_dir.mkdir(parents=True, exist_ok=True)
    outputs: List[Path] = []

    # ä¸ convertToGeoJson.py å¯¹é½çš„ç¼–ç å°è¯•é¡ºåº
    shp_encodings = ['utf-8', 'gbk', 'gb2312', 'cp936', 'latin1', 'iso-8859-1']

    for src in input_paths:
        # è·³è¿‡ macOS å…ƒæ•°æ®æ–‡ä»¶
        if src.name.startswith('._'):
            continue

        try:
            base = src.stem
            out_path = output_dir / f'{base}.geojson'

            # å¦‚æœç›®æ ‡å·²å­˜åœ¨ï¼Œä¿æŒ append æ¨¡å¼ï¼Œè·³è¿‡
            if out_path.exists():
                print(f'  âš ï¸ ç›®æ ‡å·²å­˜åœ¨ï¼Œè·³è¿‡: {out_path.name}')
                outputs.append(out_path)
                continue

            # é’ˆå¯¹ SHP ä½¿ç”¨å¤šç¼–ç å°è¯•
            if src.suffix.lower() == ".shp":
                gdf = None
                last_err: Optional[Exception] = None
                for encoding in shp_encodings:
                    try:
                        gdf = gpd.read_file(src, encoding=encoding)
                        print(f'  âœ“ {src.name} ä½¿ç”¨ç¼–ç  {encoding} è¯»å–æˆåŠŸ ({len(gdf)} è¦ç´ )')
                        break
                    except UnicodeDecodeError as e:
                        last_err = e
                        continue
                    except Exception as e:
                        last_err = e
                        continue

                if gdf is None:
                    print(f'  âœ— {src.name}: æ‰€æœ‰ç¼–ç  {shp_encodings} å‡æ— æ³•è¯»å– ({last_err})')
                    continue
            else:
                # å…¶ä»–æ ¼å¼ï¼ˆGeoJSON ç­‰ï¼‰ç›´æ¥è¯»å–
                gdf = gpd.read_file(src)
                print(f'  âœ“ {src.name} è¯»å–æˆåŠŸ ({len(gdf)} è¦ç´ )')

            # è½¬æ¢ä¸º WGS84ï¼ˆå¦‚æœæœ‰ CRS ä¸”ä¸æ˜¯ EPSG:4326ï¼‰
            if getattr(gdf, 'crs', None) and str(gdf.crs) != 'EPSG:4326':
                gdf = gdf.to_crs('EPSG:4326')

            # è¯»å–åæ ‡å‡†åŒ–ï¼ˆä»…é’ˆå¯¹éƒ¨åˆ†å›¾å±‚ä»¥å‡å°‘ä¸å¿…è¦ä¿®æ”¹ï¼‰
            try:
                standardize_layers = {'BRGA', 'HYDP', 'LRDL'}
                if base in standardize_layers:
                    # åœ¨æœ¬ä½œç”¨åŸŸå†…å¯¼å…¥æ‰€éœ€ shapely ç±»ï¼Œé¿å…å…¨å±€æ”¹åŠ¨
                    from shapely.geometry import MultiPolygon, MultiLineString, MultiPoint  # type: ignore

                    def _to_multipart(geom):
                        if geom is None:
                            return geom
                        try:
                            gt = geom.geom_type
                        except Exception:
                            return geom
                        if gt == 'Polygon':
                            return MultiPolygon([geom])
                        if gt == 'LineString':
                            return MultiLineString([geom])
                        if gt == 'Point':
                            return MultiPoint([geom])
                        return geom

                    if 'geometry' in gdf:
                        try:
                            gdf['geometry'] = gdf['geometry'].apply(lambda x: _to_multipart(x) if x is not None else x)
                        except Exception:
                            new_geoms = []
                            for _, row in gdf.iterrows():
                                geom = row.get('geometry', None) if isinstance(row, dict) else row.geometry if hasattr(row, 'geometry') else None
                                new_geoms.append(_to_multipart(geom) if geom is not None else geom)
                            gdf['geometry'] = new_geoms

                    # ç»Ÿä¸€å¯èƒ½çš„å­—æ®µåå˜ä½“ï¼ˆé¿å…åç»­å¤„ç†å› å­—æ®µæˆªæ–­äº§ç”Ÿå·®å¼‚ï¼‰
                    rename_map = {}
                    for c in list(gdf.columns):
                        lc = c.lower()
                        if lc.startswith('shape_leng'):
                            rename_map[c] = 'SHAPE_Length'
                        if lc.startswith('shape_area'):
                            rename_map[c] = 'SHAPE_Area'
                    if rename_map:
                        gdf = gdf.rename(columns=rename_map)
            except Exception:
                # è‹¥æ ‡å‡†åŒ–å¤±è´¥ï¼Œç»§ç»­ä½¿ç”¨åŸå§‹è¯»å–ç»“æœï¼ˆä¸å¯é˜»å¡æµç¨‹ï¼‰
                pass

            # ä¿å­˜ä¸º GeoJSONï¼ŒUTF-8 ç¼–ç 
            gdf.to_file(out_path, driver='GeoJSON', encoding='utf-8')
            print(f'  âœ“ å¯¼å‡º {out_path.name} ({len(gdf)} è¦ç´ )')
            outputs.append(out_path)

        except Exception as exc:
            print(f'  âœ— {src.name}: {exc}')
            continue

    return outputs


# ====================
# é€šç”¨å›¾æ•°æ®è½¬æ¢å‡½æ•°
# ====================

def convert_geojsons_to_graphs(
    original_geojsons: List[Path],
    attacked_geojson_map: dict,
    output_dir_original: Path,
    output_dir_attacked: Path,
    max_nodes=None
):
    """
    æ‰¹é‡è½¬æ¢GeoJSONä¸ºå›¾ç»“æ„ï¼ˆå¯é€‰èŠ‚ç‚¹æ•°è¿‡æ»¤ï¼‰
    
    Args:
        original_geojsons: åŸå§‹GeoJSONæ–‡ä»¶åˆ—è¡¨
        attacked_geojson_map: æ”»å‡»åçš„GeoJSONæ–‡ä»¶æ˜ å°„ {base_name: {attack_param: path}}
        output_dir_original: åŸå§‹å›¾è¾“å‡ºç›®å½•
        output_dir_attacked: æ”»å‡»å›¾è¾“å‡ºç›®å½•
        max_nodes: æœ€å¤§èŠ‚ç‚¹æ•°é˜ˆå€¼ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼Œé»˜è®¤Noneï¼‰
    """
    import shutil
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨ï¼ˆä¸åˆ é™¤å·²æœ‰æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼‰
    # âš ï¸ ä¸åˆ é™¤å·²æœ‰çš„å›¾æ–‡ä»¶ï¼Œå…è®¸å¢é‡è½¬æ¢å’Œæ–­ç‚¹ç»­ä¼ 
    output_dir_original.mkdir(parents=True, exist_ok=True)
    output_dir_attacked.mkdir(parents=True, exist_ok=True)
    
    # ç»Ÿè®¡
    skipped_original = []
    processed_original = []
    skipped_attacked = 0
    processed_attacked = 0
    
    # è½¬æ¢åŸå§‹å›¾
    threshold_info = f"ä¸é™åˆ¶" if max_nodes is None else f"{max_nodes}"
    print(f'\n[è½¬æ¢åŸå§‹å›¾] (èŠ‚ç‚¹æ•°é˜ˆå€¼: {threshold_info})')
    for src in original_geojsons:
        # â­è·³è¿‡macOSå…ƒæ•°æ®æ–‡ä»¶ï¼ˆä»¥._å¼€å¤´çš„æ–‡ä»¶ï¼‰
        if src.name.startswith('._'):
            print(f'  âš ï¸  è·³è¿‡macOSå…ƒæ•°æ®æ–‡ä»¶: {src.name}')
            continue
        
        # â­æ£€æŸ¥åŸå§‹å›¾æ˜¯å¦å·²å­˜åœ¨ï¼Œè‹¥å­˜åœ¨åˆ™è·³è¿‡
        out_path = output_dir_original / f"{src.stem}_graph.pkl"
        if out_path.exists():
            print(f'  â­ï¸  è·³è¿‡ {src.name}ï¼šåŸå§‹å›¾å·²å­˜åœ¨')
            processed_original.append(src.stem)
            continue
        
        try:
            gdf = gpd.read_file(src)
            num_nodes = len(gdf)
            
            # â­æ£€æŸ¥èŠ‚ç‚¹æ•°ï¼ˆä»…å½“max_nodesä¸ä¸ºNoneæ—¶ï¼‰
            if max_nodes is not None and num_nodes > max_nodes:
                print(f'  âš ï¸  è·³è¿‡ {src.name}: èŠ‚ç‚¹æ•°={num_nodes} > {max_nodes}')
                skipped_original.append(src.stem)
                continue
            
            data = gdf_to_graph(gdf, max_nodes)
            
            if data is not None:
                with open(out_path, 'wb') as f:
                    pickle.dump(data, f)
                print(f'  âœ“ {out_path.name} (èŠ‚ç‚¹æ•°: {num_nodes})')
                processed_original.append(src.stem)
        except Exception as exc:
            print(f'  âœ— {src.name}: {exc}')
            skipped_original.append(src.stem)
    
    # è½¬æ¢æ”»å‡»å›¾ï¼ˆè·³è¿‡è¢«è¿‡æ»¤çš„åŸå§‹å›¾å¯¹åº”çš„æ”»å‡»å›¾ï¼‰
    print('\n[è½¬æ¢æ”»å‡»å›¾]')
    for base, attack_map in attacked_geojson_map.items():
        # â­å¦‚æœåŸå§‹å›¾è¢«è·³è¿‡ï¼Œåˆ™è·³è¿‡æ‰€æœ‰å¯¹åº”çš„æ”»å‡»å›¾
        if base in skipped_original:
            print(f'  âš ï¸  è·³è¿‡ {base} çš„æ‰€æœ‰æ”»å‡»å›¾ï¼ˆåŸå§‹å›¾èŠ‚ç‚¹æ•°è¶…é™ï¼‰')
            skipped_attacked += len(attack_map)
            continue
        
        subdir = output_dir_attacked / base
        # respect KEEP_EXISTING env: if set, don't rmtree; only generate missing graphs
        KEEP_EXISTING = os.environ.get('KEEP_EXISTING', '0') in ['1', 'true', 'True']
        # â­ å¦‚æœå­ç›®å½•å·²å­˜åœ¨ä¸”å·²æœ‰å®Œæ•´çš„æ”»å‡»å›¾ï¼Œåˆ™è·³è¿‡
        if subdir.exists() and len(list(subdir.glob('*_graph.pkl'))) == len(attack_map):
            print(f'  â­ï¸  è·³è¿‡ {base}ï¼šæ”»å‡»å›¾å·²å®Œæ•´ ({len(attack_map)} ä¸ª)')
            processed_attacked += len(attack_map)
            continue
        # æ¸…ç†å¹¶é‡å»ºè¯¥æ•°æ®é›†çš„å­ç›®å½•ï¼ˆé™¤é KEEP_EXISTING ä¸ºçœŸï¼Œæ­¤æ—¶åªç”Ÿæˆç¼ºå¤±é¡¹ï¼‰
        if subdir.exists():
            if KEEP_EXISTING:
                print(f'  âš ï¸  å­ç›®å½•å­˜åœ¨ï¼ŒKEEP_EXISTING=Trueï¼Œä¿ç•™å·²æœ‰æ–‡ä»¶ï¼Œä»…ç”Ÿæˆç¼ºå¤±é¡¹: {subdir}')
            else:
                shutil.rmtree(subdir)
                subdir.mkdir(parents=True, exist_ok=True)
        else:
            subdir.mkdir(parents=True, exist_ok=True)
        
        for param, geojson_path in sorted(attack_map.items()):
            try:
                out_path = subdir / f"{geojson_path.stem}_graph.pkl"
                if out_path.exists() and KEEP_EXISTING:
                    print(f'  â­ï¸  è·³è¿‡å·²å­˜åœ¨æ”»å‡»å›¾: {base}/{out_path.name}')
                    processed_attacked += 1
                    continue
                gdf = gpd.read_file(geojson_path)
                data = gdf_to_graph(gdf, max_nodes)
                
                if data is not None:
                    with open(out_path, 'wb') as f:
                        pickle.dump(data, f)
                    print(f'  âœ“ {base}/{out_path.name}')
                    processed_attacked += 1
                else:
                    skipped_attacked += 1
            except Exception as exc:
                print(f'  âœ— {base}/{param}: {exc}')
                skipped_attacked += 1
    
    # â­è¾“å‡ºè½¬æ¢ç»Ÿè®¡
    print('\n' + '='*60)
    threshold_info = "ä¸é™åˆ¶" if max_nodes is None else str(max_nodes)
    print(f'ğŸ“Š è½¬æ¢ç»Ÿè®¡ï¼ˆèŠ‚ç‚¹æ•°é˜ˆå€¼: {threshold_info}ï¼‰')
    print('='*60)
    print(f'åŸå§‹å›¾: å¤„ç† {len(processed_original)} ä¸ª, è·³è¿‡ {len(skipped_original)} ä¸ª')
    if skipped_original:
        print(f'  è·³è¿‡çš„æ–‡ä»¶: {", ".join(skipped_original)}')
    print(f'æ”»å‡»å›¾: å¤„ç† {processed_attacked} ä¸ª, è·³è¿‡ {skipped_attacked} ä¸ª')
    print('='*60)


# ====================
# å¯¼å‡ºæ¥å£
# ====================

__all__ = [
    # è·¯å¾„é…ç½®
    'PROJECT_ROOT',
    'SCRIPT_DIR',
    'MODEL_PATH',
    'CAT32_PATH',
    'GLOBAL_SCALER_PATH',
    'K_FOR_KNN',
    
    # è‡ªé€‚åº”Kå€¼å‡½æ•°
    'adaptive_k_for_graph',
    
    # æ ‡å‡†åŒ–å™¨
    'load_global_scaler',
    
    # ç‰¹å¾æå–
    'extract_features_20d',
    
    # å›¾æ„å»º
    'build_knn_delaunay_edges',
    'gdf_to_graph',
    'convert_geojsons_to_graphs',
    
    # æ¨¡å‹åŠ è½½
    'load_improved_gat_model',
    
    # é›¶æ°´å°å·¥å…·
    'load_cat32',
    'features_to_matrix',
    'calc_nc',
    'extract_features_from_graph',
    
    # GeoJSONè½¬æ¢
    'convert_to_geojson',
]

