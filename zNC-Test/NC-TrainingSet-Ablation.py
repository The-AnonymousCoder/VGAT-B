#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®­ç»ƒé›†NCæµ‹è¯• - æ¶ˆèå®éªŒé€šç”¨è„šæœ¬
ä½¿ç”¨è¢«æ”»å‡»çš„çŸ¢é‡åœ°å›¾éªŒè¯é›¶æ°´å°çš„é²æ£’æ€§
æ”¯æŒAblation1-5æ‰€æœ‰æ¶ˆèå®éªŒ

ç”¨æ³•:
    python NC-TrainingSet-Ablation.py --ablation 1  # æ¶ˆèå®éªŒ1ï¼ˆä»…èŠ‚ç‚¹çº§ç‰¹å¾ï¼‰
    python NC-TrainingSet-Ablation.py --ablation 2  # æ¶ˆèå®éªŒ2ï¼ˆä»…å›¾çº§ç‰¹å¾ï¼‰
    python NC-TrainingSet-Ablation.py --ablation 3  # æ¶ˆèå®éªŒ3ï¼ˆæ··åˆå•æµï¼‰
    python NC-TrainingSet-Ablation.py --ablation 4  # æ¶ˆèå®éªŒ4ï¼ˆå•æ³¨æ„åŠ›å¤´ï¼‰
    python NC-TrainingSet-Ablation.py --ablation 5  # æ¶ˆèå®éªŒ5ï¼ˆGCNæ›¿ä»£GATï¼‰
"""

import os
import sys
import pickle
import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime
import shutil
import argparse

# è®¾ç½®ä¸­æ–‡å­—ä½“
try:
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'PingFang SC', 'Heiti SC', 'Songti SC']
    plt.rcParams['axes.unicode_minus'] = False
except:
    plt.rcParams['font.family'] = 'DejaVu Sans'


# â­ æ¶ˆèå®éªŒé…ç½®å­—å…¸
ABLATION_CONFIG = {
    1: {
        'name': 'Ablation1_NodeOnly',
        'description': 'æ¶ˆèå®éªŒ1ï¼ˆä»…èŠ‚ç‚¹çº§ç‰¹å¾ï¼‰',
        'model_file': 'gat_model_Ablation1_NodeOnly_best.pth',
        'model_script': 'Ablation1_NodeOnly.py',
        'model_class': 'NodeOnlyGATModel',
        'watermark_dir': 'TrainingSet-Ablation1',
        'results_dir': 'TrainingSet-Ablation1',
    },
    2: {
        'name': 'Ablation2_GraphOnly',
        'description': 'æ¶ˆèå®éªŒ2ï¼ˆä»…å›¾çº§ç‰¹å¾ï¼‰',
        'model_file': 'gat_model_Ablation2_GraphOnly_best.pth',
        'model_script': 'Ablation2_GraphOnly.py',
        'model_class': 'GraphOnlyModel',
        'watermark_dir': 'TrainingSet-Ablation2',
        'results_dir': 'TrainingSet-Ablation2',
    },
    3: {
        'name': 'Ablation3_MixedSingle',
        'description': 'æ¶ˆèå®éªŒ3ï¼ˆæ··åˆå•æµï¼‰',
        'model_file': 'gat_model_Ablation3_MixedSingle_best.pth',
        'model_script': 'Ablation3_MixedSingleStream.py',
        'model_class': 'MixedSingleStreamGATModel',
        'watermark_dir': 'TrainingSet-Ablation3',
        'results_dir': 'TrainingSet-Ablation3',
    },
    4: {
        'name': 'Ablation4_SingleHead',
        'description': 'æ¶ˆèå®éªŒ4ï¼ˆå•æ³¨æ„åŠ›å¤´ï¼‰',
        'model_file': 'gat_model_Ablation4_SingleHead_best.pth',
        'model_script': 'Ablation4_SingleHead.py',
        'model_class': 'SingleHeadGATModel',
        'watermark_dir': 'TrainingSet-Ablation4',
        'results_dir': 'TrainingSet-Ablation4',
    },
    5: {
        'name': 'Ablation5_GCN',
        'description': 'æ¶ˆèå®éªŒ5ï¼ˆGCNæ›¿ä»£GATï¼‰',
        'model_file': 'gat_model_Ablation5_GCN_best.pth',
        'model_script': 'Ablation5_GCN.py',
        'model_class': 'GCNModel',
        'watermark_dir': 'TrainingSet-Ablation5',
        'results_dir': 'TrainingSet-Ablation5',
    },
}


class RobustnessVerifier:
    """é²æ£’æ€§éªŒè¯å™¨ - æ”¯æŒæ¶ˆèå®éªŒ"""
    
    def __init__(self, ablation_id=1):
        """
        åˆå§‹åŒ–éªŒè¯å™¨
        
        Args:
            ablation_id: æ¶ˆèå®éªŒç¼–å· (1-5)
        """
        self.ablation_id = ablation_id
        self.config = ABLATION_CONFIG[ablation_id]
        
        # è®¾ç½®æ¨¡å‹è·¯å¾„
        base_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), '..'))
        self.model_path = os.path.join(base_dir, 'VGAT', 'models', self.config['model_file'])
        
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        print(f"æ¶ˆèå®éªŒ: {self.config['description']}")
        
        self.model = None
        self.load_model()
        
        # åˆå§‹åŒ–æ”»å‡»ç±»å‹æ˜ å°„å­—å…¸
        self.attack_type_mapping = self._create_attack_type_mapping()
    
    def _create_attack_type_mapping(self):
        """åˆ›å»ºæ”»å‡»ç±»å‹æ˜ å°„å­—å…¸ï¼ˆåŸºäºattack200.pyï¼‰"""
        mapping = {}
        
        # åŸºç¡€æ”»å‡»ç±»å‹
        basic_attacks = {
            "delete_10pct_vertices": "åˆ é™¤10%é¡¶ç‚¹",
            "delete_20pct_vertices": "åˆ é™¤20%é¡¶ç‚¹", 
            "delete_30pct_vertices": "åˆ é™¤30%é¡¶ç‚¹",
            "delete_40pct_vertices": "åˆ é™¤40%é¡¶ç‚¹",
            "delete_50pct_vertices": "åˆ é™¤50%é¡¶ç‚¹",
            "delete_10pct_objects": "åˆ é™¤10%å›¾å½¢å¯¹è±¡",
            "delete_20pct_objects": "åˆ é™¤20%å›¾å½¢å¯¹è±¡",
            "delete_30pct_objects": "åˆ é™¤30%å›¾å½¢å¯¹è±¡",
            "delete_40pct_objects": "åˆ é™¤40%å›¾å½¢å¯¹è±¡",
            "delete_50pct_objects": "åˆ é™¤50%å›¾å½¢å¯¹è±¡",
            "add_10pct_vertices": "æ·»åŠ 10%é¡¶ç‚¹",
            "add_20pct_vertices": "æ·»åŠ 20%é¡¶ç‚¹",
            "add_30pct_vertices": "æ·»åŠ 30%é¡¶ç‚¹",
            "add_40pct_vertices": "æ·»åŠ 40%é¡¶ç‚¹",
            "add_50pct_vertices": "æ·»åŠ 50%é¡¶ç‚¹",
            "noise_10pct_strength_0.2": "å™ªå£°æ‰°åŠ¨10%é¡¶ç‚¹ï¼Œå¼ºåº¦0.2",
            "noise_10pct_strength_0.4": "å™ªå£°æ‰°åŠ¨10%é¡¶ç‚¹ï¼Œå¼ºåº¦0.4",
            "noise_10pct_strength_0.6": "å™ªå£°æ‰°åŠ¨10%é¡¶ç‚¹ï¼Œå¼ºåº¦0.6",
            "noise_20pct_strength_0.3": "å™ªå£°æ‰°åŠ¨20%é¡¶ç‚¹ï¼Œå¼ºåº¦0.3",
            "noise_30pct_strength_0.8": "å™ªå£°æ‰°åŠ¨30%é¡¶ç‚¹ï¼Œå¼ºåº¦0.8",
            "crop_x_center_50pct": "æ²¿Xè½´ä¸­å¿ƒè£å‰ª50%",
            "crop_y_center_50pct": "æ²¿Yè½´ä¸­å¿ƒè£å‰ª50%",
            "crop_top_left": "è£å‰ªå·¦ä¸Šè§’åŒºåŸŸ",
            "crop_bottom_right": "è£å‰ªå³ä¸‹è§’åŒºåŸŸ",
            "crop_random_40pct": "éšæœºè£å‰ª40%",
            "translate_10_10": "Xè½´å³ç§»10å•ä½ï¼ŒYè½´ä¸Šç§»10å•ä½",
            "translate_20_20": "Xè½´å³ç§»20å•ä½ï¼ŒYè½´ä¸Šç§»20å•ä½",
            "translate_x_30": "ä»…Xè½´å³ç§»30å•ä½",
            "translate_y_15": "ä»…Yè½´ä¸Šç§»15å•ä½",
            "translate_neg10": "Xè½´å·¦ç§»10å•ä½ï¼ŒYè½´ä¸‹ç§»10å•ä½",
            "scale_0.5x": "ç¼©æ”¾0.5å€",
            "scale_2x": "ç¼©æ”¾2å€",
            "scale_x0.5_y2": "Xè½´ç¼©å°0.5å€ï¼ŒYè½´æ”¾å¤§2å€",
            "scale_x2_y0.5": "Xè½´æ”¾å¤§2å€ï¼ŒYè½´ç¼©å°0.5å€",
            "scale_random": "éšæœºç¼©æ”¾",
            "rotate_45": "æ—‹è½¬45åº¦",
            "rotate_90": "æ—‹è½¬90åº¦",
            "rotate_135": "æ—‹è½¬135åº¦",
            "rotate_180": "æ—‹è½¬180åº¦",
            "rotate_random": "éšæœºæ—‹è½¬",
            "flip_x": "Xè½´é•œåƒç¿»è½¬",
            "flip_y": "Yè½´é•œåƒç¿»è½¬",
            "flip_xy": "åŒæ—¶Xã€Yè½´é•œåƒç¿»è½¬",
            "reverse_vertex_order": "åè½¬é¡¶ç‚¹é¡ºåº",
            "reverse_object_order": "åè½¬å¯¹è±¡é¡ºåº",
            "shuffle_objects": "æ‰“ä¹±å¯¹è±¡é¡ºåº",
            "shuffle_vertices": "æ‰“ä¹±é¡¶ç‚¹é¡ºåº",
            "jitter_vertices": "é¡¶ç‚¹é¡ºåºéšæœºåç§»",
            "merge_objects": "åˆå¹¶å¯¹è±¡",
            "split_objects": "æ‹†åˆ†å¯¹è±¡",
        }
        
        for attack_key, description in basic_attacks.items():
            mapping[attack_key] = description
        
        # æ·»åŠ æ‰©å±•æ”»å‡»ç±»å‹
        for i in range(55, 101):
            mapping[f"attack_{i:03d}"] = f"æ‰©å±•æ”»å‡»ç­–ç•¥{i}"
        
        # æ·»åŠ ç»„åˆæ”»å‡»ç±»å‹
        for i in range(1, 101):
            mapping[f"combo_attack_{i:03d}"] = f"ç»„åˆæ”»å‡»ç­–ç•¥{i}"
        
        return mapping
    
    def get_attack_type_description(self, filename):
        """æ ¹æ®æ–‡ä»¶åè·å–æ”»å‡»ç±»å‹æè¿°"""
        base_name = filename.replace('_graph.pkl', '').replace('.geojson', '')
        
        # ç§»é™¤å›¾åå‰ç¼€
        for prefix in ['Boundary_', 'Building_', 'Lake_', 'Landuse_', 'Railways_', 'Road_']:
            if base_name.startswith(prefix):
                base_name = base_name[len(prefix):]
                break
        
        # æŸ¥æ‰¾åŒ¹é…çš„æ”»å‡»ç±»å‹
        for attack_key, description in self.attack_type_mapping.items():
            if attack_key in base_name:
                return description
        
        return base_name
    
    def load_model(self):
        """åŠ è½½æ¶ˆèå®éªŒæ¨¡å‹"""
        try:
            print(f"åŠ è½½{self.config['description']}æ¨¡å‹...")
            
            if not os.path.exists(self.model_path):
                raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
            
            # åŠ¨æ€å¯¼å…¥æ¨¡å‹ç±»
            import importlib.util
            base_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), '..'))
            model_script_path = os.path.join(base_dir, 'VGAT', self.config['model_script'])
            
            spec = importlib.util.spec_from_file_location(self.config['name'], model_script_path)
            model_module = importlib.util.module_from_spec(spec)
            sys.modules[self.config['name']] = model_module
            spec.loader.exec_module(model_module)
            
            ModelClass = getattr(model_module, self.config['model_class'])
            
            # åˆ›å»ºæ¨¡å‹å®ä¾‹
            if self.ablation_id == 5:  # GCNæ²¡æœ‰num_headså‚æ•°
                self.model = ModelClass(input_dim=20, hidden_dim=256, output_dim=1024, dropout=0.3)
            elif self.ablation_id == 4:  # å•å¤´æ³¨æ„åŠ›
                self.model = ModelClass(input_dim=20, hidden_dim=256, output_dim=1024, num_heads=1, dropout=0.3)
            else:
                self.model = ModelClass(input_dim=20, hidden_dim=256, output_dim=1024, num_heads=8, dropout=0.3)
            
            # åŠ è½½æƒé‡
            checkpoint = torch.load(self.model_path, map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            self.model.to(self.device)
            self.model.eval()
            
            print(f"âœ… {self.config['description']}æ¨¡å‹åŠ è½½å®Œæˆ")
            print(f"  è¾“å…¥ç»´åº¦: 20")
            print(f"  éšè—ç»´åº¦: 256")
            print(f"  è¾“å‡ºç»´åº¦: 1024")
            
        except Exception as e:
            print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise
    
    def extract_robust_features(self, graph_data):
        """æå–é²æ£’ç‰¹å¾"""
        if not hasattr(graph_data, 'x') or not hasattr(graph_data, 'edge_index'):
            raise ValueError("è¾“å…¥å›¾æ•°æ®æ— æ•ˆ")
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½")

        with torch.no_grad():
            features = self.model(graph_data.x.to(self.device), graph_data.edge_index.to(self.device))
            features = features.cpu().numpy()

        if features.ndim > 1:
            features = features.flatten()
        
        if len(features) != 1024:
            if len(features) < 1024:
                features = np.tile(features, (1024 // len(features) + 1,))
            features = features[:1024]
        
        return features
    
    def load_copyright_image(self, image_path=None):
        """åŠ è½½ç‰ˆæƒå›¾åƒ"""
        try:
            if image_path is None:
                image_path = os.path.normpath(os.path.join(os.path.dirname(__file__), '..', 'ZeroWatermark', 'Cat32.png'))
            image = Image.open(image_path)
            image = image.convert('L')
            image = image.resize((32, 32))
            threshold = 128
            image = image.point(lambda x: 0 if x < threshold else 255, '1')
            return np.array(image)
        except Exception as e:
            print(f"åŠ è½½ç‰ˆæƒå›¾åƒå¤±è´¥: {e}")
            return np.random.randint(0, 2, (32, 32))
    
    def features_to_matrix(self, features, target_shape):
        """å°†ç‰¹å¾å‘é‡è½¬æ¢ä¸ºçŸ©é˜µï¼ˆä½¿ç”¨ä¸­ä½æ•°é˜ˆå€¼äºŒå€¼åŒ–ï¼‰"""
        total_elements = target_shape[0] * target_shape[1]
        
        if len(features) < total_elements:
            features = np.tile(features, (total_elements // len(features) + 1,))
        
        features = features[:total_elements]
        matrix = features.reshape(target_shape)
        
        threshold = np.median(matrix)
        matrix = (matrix > threshold).astype(np.uint8)
        
        return matrix
    
    def verify_copyright(self, graph_data, zero_watermark, original_copyright):
        """éªŒè¯ç‰ˆæƒ"""
        robust_features = self.extract_robust_features(graph_data)
        feature_matrix = self.features_to_matrix(robust_features, original_copyright.shape)
        extracted_copyright = np.logical_xor(zero_watermark, feature_matrix).astype(np.uint8)
        nc_value = self.calculate_nc(original_copyright, extracted_copyright)
        
        return extracted_copyright, nc_value
    
    def calculate_nc(self, original, extracted):
        """è®¡ç®—å½’ä¸€åŒ–ç›¸å…³ç³»æ•°ï¼ˆNCå€¼ï¼‰"""
        original_vec = original.flatten().astype(float)
        extracted_vec = extracted.flatten().astype(float)
        
        dot_product = np.sum(original_vec * extracted_vec)
        norm_original = np.sqrt(np.sum(original_vec ** 2))
        norm_extracted = np.sqrt(np.sum(extracted_vec ** 2))
        
        if norm_original == 0 or norm_extracted == 0:
            return 0.0
        
        nc = dot_product / (norm_original * norm_extracted)
        return nc
    
    def load_watermark(self, filename):
        """åŠ è½½å¯¹åº”çš„é›¶æ°´å°"""
        watermark_dir = os.path.normpath(os.path.join(
            os.path.dirname(__file__), '..', 'ZeroWatermark', 'ZeroWatermark', 
            self.config['watermark_dir']
        ))
        watermark_path = os.path.join(watermark_dir, f"{filename}_watermark.npy")
        
        if os.path.exists(watermark_path):
            return np.load(watermark_path)
        else:
            print(f"é›¶æ°´å°æ–‡ä»¶ä¸å­˜åœ¨: {watermark_path}")
            return None
    
    def verify_robustness(self, original_graph, attacked_graphs, zero_watermark, copyright_image, filename, attacked_filenames=None):
        """éªŒè¯é²æ£’æ€§"""
        print(f"éªŒè¯é²æ£’æ€§: {filename}")
        
        results = []
        
        for i, attacked_graph in enumerate(attacked_graphs):
            extracted_copyright, nc_value = self.verify_copyright(
                attacked_graph, zero_watermark, copyright_image
            )
            
            attack_filename = attacked_filenames[i] if attacked_filenames and i < len(attacked_filenames) else f"attack_{i+1}"
            attack_description = self.get_attack_type_description(attack_filename)
            
            results.append({
                'attack_index': i,
                'attack_filename': attack_filename,
                'attack_description': attack_description,
                'nc_value': nc_value,
                'extracted_copyright': extracted_copyright
            })
            
            print(f"  æ”»å‡» {i+1}: NCå€¼ = {nc_value:.4f}")
        
        return results
    
    def save_robustness_results(self, results, zero_watermark, copyright_image, filename):
        """ä¿å­˜é²æ£’æ€§éªŒè¯ç»“æœ"""
        results_dir = os.path.normpath(os.path.join(
            os.path.dirname(__file__), 'NC-Results', self.config['results_dir']
        ))
        
        if not os.path.exists(results_dir):
            os.makedirs(results_dir)
        
        nc_values = [r['nc_value'] for r in results]
        avg_nc = np.mean(nc_values)
        max_nc = np.max(nc_values)
        min_nc = np.min(nc_values)
        
        # ä¿å­˜ç»“æœå›¾åƒ
        plt.figure(figsize=(15, 10))
        
        plt.subplot(3, 4, 1)
        plt.imshow(zero_watermark, cmap='gray')
        plt.title('Zero Watermark')
        plt.axis('off')
        
        plt.subplot(3, 4, 2)
        plt.imshow(copyright_image, cmap='gray')
        plt.title('Original Copyright')
        plt.axis('off')
        
        for i in range(min(6, len(results))):
            plt.subplot(3, 4, i + 7)
            plt.imshow(results[i]['extracted_copyright'], cmap='gray')
            plt.title(f'Attack {i+1}\nNC: {results[i]["nc_value"]:.4f}')
            plt.axis('off')
        
        plt.subplot(3, 4, 11)
        plt.text(0.5, 0.5, f'Average NC: {avg_nc:.4f}\nMax NC: {max_nc:.4f}\nMin NC: {min_nc:.4f}', 
                horizontalalignment='center', verticalalignment='center',
                transform=plt.gca().transAxes, fontsize=10)
        plt.title('Statistics')
        plt.axis('off')
        
        plt.tight_layout()
        plt.savefig(os.path.join(results_dir, f'{filename}_robustness_results.png'), dpi=300, bbox_inches='tight')
        plt.close()
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats_path = os.path.join(results_dir, f'{filename}_robustness_stats.txt')
        with open(stats_path, 'w', encoding='utf-8') as f:
            f.write(f"é²æ£’æ€§éªŒè¯ç»“æœ: {filename}\n")
            f.write(f"æ¶ˆèå®éªŒ: {self.config['description']}\n")
            f.write(f"å¹³å‡NCå€¼: {avg_nc:.4f}\n")
            f.write(f"æœ€é«˜NCå€¼: {max_nc:.4f}\n")
            f.write(f"æœ€ä½NCå€¼: {min_nc:.4f}\n")
            f.write(f"éªŒè¯æˆåŠŸæ•°é‡ (NC > 0.7): {sum(1 for nc in nc_values if nc > 0.7)}/{len(nc_values)}\n")
            f.write("\nè¯¦ç»†ç»“æœ:\n")
            for i, result in enumerate(results):
                f.write(f"æ”»å‡» {i+1}: NCå€¼ = {result['nc_value']:.4f}\n")
        
        print(f"é²æ£’æ€§éªŒè¯ç»“æœå·²ä¿å­˜: {results_dir}")
        return avg_nc, max_nc, min_nc
    
    def save_excel_results(self, all_results):
        """åªä¿å­˜summary.csvï¼ˆç»Ÿè®¡æ±‡æ€»ï¼‰"""
        results_dir = os.path.normpath(os.path.join(
            os.path.dirname(__file__), 'NC-Results', self.config['results_dir']
        ))
        
        if not os.path.exists(results_dir):
            os.makedirs(results_dir)
        
        timestamp = datetime.now().strftime("%Y%m%d%H%M%S")
        
        # ğŸ†• ä¿å­˜ç»Ÿè®¡æ±‡æ€»CSVï¼ˆæ¯ä¸ªå›¾çš„å¹³å‡å€¼ï¼‰
        summary_data = []
        for graph_name, results in all_results.items():
            nc_values = [r['nc_value'] for r in results]
            summary_data.append({
                'å›¾åç§°': graph_name,
                'æ”»å‡»æ•°é‡': len(nc_values),
                'å¹³å‡NCå€¼': np.mean(nc_values),
                'æœ€å¤§NCå€¼': np.max(nc_values),
                'æœ€å°NCå€¼': np.min(nc_values),
                'æ ‡å‡†å·®': np.std(nc_values)
            })
        
        # â­ æ·»åŠ æ€»ä½“ç»Ÿè®¡è¡Œ
        if summary_data:
            all_avg_nc = [d['å¹³å‡NCå€¼'] for d in summary_data]
            all_max_nc = [d['æœ€å¤§NCå€¼'] for d in summary_data]
            all_min_nc = [d['æœ€å°NCå€¼'] for d in summary_data]
            all_std = [d['æ ‡å‡†å·®'] for d in summary_data]
            total_attacks = sum(d['æ”»å‡»æ•°é‡'] for d in summary_data)
            
            summary_data.append({
                'å›¾åç§°': 'Overall Average',
                'æ”»å‡»æ•°é‡': total_attacks,
                'å¹³å‡NCå€¼': np.mean(all_avg_nc),
                'æœ€å¤§NCå€¼': np.mean(all_max_nc),
                'æœ€å°NCå€¼': np.mean(all_min_nc),
                'æ ‡å‡†å·®': np.mean(all_std)
            })
        
        df_summary = pd.DataFrame(summary_data)
        csv_summary_path = os.path.join(results_dir, f'ablation{self.ablation_id}_nc_summary_{timestamp}.csv')
        df_summary.to_csv(csv_summary_path, index=False, encoding='utf-8-sig')
        print(f"CSVç»Ÿè®¡æ±‡æ€»å·²ä¿å­˜: {csv_summary_path}")
        
        return csv_summary_path


class GraphDataLoader:
    """å›¾æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, graph_dir=None):
        if graph_dir is None:
            base_dir = os.path.normpath(os.path.join(os.path.dirname(__file__), '..'))
            graph_dir = os.path.join(base_dir, 'convertToGraph', 'Graph', 'TrainingSet')
        self.graph_dir = os.path.normpath(graph_dir)
        print(f"å›¾æ•°æ®åŠ è½½è·¯å¾„: {self.graph_dir}")
    
    def load_original_graphs_train_only(self):
        """åªåŠ è½½è®­ç»ƒé›†çš„åŸå§‹å›¾æ•°æ®"""
        original_dir = os.path.join(self.graph_dir, 'Original')
        
        if not os.path.exists(original_dir):
            print(f"ç›®å½•ä¸å­˜åœ¨: {original_dir}")
            return [], []
        
        graphs = []
        filenames = []
        
        for filename in os.listdir(original_dir):
            if filename.endswith('_graph.pkl'):
                try:
                    with open(os.path.join(original_dir, filename), 'rb') as f:
                        graph_data = pickle.load(f)
                        graphs.append(graph_data)
                        base_name = filename.replace('_graph.pkl', '')
                        filenames.append(base_name)
                        print(f"æˆåŠŸåŠ è½½è®­ç»ƒå›¾æ•°æ®: {filename}")
                except Exception as e:
                    print(f"åŠ è½½å›¾æ•°æ®å¤±è´¥ {filename}: {e}")
                    continue
        
        print(f"æ€»å…±åŠ è½½äº† {len(graphs)} ä¸ªè®­ç»ƒå›¾æ•°æ®")
        return graphs, filenames
    
    def load_attacked_graphs_for_original(self, original_filename):
        """åŠ è½½æŒ‡å®šåŸå§‹å›¾å¯¹åº”çš„æ‰€æœ‰è¢«æ”»å‡»å›¾"""
        attacked_dir = os.path.join(self.graph_dir, 'Attacked')
        
        if not os.path.exists(attacked_dir):
            print(f"è¢«æ”»å‡»å›¾ç›®å½•ä¸å­˜åœ¨: {attacked_dir}")
            return [], []
        
        subdir_path = os.path.join(attacked_dir, original_filename)
        if os.path.exists(subdir_path):
            attacked_graphs = []
            attacked_filenames = []
            for filename in sorted(os.listdir(subdir_path)):
                if filename.endswith('_graph.pkl'):
                    try:
                        with open(os.path.join(subdir_path, filename), 'rb') as f:
                            graph_data = pickle.load(f)
                            attacked_graphs.append(graph_data)
                            attacked_filenames.append(filename)
                            print(f"æˆåŠŸåŠ è½½è¢«æ”»å‡»å›¾: {filename}")
                    except Exception as e:
                        print(f"åŠ è½½è¢«æ”»å‡»å›¾å¤±è´¥ {filename}: {e}")
                        continue
            
            print(f"ä¸º {original_filename} åŠ è½½äº† {len(attacked_graphs)} ä¸ªè¢«æ”»å‡»å›¾")
            return attacked_graphs, attacked_filenames
        else:
            print(f"æ²¡æœ‰æ‰¾åˆ° {original_filename} å¯¹åº”çš„è¢«æ”»å‡»å›¾ç›®å½•")
            return [], []


def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è®­ç»ƒé›†NCæµ‹è¯• - æ¶ˆèå®éªŒ')
    parser.add_argument('--ablation', type=int, required=True, choices=[1, 2, 3, 4, 5],
                       help='æ¶ˆèå®éªŒç¼–å· (1-5)')
    args = parser.parse_args()
    
    ablation_id = args.ablation
    config = ABLATION_CONFIG[ablation_id]
    
    print("="*70)
    print(f"è®­ç»ƒé›†NCæµ‹è¯• - {config['description']}")
    print("="*70)
    print()
    
    # æ¸…ç†å¹¶å‡†å¤‡ç»“æœè¾“å‡ºç›®å½•
    results_root = os.path.normpath(os.path.join(os.path.dirname(__file__), 'NC-Results', config['results_dir']))
    if os.path.exists(results_root):
        print(f"æ¸…ç†æ—§çš„ç»“æœç›®å½•: {results_root}")
        try:
            shutil.rmtree(results_root)
            print("[OK] æ—§ç»“æœå·²æ¸…ç†")
        except Exception as e:
            print(f"[WARNING] æ¸…ç†ç›®å½•æ—¶å‡ºé”™: {e}")
    os.makedirs(results_root, exist_ok=True)
    print()
    
    # åŠ è½½è®­ç»ƒé›†çš„åŸå§‹å›¾æ•°æ®
    data_loader = GraphDataLoader()
    original_graphs, filenames = data_loader.load_original_graphs_train_only()
    
    if not original_graphs:
        print("æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå›¾æ•°æ®")
        return
    
    # åˆ›å»ºé²æ£’æ€§éªŒè¯å™¨
    verifier = RobustnessVerifier(ablation_id=ablation_id)
    
    # åŠ è½½ç‰ˆæƒå›¾åƒ
    copyright_image = verifier.load_copyright_image()
    print(f"ç‰ˆæƒå›¾åƒå¤§å°: {copyright_image.shape}")
    
    # ä¸ºæ¯ä¸ªè®­ç»ƒå›¾éªŒè¯é²æ£’æ€§
    all_avg_nc = []
    all_max_nc = []
    all_min_nc = []
    all_excel_results = {}
    
    for i, (original_graph, filename) in enumerate(zip(original_graphs, filenames)):
        print(f"\nå¤„ç†ç¬¬ {i+1}/{len(original_graphs)} ä¸ªå›¾: {filename}")
        
        # åŠ è½½å¯¹åº”çš„é›¶æ°´å°
        zero_watermark = verifier.load_watermark(filename)
        if zero_watermark is None:
            print(f"è·³è¿‡ {filename}ï¼Œé›¶æ°´å°ä¸å­˜åœ¨")
            continue
        
        # åŠ è½½å¯¹åº”çš„è¢«æ”»å‡»å›¾
        attacked_graphs, attacked_filenames = data_loader.load_attacked_graphs_for_original(filename)
        
        if not attacked_graphs:
            print(f"è·³è¿‡ {filename}ï¼Œæ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„è¢«æ”»å‡»å›¾")
            continue
        
        # éªŒè¯é²æ£’æ€§
        results = verifier.verify_robustness(
            original_graph, attacked_graphs, zero_watermark, copyright_image, filename, attacked_filenames
        )
        
        # æ”¶é›†Excelæ•°æ®
        all_excel_results[filename] = results
        
        # ä¿å­˜ç»“æœ
        avg_nc, max_nc, min_nc = verifier.save_robustness_results(
            results, zero_watermark, copyright_image, filename
        )
        
        all_avg_nc.append(avg_nc)
        all_max_nc.append(max_nc)
        all_min_nc.append(min_nc)
    
    # è¾“å‡ºæ€»ä½“ç»“æœ
    if all_avg_nc:
        # æŒ‰æ”»å‡»ç±»å‹ç»Ÿè®¡
        if all_excel_results:
            print(f"\næŒ‰æ”»å‡»ç±»å‹ç»Ÿè®¡:")
            print("-" * 50)
            
            attack_nc_values = {}
            for filename, results in all_excel_results.items():
                for result in results:
                    attack_desc = result.get('attack_description', 'æœªçŸ¥æ”»å‡»')
                    nc_value = result.get('nc_value', 0)
                    if attack_desc not in attack_nc_values:
                        attack_nc_values[attack_desc] = []
                    attack_nc_values[attack_desc].append(nc_value)
            
            for attack_type, nc_values in attack_nc_values.items():
                avg_nc = np.mean(nc_values)
                max_nc = np.max(nc_values)
                min_nc = np.min(nc_values)
                success_count = sum(1 for nc in nc_values if nc > 0.7)
                success_rate = (success_count / len(nc_values)) * 100
                
                print(f"{attack_type}:")
                print(f"  æµ‹è¯•æ•°é‡: {len(nc_values)}")
                print(f"  æˆåŠŸéªŒè¯æ•°: {success_count}")
                print(f"  æˆåŠŸç‡: {success_rate:.2f}%")
                print(f"  å¹³å‡NCå€¼: {avg_nc:.4f}")
                print(f"  æœ€å¤§NCå€¼: {max_nc:.4f}")
                print(f"  æœ€å°NCå€¼: {min_nc:.4f}")
                print()
        
        print(f"\n{'='*70}")
        print(f"{config['description']} - è®­ç»ƒé›†NCå€¼éªŒè¯æ€»ä½“ç»“æœ")
        print(f"{'='*70}")
        print(f"å¤„ç†çš„å›¾æ•°é‡: {len(all_avg_nc)}")
        
        # æŒ‰åŸå§‹åœ°å›¾ç»Ÿè®¡
        print(f"\næŒ‰åŸå§‹åœ°å›¾ç»Ÿè®¡:")
        print("-" * 50)
        for i, filename in enumerate(filenames):
            if i < len(all_avg_nc):
                avg_nc = all_avg_nc[i]
                max_nc = all_max_nc[i]
                min_nc = all_min_nc[i]
                success_status = "æˆåŠŸ" if avg_nc > 0.7 else "å¤±è´¥"
                
                print(f"{filename}:")
                print(f"  å¹³å‡NCå€¼: {avg_nc:.4f}")
                print(f"  æœ€å¤§NCå€¼: {max_nc:.4f}")
                print(f"  æœ€å°NCå€¼: {min_nc:.4f}")
                print(f"  éªŒè¯çŠ¶æ€: {success_status}")
        
        # æ€»ä½“ç»Ÿè®¡
        print(f"\næ€»ä½“ç»Ÿè®¡:")
        print("-" * 50)
        overall_avg_nc = np.mean(all_avg_nc)
        overall_max_nc = np.mean(all_max_nc)
        overall_min_nc = np.mean(all_min_nc)
        overall_std_nc = np.std(all_avg_nc)
        
        print(f"æ€»ä½“å¹³å‡NCå€¼: {overall_avg_nc:.4f}")
        print(f"æ€»ä½“æœ€å¤§NCå€¼: {overall_max_nc:.4f}")
        print(f"æ€»ä½“æœ€å°NCå€¼: {overall_min_nc:.4f}")
        print(f"æ€»ä½“NCå€¼æ ‡å‡†å·®: {overall_std_nc:.4f}")
        
        # æˆåŠŸç‡ç»Ÿè®¡
        success_count = sum(1 for avg_nc in all_avg_nc if avg_nc > 0.7)
        success_rate = (success_count / len(all_avg_nc)) * 100
        
        print(f"\næˆåŠŸç‡ç»Ÿè®¡:")
        print("-" * 50)
        print(f"æ€»éªŒè¯æ•°é‡: {len(all_avg_nc)}")
        print(f"æˆåŠŸéªŒè¯æ•°é‡: {success_count}")
        print(f"æ€»ä½“æˆåŠŸç‡: {success_rate:.2f}%")
        
        print(f"\néªŒè¯æˆåŠŸç»Ÿè®¡:")
        print("-" * 50)
        print(f"åœ°å›¾éªŒè¯æˆåŠŸæ•°é‡: {success_count}/{len(all_avg_nc)}")
        
        # ç”ŸæˆExcelè¡¨æ ¼
        if all_excel_results:
            verifier.save_excel_results(all_excel_results)
        
        print(f"\n{'='*70}")
        print(f"[OK] é²æ£’æ€§éªŒè¯å®Œæˆï¼")
        print(f"ç»“æœä¿å­˜ç›®å½•: {results_root}")
        print(f"{'='*70}")
    else:
        print("\n[WARNING] æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å›¾æ•°æ®")
        print(f"{'='*70}")


if __name__ == "__main__":
    main()

