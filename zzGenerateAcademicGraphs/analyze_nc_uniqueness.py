#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†ææµ‹è¯•é›†é›¶æ°´å°çš„NCå”¯ä¸€æ€§
åªåˆ†æNCå€¼ï¼ˆå½’ä¸€åŒ–ç›¸å…³ç³»æ•°ï¼‰
"""

import numpy as np
import cv2
from pathlib import Path
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt
import json
import re
import pandas as pd
import shutil
from pathlib import Path as _Path


def load_match_map(script_dir: _Path) -> tuple[dict, list]:
    """Load mapping from zzGenerateAcademicGraphs/match.txt (value -> key)."""
    mfile = script_dir / "match.txt"
    mapping = {}
    order: list = []
    if not mfile.exists():
        return mapping
    with mfile.open("r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or ':' not in line:
                continue
            key, val = line.split(':', 1)
            key = key.strip()
            val = val.strip()
            if val:
                mapping[val] = key
    return mapping, order

def generate_label_from_filename(filename):
    """ä»æ–‡ä»¶åç”Ÿæˆç®€æ´çš„æ ‡ç­¾"""
    # ç§»é™¤æ‰©å±•åå’Œ_watermarkåç¼€
    base = filename.replace('_watermark.png', '').replace('_watermark.npy', '')
    
    # æå–å…³é”®éƒ¨åˆ†
    # å¤„ç† H50-XXX æ ¼å¼
    if base.startswith('H50-'):
        return base.replace('H50-', '')
    
    # å¤„ç† shanghai-latest-free.shp-gis_osm_xxx æ ¼å¼
    if 'gis_osm_' in base:
        match = re.search(r'gis_osm_(\w+)_', base)
        if match:
            category = match.group(1)
            # è½¬æ¢ä¸ºæ›´å‹å¥½çš„åç§°
            category_map = {
                'landuse': 'Landuse',
                'natural': 'ScenicSpot',
                'railways': 'Railways',
                'waterways': 'Waterways',
                'places': 'Places',
                'transport': 'Transport'
            }
            return category_map.get(category, category.capitalize())
    
    # å¦‚æœæ–‡ä»¶åå¤ªé•¿ï¼Œæˆªå–å…³é”®éƒ¨åˆ†
    if len(base) > 15:
        # å°è¯•æå–æœ€åä¸€ä¸ªæœ‰æ„ä¹‰çš„éƒ¨åˆ†
        parts = base.split('-')
        if len(parts) > 1:
            return parts[-1]
        return base[:15]
    
    return base

def analyze_nc_uniqueness():
    """åˆ†æé›¶æ°´å°NCçŸ©é˜µ"""
    
    script_dir = Path(__file__).resolve().parent
    project_root = script_dir.parent
    # Prefer original zNC-Test data locations (do not change input paths)
    folder_candidates = [
        project_root / 'zNC-Test' / 'vector-data-zerowatermark',
        script_dir / 'vector-data-zerowatermark',
    ]
    watermark_dir = next((p for p in folder_candidates if p.exists()), folder_candidates[0])
    print(f"[INFO] Using watermark directory: {watermark_dir}")
    match_map, match_order = load_match_map(script_dir)
    
    # è‡ªé€‚åº”æ‰«ææ‰€æœ‰æ°´å°æ–‡ä»¶ï¼ˆä¼˜å…ˆä½¿ç”¨.pngï¼‰
    watermark_files = {}
    for ext in ['.png', '.npy']:
        for file_path in watermark_dir.glob(f'*_watermark{ext}'):
            base_name = file_path.stem.replace('_watermark', '')
            if base_name not in watermark_files:
                watermark_files[base_name] = file_path
    
    if not watermark_files:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æ°´å°æ–‡ä»¶")
        return
    
    # åŠ è½½é›¶æ°´å°å‘é‡
    vectors = []
    found_labels = []
    found_files = []
    
    print(f"\nğŸ“Š æ‰¾åˆ° {len(watermark_files)} ä¸ªæ°´å°æ–‡ä»¶ï¼Œå¼€å§‹åŠ è½½...\n")
    
    for base_name, file_path in sorted(watermark_files.items()):
        label = generate_label_from_filename(file_path.name)
        
        try:
            if file_path.suffix == '.png':
                img = cv2.imread(str(file_path), 0)
                if img is None:
                    print(f"âš ï¸  è­¦å‘Š: æ— æ³•è¯»å–å›¾ç‰‡ {file_path.name}")
                    continue
                vec = (img.flatten() / 255).astype(np.uint8)
            else:  # .npy
                vec = np.load(file_path).astype(np.uint8)
                # å¦‚æœæ˜¯2Dæ•°ç»„ï¼Œå±•å¹³
                if vec.ndim > 1:
                    vec = vec.flatten()
            
            vectors.append(vec)
            found_labels.append(label)
            found_files.append(base_name)
            print(f"âœ“ åŠ è½½ {label:20s}: {vec.shape} -> {len(vec)} bits")
        except Exception as e:
            print(f"âš ï¸  è­¦å‘Š: åŠ è½½ {file_path.name} å¤±è´¥: {e}")
            continue
    
    if not vectors:
        print("âŒ é”™è¯¯: æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ°´å°æ–‡ä»¶")
        return
    
    n = len(vectors)
    print(f"\nğŸ“ è®¡ç®— {n}x{n} NCçŸ©é˜µ...\n")
    
    # è®¡ç®—NCçŸ©é˜µ
    nc_matrix = np.zeros((n, n))
    for i in range(n):
        for j in range(n):
            v1 = vectors[i].astype(float)
            v2 = vectors[j].astype(float)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)
            if norm1 > 0 and norm2 > 0:
                nc = np.dot(v1, v2) / (norm1 * norm2)
            else:
                nc = 0.0
            nc_matrix[i, j] = nc
    
    # æå–éå¯¹è§’çº¿å…ƒç´ 
    mask = ~np.eye(n, dtype=bool)
    off_diag = nc_matrix[mask]
    
    # ç»Ÿè®¡ä¿¡æ¯
    max_off_diag_nc = float(np.max(off_diag))
    min_off_diag_nc = float(np.min(off_diag))
    mean_off_diag_nc = float(np.mean(off_diag))
    std_off_diag_nc = float(np.std(off_diag))
    median_off_diag_nc = float(np.median(off_diag))
    
    # ç»Ÿè®¡å„åŒºé—´çš„é…å¯¹æ•°
    ranges = [
        (0.0, 0.5, "æä½ç›¸ä¼¼"),
        (0.5, 0.75, "ä½ç›¸ä¼¼"),
        (0.75, 0.85, "ä¸­ç­‰ç›¸ä¼¼"),
        (0.85, 0.9, "é«˜ç›¸ä¼¼"),
        (0.9, 1.0, "æé«˜ç›¸ä¼¼")
    ]
    
    total_pairs = len(off_diag)
    
    # ç»Ÿè®¡ä¸åŒé˜ˆå€¼çš„é…å¯¹æ•°
    threshold_uniqueness = 0.82
    pairs_ge_080 = int(np.sum(off_diag >= 0.80))
    pairs_ge_082 = int(np.sum(off_diag >= 0.82))
    pairs_ge_085 = int(np.sum(off_diag >= 0.85))
    pairs_ge_090 = int(np.sum(off_diag >= 0.90))
    
    # æ‰¾å‡ºæœ€é«˜çš„NCå€¼é…å¯¹
    indices = np.triu_indices(n, k=1)
    nc_pairs = [(found_labels[i], found_labels[j], nc_matrix[i, j]) 
                for i, j in zip(indices[0], indices[1])]
    nc_pairs.sort(key=lambda x: x[2], reverse=True)
    
    # Reorder according to match.txt order for display if mapping provided
    if match_order:
        idx_order = sorted(range(len(found_files)), key=lambda i: (match_order.index(found_files[i]) if found_files[i] in match_order else len(match_order)+i))
        nc_matrix = nc_matrix[np.ix_(idx_order, idx_order)]
        found_labels = [found_labels[i] for i in idx_order]
        found_files = [found_files[i] for i in idx_order]
    # è¯¦ç»†åˆ†æè¾“å‡º
    print("\n" + "=" * 80)
    print("ğŸ“Š NCå”¯ä¸€æ€§åˆ†ææŠ¥å‘Š")
    print("=" * 80)
    print(f"\nğŸ“ˆ åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»é…å¯¹æ•°: {total_pairs}")
    print(f"   æœ€å¤§éå¯¹è§’çº¿NCå€¼: {max_off_diag_nc:.6f}")
    print(f"   æœ€å°éå¯¹è§’çº¿NCå€¼: {min_off_diag_nc:.6f}")
    print(f"   å¹³å‡éå¯¹è§’çº¿NCå€¼: {mean_off_diag_nc:.6f}")
    print(f"   æ ‡å‡†å·®: {std_off_diag_nc:.6f}")
    print(f"   ä¸­ä½æ•°: {median_off_diag_nc:.6f}")
    
    print(f"\nğŸ¯ é˜ˆå€¼ç»Ÿè®¡:")
    print(f"   NC â‰¥ 0.80 çš„é…å¯¹æ•°: {pairs_ge_080} ({pairs_ge_080/total_pairs*100:.2f}%)")
    print(f"   NC â‰¥ 0.82 çš„é…å¯¹æ•°: {pairs_ge_082} ({pairs_ge_082/total_pairs*100:.2f}%)")
    print(f"   NC â‰¥ 0.85 çš„é…å¯¹æ•°: {pairs_ge_085} ({pairs_ge_085/total_pairs*100:.2f}%)")
    print(f"   NC â‰¥ 0.90 çš„é…å¯¹æ•°: {pairs_ge_090} ({pairs_ge_090/total_pairs*100:.2f}%)")
    
    print(f"\nğŸ“Š ç›¸ä¼¼åº¦åˆ†å¸ƒ:")
    for low, high, desc in ranges:
        count = int(np.sum((off_diag >= low) & (off_diag < high)))
        if high == 1.0:
            count = int(np.sum(off_diag >= low))
        pct = count / total_pairs * 100 if total_pairs > 0 else 0
        print(f"   {desc:8s} [{low:.2f}-{high:.2f}): {count:3d} å¯¹ ({pct:5.2f}%)")
    
    # å”¯ä¸€æ€§è¯„ä¼°
    uniqueness_ok = max_off_diag_nc < threshold_uniqueness
    print(f"\nâœ… å”¯ä¸€æ€§è¯„ä¼° (é˜ˆå€¼={threshold_uniqueness}):")
    if uniqueness_ok:
        print(f"   âœ“ é€šè¿‡: æœ€å¤§éå¯¹è§’çº¿NCå€¼ {max_off_diag_nc:.6f} < {threshold_uniqueness}")
    else:
        print(f"   âœ— æœªé€šè¿‡: æœ€å¤§éå¯¹è§’çº¿NCå€¼ {max_off_diag_nc:.6f} â‰¥ {threshold_uniqueness}")
    
    # æ˜¾ç¤ºé«˜ç›¸ä¼¼åº¦é…å¯¹
    high_sim_pairs = [p for p in nc_pairs if p[2] >= 0.75]
    if high_sim_pairs:
        print(f"\nâš ï¸  é«˜ç›¸ä¼¼åº¦é…å¯¹ (NC â‰¥ 0.75):")
        for label1, label2, nc_val in high_sim_pairs[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"   {label1:20s} <-> {label2:20s}: {nc_val:.6f}")
        if len(high_sim_pairs) > 10:
            print(f"   ... è¿˜æœ‰ {len(high_sim_pairs) - 10} å¯¹æœªæ˜¾ç¤º")
    
    print("\n" + "=" * 80 + "\n")
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ° zzGenerateAcademicGraphs ç›®å½•ï¼ˆè„šæœ¬æ‰€åœ¨ç›®å½•ï¼‰
    out_dir = script_dir
    out_dir.mkdir(parents=True, exist_ok=True)
    png_dir = out_dir / "PNG"
    csv_dir = out_dir / "CSV"
    manuscript_dir = out_dir.parent / "zzManuscript" / "AcademicGraphs"
    for d in (png_dir, csv_dir, manuscript_dir):
        d.mkdir(parents=True, exist_ok=True)
    
    stats = {
        "max_off_diag_nc": max_off_diag_nc,
        "min_off_diag_nc": min_off_diag_nc,
        "mean_off_diag_nc": mean_off_diag_nc,
        "std_off_diag_nc": std_off_diag_nc,
        "median_off_diag_nc": median_off_diag_nc,
        "pairs_ge_0.80": pairs_ge_080,
        "pairs_ge_0.82": pairs_ge_082,
        "pairs_ge_0.85": pairs_ge_085,
        "pairs_ge_0.90": pairs_ge_090,
        "total_pairs": total_pairs,
        "threshold_ok": uniqueness_ok,
        "threshold_uniqueness": threshold_uniqueness,
        "high_similarity_pairs": [
            {"label1": label1, "label2": label2, "nc": float(nc_val)}
            for label1, label2, nc_val in high_sim_pairs[:20]
        ]
    }
    
    # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°ä¸¤ä¸ªä½ç½®
    stats_path = csv_dir / 'nc_uniqueness_stats.json'
    with open(stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    print(f"âœ“ ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {stats_path}")
    # Also save local copy in CSV dir (same)
    local_stats_path = csv_dir / 'nc_uniqueness_stats.json'
    with open(local_stats_path, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)
    print(f"âœ“ ç»Ÿè®¡ä¿¡æ¯å·² saved: {local_stats_path}")
    
    # ä¿å­˜NCçŸ©é˜µä¸ºCSVæ–‡ä»¶ï¼ˆçŸ­æ ‡ç­¾ï¼‰ä¸å…¨ç§°CSV
    # Map full file basenames to user-friendly names via match.txt when available
    mapped_files = [match_map.get(f, f) for f in found_files]
    nc_df = pd.DataFrame(nc_matrix, index=found_labels, columns=found_labels)
    csv_path = csv_dir / 'NC_Matrix.csv'
    nc_df.to_csv(csv_path, float_format='%.6f')
    print(f"âœ“ NCçŸ©é˜µCSVå·²ä¿å­˜: {csv_path}")

    # å…¨ç§°ç‰ˆæœ¬ï¼ˆä½¿ç”¨åŸå§‹æ–‡ä»¶åŸºåï¼Œä¾¿äºå®Œæ•´æ˜¾ç¤ºï¼‰
    nc_df_full = pd.DataFrame(nc_matrix, index=found_files, columns=found_files)
    csv_full_path = csv_dir / 'NC_Matrix_full.csv'
    nc_df_full.to_csv(csv_full_path, float_format='%.6f')
    print(f"âœ“ NCçŸ©é˜µå…¨ç§°CSVå·²ä¿å­˜: {csv_full_path}")

    # ä¿å­˜æ ‡ç­¾æ˜ å°„ï¼ˆçŸ­æ ‡ç­¾ -> å…¨ç§°ï¼‰
    mapping_path = csv_dir / 'label_mapping.json'
    mapping = [{"short": s, "full": f, "mapped": match_map.get(f, f)} for s, f in zip(found_labels, found_files)]
    with open(mapping_path, 'w', encoding='utf-8') as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)
    print(f"âœ“ æ ‡ç­¾æ˜ å°„å·²ä¿å­˜: {mapping_path}")
    
    # ç”Ÿæˆçƒ­åŠ›å›¾ï¼ˆç»Ÿä¸€é£æ ¼ï¼šæ— æ ‡é¢˜ï¼Œæ ‡æ³¨æ–‡å­—ä¸ºé»‘è‰²ï¼‰
    # ä½¿ç”¨åŒ¹é…è¡¨ä¸­å‹å¥½åç§°ï¼ˆè‹¥æœ‰ï¼‰ä½œä¸ºæ˜¾ç¤ºæ ‡ç­¾
    display_labels = mapped_files
    plt.close('all')  # Clear any existing figures
    fig, ax = plt.subplots(figsize=(10, 8))
    im = ax.imshow(nc_matrix, cmap='viridis', vmin=0, vmax=1, aspect='auto')
    
    # æ·»åŠ colorbar
    cbar = plt.colorbar(im, ax=ax)
    cbar.set_label('NC Value', rotation=270, labelpad=20, fontsize=19)
    cbar.ax.tick_params(labelsize=19)  # colorbaråˆ»åº¦å­—ä½“å¤§å°å¢å¤§6å·
    
    # è®¾ç½®åˆ»åº¦ï¼ˆä½¿ç”¨å…¨ç§°æ ‡ç­¾ä»¥ä¾¿åœ¨çƒ­åŠ›å›¾ä¸Šå®Œæ•´å±•ç¤ºçŸ¢é‡å›¾åï¼‰
    ax.set_xticks(np.arange(len(display_labels)))
    ax.set_yticks(np.arange(len(display_labels)))
    # increase fontsize by 2 compared to previous (was default small)
    ax.set_xticklabels(display_labels, fontsize=19)
    ax.set_yticklabels(display_labels, fontsize=19)
    
    # æ—‹è½¬xè½´æ ‡ç­¾
    plt.setp(ax.get_xticklabels(), rotation=45, ha="right", rotation_mode="anchor")
    
    # æ·»åŠ æ•°å€¼æ ‡æ³¨ï¼ˆç»Ÿä¸€ä¸ºé»‘è‰²å­—ä½“ï¼‰ï¼Œå­—ä½“æ¯”ä¹‹å‰å¤§2å·
    for i in range(len(found_labels)):
        for j in range(len(found_labels)):
            text = ax.text(j, i, f'{nc_matrix[i, j]:.2f}',
                          ha="center", va="center",
                          color="black",
                          fontsize=19)
    
    plt.tight_layout()
    
    # ä¿å­˜çƒ­åŠ›å›¾åˆ°ä¸¤ä¸ªä½ç½®
    output_path = png_dir / 'NC_Matrix_Heatmap.png'
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    try:
        shutil.copy2(str(output_path), str(manuscript_dir / output_path.name))
        print(f"âœ“ çƒ­åŠ›å›¾å·²å¤åˆ¶åˆ°æ‰‹ç¨¿ç›®å½•: {manuscript_dir / output_path.name}")
    except Exception as e:
        print(f"âš ï¸ è­¦å‘Š: å¤åˆ¶åˆ°æ‰‹ç¨¿ç›®å½•å¤±è´¥: {e}")
    print(f"âœ“ çƒ­åŠ›å›¾å·²ä¿å­˜: {output_path}")
    plt.close()

if __name__ == '__main__':
    analyze_nc_uniqueness()
